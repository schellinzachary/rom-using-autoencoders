\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\@input{front/title.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Contents}{i}{chapter*.1}\protected@file@percent }
\@input{chapters/introduction.aux}
\@input{chapters/BGK_model.aux}
\@input{chapters/Deep_Learning.aux}
\@input{chapters/Reduced_order_Modeling.aux}
\@input{chapters/Results.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Hyperparameters for the Fully Connected Autoencoder}{38}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApA}{{A}{38}{Hyperparameters for the Fully Connected Autoencoder}{appendix.A}{}}
\newlabel{Ch:ApA@cref}{{[appendix][1][2147483647]A}{[1][38][]38}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces \footnotesize  Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }}{38}{figure.caption.30}\protected@file@percent }
\newlabel{Fig:ActScheme}{{A.1}{38}{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }{figure.caption.30}{}}
\newlabel{Fig:ActScheme@cref}{{[figure][1][2147483647,1]A.1}{[1][38][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }}{38}{table.caption.31}\protected@file@percent }
\newlabel{Tab:First Guess}{{A.1}{38}{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }{table.caption.31}{}}
\newlabel{Tab:First Guess@cref}{{[table][1][2147483647,1]A.1}{[1][38][]38}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }}{39}{table.caption.32}\protected@file@percent }
\newlabel{Tab:Depth}{{A.2}{39}{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.32}{}}
\newlabel{Tab:Depth@cref}{{[table][2][2147483647,1]A.2}{[1][39][]39}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }}{40}{table.caption.33}\protected@file@percent }
\newlabel{Tab:Width}{{A.3}{40}{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.33}{}}
\newlabel{Tab:Width@cref}{{[table][3][2147483647,1]A.3}{[1][39][]40}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }}{41}{table.caption.34}\protected@file@percent }
\newlabel{Tab:Batch}{{A.4}{41}{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }{table.caption.34}{}}
\newlabel{Tab:Batch@cref}{{[table][4][2147483647,1]A.4}{[1][39][]41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\) all evaluated with the best performing model.\relax }}{42}{table.caption.35}\protected@file@percent }
\newlabel{Tab:activations}{{A.5}{42}{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\) all evaluated with the best performing model.\relax }{table.caption.35}{}}
\newlabel{Tab:activations@cref}{{[table][5][2147483647,1]A.5}{[1][41][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Five experiments over the different depth with $\hy  $ left and $\rare  $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }}{43}{figure.caption.36}\protected@file@percent }
\newlabel{Fig:Depth}{{A.2}{43}{Five experiments over the different depth with $\hy $ left and $\rare $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }{figure.caption.36}{}}
\newlabel{Fig:Depth@cref}{{[figure][2][2147483647,1]A.2}{[1][42][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Five experiments over different width with $\hy  $ left and $\rare  $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }}{44}{figure.caption.37}\protected@file@percent }
\newlabel{Fig:Width}{{A.3}{44}{Five experiments over different width with $\hy $ left and $\rare $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }{figure.caption.37}{}}
\newlabel{Fig:Width@cref}{{[figure][3][2147483647,1]A.3}{[1][42][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{45}{figure.caption.38}\protected@file@percent }
\newlabel{Fig:batch}{{A.4}{45}{Five experiments over different batch sizes with $\hy $ left and $\rare $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }{figure.caption.38}{}}
\newlabel{Fig:batch@cref}{{[figure][4][2147483647,1]A.4}{[1][42][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{47}{figure.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Hyperparameters for the Convolutional Autoencoder}{48}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApB}{{B}{48}{Hyperparameters for the Convolutional Autoencoder}{appendix.B}{}}
\newlabel{Ch:ApB@cref}{{[appendix][2][2147483647]B}{[1][48][]48}}
\bibdata{other/references}
\bibcite{BGK}{1}
\bibcite{Carlberg}{2}
\bibcite{bukka2020assessment}{3}
\bibcite{Franz}{4}
\bibcite{Kutz}{5}
\bibcite{Bernard}{6}
\bibcite{Goodfellow}{7}
\bibcite{Rumelhart}{8}
\bibcite{Ballard}{9}
\bibcite{Rifai2011}{10}
\bibcite{Rifai_2011a}{11}
\bibcite{rifai2012generative}{12}
\bibcite{schaaf}{13}
\bibcite{puppo2019kinetic}{14}
\bibcite{Sod}{15}
\bibcite{CFD1}{16}
\bibcite{LeCun98}{17}
\bibcite{he2015}{18}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{49}{appendix*.41}\protected@file@percent }
\bibcite{torch_ini}{19}
\bibcite{bushaev_2017}{20}
\bibcite{kingma2017adam}{21}
\bibcite{lane_2018}{22}
\bibcite{divyanshu_2020}{23}
\bibcite{dumoulin2018guide}{24}
\bibcite{ohlberger2015reduced}{25}
\bibcite{NumaKUL}{26}
\ttl@finishall
