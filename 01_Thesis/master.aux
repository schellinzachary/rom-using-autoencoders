\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\@input{front/title.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Contents}{i}{chapter*.1}\protected@file@percent }
\@input{chapters/introduction.aux}
\@input{chapters/BGK_model.aux}
\@input{chapters/Deep_Learning.aux}
\@input{chapters/Reduced_order_Modeling.aux}
\@input{chapters/Results.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Hyperparameters for the Fully Connected Autoencoder}{40}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApA}{{A}{40}{Hyperparameters for the Fully Connected Autoencoder}{appendix.A}{}}
\newlabel{Ch:ApA@cref}{{[appendix][1][2147483647]A}{[1][40][]40}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces \footnotesize  Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }}{40}{figure.caption.30}\protected@file@percent }
\newlabel{Fig:ActScheme}{{A.1}{40}{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }{figure.caption.30}{}}
\newlabel{Fig:ActScheme@cref}{{[figure][1][2147483647,1]A.1}{[1][40][]40}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }}{40}{table.caption.31}\protected@file@percent }
\newlabel{Tab:First Guess}{{A.1}{40}{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }{table.caption.31}{}}
\newlabel{Tab:First Guess@cref}{{[table][1][2147483647,1]A.1}{[1][40][]40}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }}{41}{table.caption.32}\protected@file@percent }
\newlabel{Tab:Depth}{{A.2}{41}{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.32}{}}
\newlabel{Tab:Depth@cref}{{[table][2][2147483647,1]A.2}{[1][41][]41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }}{42}{table.caption.33}\protected@file@percent }
\newlabel{Tab:Width}{{A.3}{42}{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.33}{}}
\newlabel{Tab:Width@cref}{{[table][3][2147483647,1]A.3}{[1][41][]42}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }}{43}{table.caption.34}\protected@file@percent }
\newlabel{Tab:Batch}{{A.4}{43}{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }{table.caption.34}{}}
\newlabel{Tab:Batch@cref}{{[table][4][2147483647,1]A.4}{[1][41][]43}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }}{44}{table.caption.35}\protected@file@percent }
\newlabel{Tab:activations}{{A.5}{44}{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }{table.caption.35}{}}
\newlabel{Tab:activations@cref}{{[table][5][2147483647,1]A.5}{[1][43][]44}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Summary of the final hyperparameters for both input data.\relax }}{44}{table.caption.36}\protected@file@percent }
\newlabel{Tab:Final}{{A.6}{44}{Summary of the final hyperparameters for both input data.\relax }{table.caption.36}{}}
\newlabel{Tab:Final@cref}{{[table][6][2147483647,1]A.6}{[1][44][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Five experiments over the different depth with $\hy  $ left and $\rare  $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }}{45}{figure.caption.37}\protected@file@percent }
\newlabel{Fig:Depth}{{A.2}{45}{Five experiments over the different depth with $\hy $ left and $\rare $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }{figure.caption.37}{}}
\newlabel{Fig:Depth@cref}{{[figure][2][2147483647,1]A.2}{[1][44][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Five experiments over different width with $\hy  $ left and $\rare  $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }}{46}{figure.caption.38}\protected@file@percent }
\newlabel{Fig:Width}{{A.3}{46}{Five experiments over different width with $\hy $ left and $\rare $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }{figure.caption.38}{}}
\newlabel{Fig:Width@cref}{{[figure][3][2147483647,1]A.3}{[1][44][]46}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{47}{figure.caption.39}\protected@file@percent }
\newlabel{Fig:batch}{{A.4}{47}{Five experiments over different batch sizes with $\hy $ left and $\rare $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }{figure.caption.39}{}}
\newlabel{Fig:batch@cref}{{[figure][4][2147483647,1]A.4}{[1][44][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Eight experiments with different combinations of activation functions for \(\hy  \) (left) and \(\rare  \) (right). Shown are training- and validation error over 5000 epochs.\relax }}{49}{figure.caption.41}\protected@file@percent }
\newlabel{Fig:Activations}{{A}{49}{Hyperparameters for the Fully Connected Autoencoder}{figure.caption.41}{}}
\newlabel{Fig:Activations@cref}{{[appendix][1][2147483647]A}{[1][49][]49}}
\citation{Carlberg}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Hyperparameters for the Convolutional Autoencoder}{50}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApB}{{B}{50}{Hyperparameters for the Convolutional Autoencoder}{appendix.B}{}}
\newlabel{Ch:ApB@cref}{{[appendix][2][2147483647]B}{[1][50][]50}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-4} at the 500th epoch as in \cref  {Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.\relax }}{50}{table.caption.42}\protected@file@percent }
\newlabel{Tab:small}{{B.1}{50}{Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-4} at the 500th epoch as in \cref {Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.\relax }{table.caption.42}{}}
\newlabel{Tab:small@cref}{{[table][1][2147483647,2]B.1}{[1][50][]50}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Training of five independent folds. Summary of minimum training- and minimum validation error for a small model with two convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }}{51}{table.caption.43}\protected@file@percent }
\newlabel{Tab:kFold}{{B.2}{51}{Training of five independent folds. Summary of minimum training- and minimum validation error for a small model with two convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }{table.caption.43}{}}
\newlabel{Tab:kFold@cref}{{[table][2][2147483647,2]B.2}{[1][50][]51}}
\newlabel{Fig:kFold}{{\caption@xref {Fig:kFold}{ on input line 213}}{52}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.44}{}}
\newlabel{Fig:kFold@cref}{{[appendix][2][2147483647]B}{[1][52][]52}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Training- and validation error over 1000 epochs for five independent folds.\relax }}{52}{figure.caption.44}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Architecture for the models with three (left) and four (right) convolutional layers in encoder and decoder. Kernel size and stride width are \(3\times 3\) for both features and models.\relax }}{53}{table.caption.45}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Results for increasing number of layers. Summary of minimum training- and minimum validation error for a model with three and a model with convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{53}{table.caption.46}\protected@file@percent }
\newlabel{Tab:Layer}{{B.4}{53}{Results for increasing number of layers. Summary of minimum training- and minimum validation error for a model with three and a model with convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.46}{}}
\newlabel{Tab:Layer@cref}{{[table][4][2147483647,2]B.4}{[1][52][]53}}
\newlabel{Fig:Layer}{{\caption@xref {Fig:Layer}{ on input line 283}}{53}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.47}{}}
\newlabel{Fig:Layer@cref}{{[appendix][2][2147483647]B}{[1][52][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Increasing the number of layers. Shown are training- and validation error over 2000 epochs a model with two and a model with three convolutional layers in the encoder. Channels are chosen to reach a maximum of 16 in the last layer of encoder. Kernel size and stride width are both \(3\times 3\).\relax }}{53}{figure.caption.47}\protected@file@percent }
\citation{Carlberg}
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces Increasing the batch size to four. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{54}{table.caption.48}\protected@file@percent }
\newlabel{Tab:Batch4}{{B.5}{54}{Increasing the batch size to four. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.48}{}}
\newlabel{Tab:Batch4@cref}{{[table][5][2147483647,2]B.5}{[1][52][]54}}
\newlabel{Fig:Batch}{{\caption@xref {Fig:Batch}{ on input line 306}}{54}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.49}{}}
\newlabel{Fig:Batch@cref}{{[appendix][2][2147483647]B}{[1][54][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Increasing the batch size to four. Shown are training- and validation error over 2000 epochs for the models with two, three and four convolutional layers in the encoder.\relax }}{54}{figure.caption.49}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.6}{\ignorespaces Increasing the channel growth rate to quadratic. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. Note that the channel sizes of the two layer model are only increased, the growth rate has already been quadratic.\relax }}{55}{table.caption.50}\protected@file@percent }
\newlabel{Tab:Channels}{{B.6}{55}{Increasing the channel growth rate to quadratic. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. Note that the channel sizes of the two layer model are only increased, the growth rate has already been quadratic.\relax }{table.caption.50}{}}
\newlabel{Tab:Channels@cref}{{[table][6][2147483647,2]B.6}{[1][54][]55}}
\newlabel{Fig:Channels}{{\caption@xref {Fig:Channels}{ on input line 329}}{55}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.51}{}}
\newlabel{Fig:Channels@cref}{{[appendix][2][2147483647]B}{[1][55][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Increasing the channel growth rate to quadratic. Shown are training- and validation error over 2000 epochs for the model with two, three and four convolutional layers in the encoder.\relax }}{55}{figure.caption.51}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.7}{\ignorespaces Variation of activations for hidden-/code layers. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{56}{table.caption.52}\protected@file@percent }
\newlabel{Tab:Activations}{{B.7}{56}{Variation of activations for hidden-/code layers. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.52}{}}
\newlabel{Tab:Activations@cref}{{[table][7][2147483647,2]B.7}{[1][55][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Rotated and flipped version of one original example from the dataset showing \(v\) over \(x\) and \(t\).\relax }}{56}{figure.caption.53}\protected@file@percent }
\newlabel{Fig:DataAug}{{B.5}{56}{Rotated and flipped version of one original example from the dataset showing \(v\) over \(x\) and \(t\).\relax }{figure.caption.53}{}}
\newlabel{Fig:DataAug@cref}{{[figure][5][2147483647,2]B.5}{[1][56][]56}}
\@writefile{lot}{\contentsline {table}{\numberline {B.8}{\ignorespaces Data augmentation with and without learning rate adjustement after the 1250th epoch. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{57}{table.caption.54}\protected@file@percent }
\newlabel{Tab:DataAug}{{B.8}{57}{Data augmentation with and without learning rate adjustement after the 1250th epoch. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.54}{}}
\newlabel{Tab:DataAug@cref}{{[table][8][2147483647,2]B.8}{[1][57][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Using augumented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }}{57}{figure.caption.55}\protected@file@percent }
\newlabel{Fig:DatAug}{{B.6}{57}{Using augumented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }{figure.caption.55}{}}
\newlabel{Fig:DatAug@cref}{{[figure][6][2147483647,2]B.6}{[1][57][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Learning rate adjustment with augmented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }}{57}{figure.caption.56}\protected@file@percent }
\newlabel{Fig:DatAugSched}{{B.7}{57}{Learning rate adjustment with augmented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }{figure.caption.56}{}}
\newlabel{Fig:DatAugSched@cref}{{[figure][7][2147483647,2]B.7}{[1][57][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Eight experiments with different combinations of activation functions for the model with two (left) and four (right) convolutional layers in encoder. Shown are training- and validation error over 2000 epochs.\relax }}{59}{figure.caption.58}\protected@file@percent }
\newlabel{Fig:ActivationsC}{{B.8}{59}{Eight experiments with different combinations of activation functions for the model with two (left) and four (right) convolutional layers in encoder. Shown are training- and validation error over 2000 epochs.\relax }{figure.caption.58}{}}
\newlabel{Fig:ActivationsC@cref}{{[figure][8][2147483647,2]B.8}{[1][59][]59}}
\@writefile{lot}{\contentsline {table}{\numberline {B.9}{\ignorespaces Final model.\relax }}{59}{table.caption.59}\protected@file@percent }
\newlabel{Tab:FinalC}{{B.9}{59}{Final model.\relax }{table.caption.59}{}}
\newlabel{Tab:FinalC@cref}{{[table][9][2147483647,2]B.9}{[1][59][]59}}
\bibdata{other/references}
\bibcite{BGK}{1}
\bibcite{rarefiedGDapplc}{2}
\bibcite{Carlberg}{3}
\bibcite{bukka2020assessment}{4}
\bibcite{Goodfellow}{5}
\bibcite{Rumelhart}{6}
\bibcite{Ballard}{7}
\bibcite{Rifai2011}{8}
\bibcite{kingma2014autoencoding}{9}
\bibcite{tolstikhin2019wasserstein}{10}
\bibcite{HochSchm97}{11}
\bibcite{vaswani2017attention}{12}
\bibcite{dosovitskiy2021image}{13}
\bibcite{reiss2018shifted}{14}
\bibcite{krah2020wavelet}{15}
\bibcite{Bernard}{16}
\bibcite{eivazi2021recurrent}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{60}{appendix*.60}\protected@file@percent }
\bibcite{NumaKUL}{18}
\bibcite{schaaf}{19}
\bibcite{puppo2019kinetic}{20}
\bibcite{Sod}{21}
\bibcite{CFD1}{22}
\bibcite{Kutz}{23}
\bibcite{LeCun98}{24}
\bibcite{he2015}{25}
\bibcite{torch_ini}{26}
\bibcite{bushaev_2017}{27}
\bibcite{kingma2017adam}{28}
\bibcite{lane_2018}{29}
\bibcite{divyanshu_2020}{30}
\bibcite{dumoulin2018guide}{31}
\bibcite{ohlberger2015reduced}{32}
\bibcite{fan2021interpretability}{33}
\ttl@finishall
