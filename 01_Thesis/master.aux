\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\@input{front/title.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Contents}{i}{chapter*.1}\protected@file@percent }
\@input{chapters/introduction.aux}
\@input{chapters/BGK_model.aux}
\@input{chapters/Deep_Learning.aux}
\@input{chapters/Reduced_order_Modeling.aux}
\@input{chapters/Results.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Hyperparameters for the Fully Connected Autoencoder}{39}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApA}{{A}{39}{Hyperparameters for the Fully Connected Autoencoder}{appendix.A}{}}
\newlabel{Ch:ApA@cref}{{[appendix][1][2147483647]A}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces \footnotesize  Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }}{39}{figure.caption.30}\protected@file@percent }
\newlabel{Fig:ActScheme}{{A.1}{39}{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.\relax }{figure.caption.30}{}}
\newlabel{Fig:ActScheme@cref}{{[figure][1][2147483647,1]A.1}{[1][39][]39}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }}{39}{table.caption.31}\protected@file@percent }
\newlabel{Tab:First Guess}{{A.1}{39}{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.\relax }{table.caption.31}{}}
\newlabel{Tab:First Guess@cref}{{[table][1][2147483647,1]A.1}{[1][39][]39}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }}{40}{table.caption.32}\protected@file@percent }
\newlabel{Tab:Depth}{{A.2}{40}{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.32}{}}
\newlabel{Tab:Depth@cref}{{[table][2][2147483647,1]A.2}{[1][40][]40}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }}{41}{table.caption.33}\protected@file@percent }
\newlabel{Tab:Width}{{A.3}{41}{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima where reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.33}{}}
\newlabel{Tab:Width@cref}{{[table][3][2147483647,1]A.3}{[1][40][]41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }}{42}{table.caption.34}\protected@file@percent }
\newlabel{Tab:Batch}{{A.4}{42}{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L 2\) is also given but evaluated with the model at the last epoch.\relax }{table.caption.34}{}}
\newlabel{Tab:Batch@cref}{{[table][4][2147483647,1]A.4}{[1][40][]42}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }}{43}{table.caption.35}\protected@file@percent }
\newlabel{Tab:activations}{{A.5}{43}{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }{table.caption.35}{}}
\newlabel{Tab:activations@cref}{{[table][5][2147483647,1]A.5}{[1][42][]43}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Summary of the final hyperparameters for both input data.\relax }}{43}{table.caption.36}\protected@file@percent }
\newlabel{Tab:Final}{{A.6}{43}{Summary of the final hyperparameters for both input data.\relax }{table.caption.36}{}}
\newlabel{Tab:Final@cref}{{[table][6][2147483647,1]A.6}{[1][43][]43}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Five experiments over the different depth with $\hy  $ left and $\rare  $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }}{44}{figure.caption.37}\protected@file@percent }
\newlabel{Fig:Depth}{{A.2}{44}{Five experiments over the different depth with $\hy $ left and $\rare $ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.\relax }{figure.caption.37}{}}
\newlabel{Fig:Depth@cref}{{[figure][2][2147483647,1]A.2}{[1][43][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Five experiments over different width with $\hy  $ left and $\rare  $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }}{45}{figure.caption.38}\protected@file@percent }
\newlabel{Fig:Width}{{A.3}{45}{Five experiments over different width with $\hy $ left and $\rare $ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.\relax }{figure.caption.38}{}}
\newlabel{Fig:Width@cref}{{[figure][3][2147483647,1]A.3}{[1][43][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{46}{figure.caption.39}\protected@file@percent }
\newlabel{Fig:batch}{{A.4}{46}{Five experiments over different batch sizes with $\hy $ left and $\rare $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }{figure.caption.39}{}}
\newlabel{Fig:batch@cref}{{[figure][4][2147483647,1]A.4}{[1][43][]46}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{48}{figure.caption.41}\protected@file@percent }
\newlabel{Fig:Activations}{{A}{48}{Hyperparameters for the Fully Connected Autoencoder}{figure.caption.41}{}}
\newlabel{Fig:Activations@cref}{{[appendix][1][2147483647]A}{[1][48][]48}}
\citation{Carlberg}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Hyperparameters for the Convolutional Autoencoder}{49}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApB}{{B}{49}{Hyperparameters for the Convolutional Autoencoder}{appendix.B}{}}
\newlabel{Ch:ApB@cref}{{[appendix][2][2147483647]B}{[1][49][]49}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-4} at the 500th epoch as in \cref  {Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.\relax }}{49}{table.caption.42}\protected@file@percent }
\newlabel{Tab:small}{{B.1}{49}{Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-4} at the 500th epoch as in \cref {Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.\relax }{table.caption.42}{}}
\newlabel{Tab:small@cref}{{[table][1][2147483647,2]B.1}{[1][49][]49}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Results for the k-fold experiments. Minimum values of training- and validation error as well as the epoch of the minimum validation error and the \(\L 2\) are given. The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }}{50}{table.caption.43}\protected@file@percent }
\newlabel{Tab:kFold}{{B.2}{50}{Results for the k-fold experiments. Minimum values of training- and validation error as well as the epoch of the minimum validation error and the \(\L 2\) are given. The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }{table.caption.43}{}}
\newlabel{Tab:kFold@cref}{{[table][2][2147483647,2]B.2}{[1][49][]50}}
\newlabel{Fig:kFold}{{\caption@xref {Fig:kFold}{ on input line 213}}{51}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.44}{}}
\newlabel{Fig:kFold@cref}{{[appendix][2][2147483647]B}{[1][51][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Training- and validation error over 1000 epochs for five independent folds.\relax }}{51}{figure.caption.44}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces \relax }}{51}{table.caption.46}\protected@file@percent }
\newlabel{Tab:Layer}{{B.4}{51}{\relax }{table.caption.46}{}}
\newlabel{Tab:Layer@cref}{{[table][4][2147483647,2]B.4}{[1][51][]51}}
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Architecture for the models with three (left) and four (ricght) convolutional layers in encoder and decoder. Kernel size and stride width are \(3\times 3\) for both features and models.\relax }}{52}{table.caption.45}\protected@file@percent }
\newlabel{Fig:Layer}{{\caption@xref {Fig:Layer}{ on input line 283}}{52}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.47}{}}
\newlabel{Fig:Layer@cref}{{[appendix][2][2147483647]B}{[1][51][]52}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Training- and validation error over 2000 epochs for two models with three and four convolutional layers. Channels are chosen to reach a maximum of 16 in the last layer of encoder. Kernel size and stride width are both \(3\times 3\).\relax }}{52}{figure.caption.47}\protected@file@percent }
\newlabel{Fig:Batch}{{\caption@xref {Fig:Batch}{ on input line 290}}{53}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.48}{}}
\newlabel{Fig:Batch@cref}{{[appendix][2][2147483647]B}{[1][52][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Training- and validation error over 2000 epochs for the small model with two convolutional layers in encoder and decoder as well as the models with two three and four convolutional layers. All three models are trained with a batch size of 4.\relax }}{53}{figure.caption.48}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces \relax }}{53}{table.caption.49}\protected@file@percent }
\newlabel{Tab:Batch4}{{B.5}{53}{\relax }{table.caption.49}{}}
\newlabel{Tab:Batch4@cref}{{[table][5][2147483647,2]B.5}{[1][53][]53}}
\newlabel{Fig:Channels}{{\caption@xref {Fig:Channels}{ on input line 314}}{54}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.50}{}}
\newlabel{Fig:Channels@cref}{{[appendix][2][2147483647]B}{[1][53][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Training- and validation error over 2000 epochs for the small model with two convolutional layers in encoder and decoder as well as the models with two three and four convolutional layers. All three models are trained with a batch size of 4.\relax }}{54}{figure.caption.50}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.6}{\ignorespaces \relax }}{54}{table.caption.51}\protected@file@percent }
\newlabel{Tab:Channels}{{B.6}{54}{\relax }{table.caption.51}{}}
\newlabel{Tab:Channels@cref}{{[table][6][2147483647,2]B.6}{[1][54][]54}}
\bibdata{other/references}
\bibcite{BGK}{1}
\bibcite{Carlberg}{2}
\bibcite{bukka2020assessment}{3}
\bibcite{Franz}{4}
\bibcite{Kutz}{5}
\bibcite{Bernard}{6}
\bibcite{Goodfellow}{7}
\bibcite{Rumelhart}{8}
\bibcite{Ballard}{9}
\bibcite{Rifai2011}{10}
\bibcite{Rifai_2011a}{11}
\bibcite{rifai2012generative}{12}
\bibcite{schaaf}{13}
\bibcite{puppo2019kinetic}{14}
\bibcite{Sod}{15}
\bibcite{CFD1}{16}
\bibcite{LeCun98}{17}
\bibcite{he2015}{18}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{55}{appendix*.52}\protected@file@percent }
\bibcite{torch_ini}{19}
\bibcite{bushaev_2017}{20}
\bibcite{kingma2017adam}{21}
\bibcite{lane_2018}{22}
\bibcite{divyanshu_2020}{23}
\bibcite{dumoulin2018guide}{24}
\bibcite{ohlberger2015reduced}{25}
\bibcite{NumaKUL}{26}
\ttl@finishall
