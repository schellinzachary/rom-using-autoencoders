\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\@input{front/title.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Contents}{i}{chapter*.1}\protected@file@percent }
\@input{chapters/introduction.aux}
\@input{chapters/BGK_model.aux}
\@input{chapters/Deep_Learning.aux}
\@input{chapters/Reduced_order_Modeling.aux}
\@input{chapters/Results.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Hyperparameters for the Fully Connected Autoencoder}{41}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApA}{{A}{41}{Hyperparameters for the Fully Connected Autoencoder}{appendix.A}{}}
\newlabel{Ch:ApA@cref}{{[appendix][1][2147483647]A}{[1][41][]41}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Scheme of a network with four layers showing the location of the activations within the network. Activation act. a activates the output of the input layer and the output of any other hidden layer besides the output of the bottleneck layer. Act. b activates the bottleneck layer.\relax }}{41}{figure.caption.32}\protected@file@percent }
\newlabel{Fig:ActScheme}{{A.1}{41}{Scheme of a network with four layers showing the location of the activations within the network. Activation act. a activates the output of the input layer and the output of any other hidden layer besides the output of the bottleneck layer. Act. b activates the bottleneck layer.\relax }{figure.caption.32}{}}
\newlabel{Fig:ActScheme@cref}{{[figure][1][2147483647,1]A.1}{[1][41][]41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces The initial selection of batch size, bottleneck size, number of epochs, learning rate, and applied activation functions.\relax }}{41}{table.caption.33}\protected@file@percent }
\newlabel{Tab:First Guess}{{A.1}{41}{The initial selection of batch size, bottleneck size, number of epochs, learning rate, and applied activation functions.\relax }{table.caption.33}{}}
\newlabel{Tab:First Guess@cref}{{[table][1][2147483647,1]A.1}{[1][41][]41}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima were reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }}{42}{table.caption.34}\protected@file@percent }
\newlabel{Tab:Depth}{{A.2}{42}{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L 2\). The minima were reached around the last 50 epochs of the training. The \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.34}{}}
\newlabel{Tab:Depth@cref}{{[table][2][2147483647,1]A.2}{[1][42][]42}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima were reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }}{43}{table.caption.35}\protected@file@percent }
\newlabel{Tab:Width}{{A.3}{43}{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L 2\). The minima were reached around the last 50 epochs of the training, the \(\L 2\) is evaluated with the model at the last epoch.\relax }{table.caption.35}{}}
\newlabel{Tab:Width@cref}{{[table][3][2147483647,1]A.3}{[1][43][]43}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Results for the variation of batch sizes. Given is the minimum value of validation error as well as the corresponding epoch. Additionally, the \(\L 2\) is given but evaluated with the model at the last epoch.\relax }}{44}{table.caption.36}\protected@file@percent }
\newlabel{Tab:Batch}{{A.4}{44}{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the corresponding epoch. Additionally, the \(\L 2\) is given but evaluated with the model at the last epoch.\relax }{table.caption.36}{}}
\newlabel{Tab:Batch@cref}{{[table][4][2147483647,1]A.4}{[1][43][]44}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }}{45}{table.caption.37}\protected@file@percent }
\newlabel{Tab:activations}{{A.5}{45}{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the corresponding epoch and the \(\L 2\). The \(\L 2\) is evaluated with the models saved when the minimum validation error was achieved during training.\relax }{table.caption.37}{}}
\newlabel{Tab:activations@cref}{{[table][5][2147483647,1]A.5}{[1][44][]45}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Summary of the final hyperparameters for both input data.\relax }}{45}{table.caption.38}\protected@file@percent }
\newlabel{Tab:Final}{{A.6}{45}{Summary of the final hyperparameters for both input data.\relax }{table.caption.38}{}}
\newlabel{Tab:Final@cref}{{[table][6][2147483647,1]A.6}{[1][45][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Five experiments over the different depths with $\hy  $ left and $\rare  $ right. The number of layers used for every experiment is given. Training and validation loss are shown over 2000 epochs.\relax }}{46}{figure.caption.39}\protected@file@percent }
\newlabel{Fig:Depth}{{A.2}{46}{Five experiments over the different depths with $\hy $ left and $\rare $ right. The number of layers used for every experiment is given. Training and validation loss are shown over 2000 epochs.\relax }{figure.caption.39}{}}
\newlabel{Fig:Depth@cref}{{[figure][2][2147483647,1]A.2}{[1][45][]46}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Five experiments over different widths with $\hy  $ left and $\rare  $ right. The number of nodes used for every experiment is given. Training and validation loss are shown over 4000 epochs.\relax }}{47}{figure.caption.40}\protected@file@percent }
\newlabel{Fig:Width}{{A.3}{47}{Five experiments over different widths with $\hy $ left and $\rare $ right. The number of nodes used for every experiment is given. Training and validation loss are shown over 4000 epochs.\relax }{figure.caption.40}{}}
\newlabel{Fig:Width@cref}{{[figure][3][2147483647,1]A.3}{[1][45][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Five experiments over different batch sizes with $\hy  $ left and $\rare  $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }}{48}{figure.caption.41}\protected@file@percent }
\newlabel{Fig:batch}{{A.4}{48}{Five experiments over different batch sizes with $\hy $ left and $\rare $ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.\relax }{figure.caption.41}{}}
\newlabel{Fig:batch@cref}{{[figure][4][2147483647,1]A.4}{[1][45][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Eight experiments with different combinations of activation functions for \(\hy  \) (left) and \(\rare  \) (right). Shown are training- and validation error over 5000 epochs.\relax }}{50}{figure.caption.43}\protected@file@percent }
\newlabel{Fig:Activations}{{A}{50}{Hyperparameters for the Fully Connected Autoencoder}{figure.caption.43}{}}
\newlabel{Fig:Activations@cref}{{[appendix][1][2147483647]A}{[1][50][]50}}
\citation{Carlberg}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Hyperparameters for the Convolutional Autoencoder}{51}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:ApB}{{B}{51}{Hyperparameters for the Convolutional Autoencoder}{appendix.B}{}}
\newlabel{Ch:ApB@cref}{{[appendix][2][2147483647]B}{[1][51][]51}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces The initial selection of hyperparameters comprises 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-5} at the 500th epoch as in \cref  {Ch:ApA}. Depth counts the number of convolutional/transposed convolutional layers and excludes the fully connected layers.\relax }}{51}{table.caption.44}\protected@file@percent }
\newlabel{Tab:small}{{B.1}{51}{The initial selection of hyperparameters comprises 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num {e-4} to \num {e-5} at the 500th epoch as in \cref {Ch:ApA}. Depth counts the number of convolutional/transposed convolutional layers and excludes the fully connected layers.\relax }{table.caption.44}{}}
\newlabel{Tab:small@cref}{{[table][1][2147483647,2]B.1}{[1][51][]51}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Training of five independent folds. Summary of minimum training- and minimum validation error for a small model with two convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }}{52}{table.caption.45}\protected@file@percent }
\newlabel{Tab:kFold}{{B.2}{52}{Training of five independent folds. Summary of minimum training- and minimum validation error for a small model with two convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. The mean and standard deviation of the validation error are \num {3.55e-5} and \num {2.13e-5} respectively with a corresponding variance of \num {4.56e-10}.\relax }{table.caption.45}{}}
\newlabel{Tab:kFold@cref}{{[table][2][2147483647,2]B.2}{[1][51][]52}}
\newlabel{Fig:kFold}{{\caption@xref {Fig:kFold}{ on input line 217}}{53}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.46}{}}
\newlabel{Fig:kFold@cref}{{[appendix][2][2147483647]B}{[1][53][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Training- and validation error over 1000 epochs for five independent folds.\relax }}{53}{figure.caption.46}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Architecture for the models with three (left) and four (right) convolutional layers in encoder and decoder. Kernel size and stride width are \(3\times 3\) for both features and models.\relax }}{54}{table.caption.47}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Results for increasing the number of layers. Summary of minimum training- and minimum validation error for a model with three and a model with convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{54}{table.caption.48}\protected@file@percent }
\newlabel{Tab:Layer}{{B.4}{54}{Results for increasing the number of layers. Summary of minimum training- and minimum validation error for a model with three and a model with convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.48}{}}
\newlabel{Tab:Layer@cref}{{[table][4][2147483647,2]B.4}{[1][53][]54}}
\newlabel{Fig:Layer}{{\caption@xref {Fig:Layer}{ on input line 287}}{54}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.49}{}}
\newlabel{Fig:Layer@cref}{{[appendix][2][2147483647]B}{[1][53][]54}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Increasing the number of layers. Shown are training- and validation errors over 2000 epochs a model with two and a model with three convolutional layers in the encoder. Channels are chosen to reach a maximum of 16 in the last layer of the encoder. Kernel size and stride width are both \(3\times 3\).\relax }}{54}{figure.caption.49}\protected@file@percent }
\citation{Carlberg}
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces Increasing the batch size to four. Summary of minimum training- and minimum validation error for the models with two, three, and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{55}{table.caption.50}\protected@file@percent }
\newlabel{Tab:Batch4}{{B.5}{55}{Increasing the batch size to four. Summary of minimum training- and minimum validation error for the models with two, three, and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.50}{}}
\newlabel{Tab:Batch4@cref}{{[table][5][2147483647,2]B.5}{[1][53][]55}}
\newlabel{Fig:Batch}{{\caption@xref {Fig:Batch}{ on input line 310}}{55}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.51}{}}
\newlabel{Fig:Batch@cref}{{[appendix][2][2147483647]B}{[1][55][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Increasing the batch size to four. Shown are training- and validation error over 2000 epochs for the models with two, three, and four convolutional layers in the encoder.\relax }}{55}{figure.caption.51}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.6}{\ignorespaces Increasing the channel growth rate to quadratic. Summary of minimum training- and minimum validation error for the models with two, three, and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. Note that the channel sizes of the two-layer model are only increased, the growth rate has already been quadratic.\relax }}{56}{table.caption.52}\protected@file@percent }
\newlabel{Tab:Channels}{{B.6}{56}{Increasing the channel growth rate to quadratic. Summary of minimum training- and minimum validation error for the models with two, three, and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached. Note that the channel sizes of the two-layer model are only increased, the growth rate has already been quadratic.\relax }{table.caption.52}{}}
\newlabel{Tab:Channels@cref}{{[table][6][2147483647,2]B.6}{[1][55][]56}}
\newlabel{Fig:Channels}{{\caption@xref {Fig:Channels}{ on input line 333}}{56}{Hyperparameters for the Convolutional Autoencoder}{figure.caption.53}{}}
\newlabel{Fig:Channels@cref}{{[appendix][2][2147483647]B}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Increasing the channel growth rate to quadratic. Shown are training- and validation errors over 2000 epochs for the model with two, three, and four convolutional layers in the encoder.\relax }}{56}{figure.caption.53}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.7}{\ignorespaces Variation of activations for hidden-/code layers. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{57}{table.caption.54}\protected@file@percent }
\newlabel{Tab:Activations}{{B.7}{57}{Variation of activations for hidden-/code layers. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.54}{}}
\newlabel{Tab:Activations@cref}{{[table][7][2147483647,2]B.7}{[1][56][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Rotated and flipped version of one original example from the dataset showing \(v\) over \(x\) and \(t\).\relax }}{57}{figure.caption.55}\protected@file@percent }
\newlabel{Fig:DataAug}{{B.5}{57}{Rotated and flipped version of one original example from the dataset showing \(v\) over \(x\) and \(t\).\relax }{figure.caption.55}{}}
\newlabel{Fig:DataAug@cref}{{[figure][5][2147483647,2]B.5}{[1][57][]57}}
\@writefile{lot}{\contentsline {table}{\numberline {B.8}{\ignorespaces Data augmentation with and without learning rate adjustment after the 1250th epoch. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder and the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }}{58}{table.caption.56}\protected@file@percent }
\newlabel{Tab:DataAug}{{B.8}{58}{Data augmentation with and without learning rate adjustment after the 1250th epoch. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder and the corresponding \(\L 2\) and the epoch in which those values are reached.\relax }{table.caption.56}{}}
\newlabel{Tab:DataAug@cref}{{[table][8][2147483647,2]B.8}{[1][58][]58}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Using augmented data. Shown are training- and validation errors over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }}{58}{figure.caption.57}\protected@file@percent }
\newlabel{Fig:DatAug}{{B.6}{58}{Using augmented data. Shown are training- and validation errors over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }{figure.caption.57}{}}
\newlabel{Fig:DatAug@cref}{{[figure][6][2147483647,2]B.6}{[1][58][]58}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Learning rate adjustment with augmented data. Shown are training- and validation errors over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }}{58}{figure.caption.58}\protected@file@percent }
\newlabel{Fig:DatAugSched}{{B.7}{58}{Learning rate adjustment with augmented data. Shown are training- and validation errors over 2000 epochs for the model with two and four convolutional layers in the encoder.\relax }{figure.caption.58}{}}
\newlabel{Fig:DatAugSched@cref}{{[figure][7][2147483647,2]B.7}{[1][58][]58}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Eight experiments with different combinations of activation functions for the model with two (left) and four (right) convolutional layers in the encoder. Shown are training- and validation errors over 2000 epochs.\relax }}{60}{figure.caption.60}\protected@file@percent }
\newlabel{Fig:ActivationsC}{{B.8}{60}{Eight experiments with different combinations of activation functions for the model with two (left) and four (right) convolutional layers in the encoder. Shown are training- and validation errors over 2000 epochs.\relax }{figure.caption.60}{}}
\newlabel{Fig:ActivationsC@cref}{{[figure][8][2147483647,2]B.8}{[1][60][]60}}
\@writefile{lot}{\contentsline {table}{\numberline {B.9}{\ignorespaces Final model.\relax }}{60}{table.caption.61}\protected@file@percent }
\newlabel{Tab:FinalC}{{B.9}{60}{Final model.\relax }{table.caption.61}{}}
\newlabel{Tab:FinalC@cref}{{[table][9][2147483647,2]B.9}{[1][60][]60}}
\bibdata{other/references}
\bibcite{BGK}{1}
\bibcite{rarefiedGDapplc}{2}
\bibcite{Carlberg}{3}
\bibcite{bukka2020assessment}{4}
\bibcite{Goodfellow}{5}
\bibcite{Rumelhart}{6}
\bibcite{Ballard}{7}
\bibcite{Rifai2011}{8}
\bibcite{kingma2014autoencoding}{9}
\bibcite{tolstikhin2019wasserstein}{10}
\bibcite{HochSchm97}{11}
\bibcite{vaswani2017attention}{12}
\bibcite{dosovitskiy2021image}{13}
\bibcite{reiss2018shifted}{14}
\bibcite{krah2020wavelet}{15}
\bibcite{Bernard}{16}
\bibcite{eivazi2021recurrent}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{61}{appendix*.62}\protected@file@percent }
\bibcite{NumaKUL}{18}
\bibcite{schaaf}{19}
\bibcite{puppo2019kinetic}{20}
\bibcite{Sod}{21}
\bibcite{CFD1}{22}
\bibcite{Kutz}{23}
\bibcite{LeCun98}{24}
\bibcite{he2015}{25}
\bibcite{kingma2017adam}{26}
\bibcite{bushaev_2017}{27}
\bibcite{lane_2018}{28}
\bibcite{divyanshu_2020}{29}
\bibcite{dumoulin2018guide}{30}
\bibcite{ohlberger2015reduced}{31}
\bibcite{fan2021interpretability}{32}
\ttl@finishall
