% !TEX root = master.tex


\begin{center}
	{\sffamily \bfseries\Large Appendix}\\
\end{center}%\hspace*{\fill}\\[1.5cm]
\vspace{1cm}
\chapter{Hyperparameters for the Fully Connected Autoencoder}
\label{Ch:ApA}
%\pagenumbering{arabic}

The finding of appropriate hyperparameters for the fully connected autoencoder is described here. The hyperparameters include number of layers i.e. depth, number of nodes per hidden layer i.e. width, batch size and non-linear activation functions, number of epochs for training and learning rate. The experiments are evaluated through the validation error which estimates the model's ability to generalize, the training error which estimates the optimization to training data and the $\L2$ as described in \cref{Ch:ROM}. Both validation- and training error are described in \cref{Ch:DimRedAl} and provide information about under- and overfitting. Moreover the validation error is the essential metric for validating a model's performance. The $\L2$ on the other hand gives an estimate of how well the model performs on the whole dataset hence is used as a comparative metric against POD.\\ 
To start with a working model first a guess about the some initial hyperparameters is done, which are summarized in \Cref{Tab:First Guess}. These include a mini-batch size of 16, the width of the bottleneck layer is 3 and 5  for \(\hy\) and \(\rare\) respectively and a learning rate of 0.0001. Activation LeakyReLU is applied for the outputs of the input- and  any hidden layer which does not output the code, referred to as activations hidden. Tanh is applied for the output of the last hidden layer in the encoder which outputs the code, referred to as activation code. A visualization of the activation scheme is provided in \cref{Fig:ActScheme}. Moreover 2000 initial number of epochs are used. This might seem exaggerated but is justified by the little amount of input data and the small size of the network which yiels a fast training.
\begin{center}
	\begin{figure}[H]
		\centering
		\input{Figures/Parameterstudy/Fully_Connected/Activations/act_scheme.tex}
		\caption{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.}
		\label{Fig:ActScheme}
		\end{figure}
\end{center}
\begin{table}[H]
	\centering
	\caption{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Mini-batch size   & Intrinsic dimensions &   Epochs &Learning rate & Activations hidden/code \\   
		\hline
		16 		&	3/5 &     2000&	    0.0001 & LeakyReLU/Tanh\\
		\bottomrule
	\end{tabular*} \label{Tab:First Guess}
\end{table}
Five designs for finding an optimal number of layers i.e. depth are run. The hidden layers are designed to halve the input at each step. Not within this scope is the bottlenck layer which has a fixed size of 5 and 3 for $\rare$ and $\hy$ respectively and the first and last hidden layer (after the input layer and before the output layer). Those should provide an abstraction without shrinking the incoming data. Note that this design feature was validated in an initial exploration cycle which is not included in this thesis. The five designs range from 10 layers in (a) to 2 layers in (e) always taking one layer away from encoder and decoder hence decreasing by two layers and are as follows:     
\begin{enumerate}
	\item 10 layers with layer widths: 40, 40, 20, 10, 5, 3/5, 5, 10, 20, 40, 40.
	\item 8 layers with layer widths: 40, 40, 20 , 10, 3/5, 10, 20, 40, 40.
	\item 6 layers with layer widths: 40, 40, 20 , 3/5, 20, 40, 40.
	\item 4 layers with layer widths: 40, 40, 3/5, 40, 40.
	\item 2 layers with layer widths: 40, 3/5, 40.
	\end{enumerate}
The model's depth is determined in a primary step, as it sets a consequential part of the model's representational capacity and therefore can initiate over- and underfitting at an early stage in the hyperparameter search. The results of the experimentation are shown in \cref{Fig:Depth} and \cref{Tab:Depth} for both rarefaction levels.\\
\begin{table}[htp]
	\centering
	\caption{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training. The \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Depth & \multicolumn{2}{c}{Minimum training error} & \multicolumn{2}{c}{Minimum validation error} & \multicolumn{2}{c}{\(\L2\) }\\ [.5ex]
		 & \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		10& \num{1.53e-7} & \num{5.96e-7} & \num{2.22e-7} & \num{5.19e-7} & 0.0048 & 0.0091\\ \hline
		8 & \num{1.17e-7 }& \num{2.05e-7} & \num{1.58e-7} & \num{2.32e-7} & 0.0041 & 0.0054\\ \hline
		6 & \num{9.76e-8} & \num{1.40e-7} & \num{1.49e-7} & \num{1.72e-7} & 0.0038 & 0.0045\\ \hline
		4 & \num{6.29e-8} & \num{1.52e-7} & \num{7.74e-8} & \num{1.61e-7} & 0.0031 & 0.0048 \\ \hline
		2 & \num{1.29e-6} & \num{3.29e-6} & \num{1.37e-6} & \num{3.42e-6} & 0.0136 & 0.0217\\ \hline
	\end{tabular*}\label{Tab:Depth}
\end{table}\noindent
For \(\hy\) the lowest validation error of \num{7.74e-8} and an \(\L2\) of 0.0031 is reached with 4 layers, hence constitutes the best performing design out of the five. Additionally, as seen in \cref{Fig:Depth}(left), does a design exceeding 4 layers results in a slight overfitting after around 500 epochs. Less than 4 layers do not reach the validation error and \(\L2\) of the other designs, yielding the conclusion, that the capacity is too low. Overfitting occurs with 4 layers only after the 1000th epoch and is less than with the other three models that show overfitting.\\
For \(\rare\) the lowest validation error of \num{1.61e-7} is reached again with 4 layers. On the other hand the lowest \(\L2\) of 0.0031 and the lowest training error of \num{1.40e-7} are reached with 6 layers. Contrary to the afore discussed case the training error and \(\L2\) are of lower magnitude for 6 layers, except for the validation error. Looking at \cref{Fig:Depth}(right), we observe that the model with 6 layers starts to overfit after the 1500 epochs, yielding a decreasing training error and a stagnating validation error. Hence the model improved in the optimization task which additionally improves the \(\L2\). It's generalization ability, measured by the validation error, did not improve and is greater than the validation error reached with 4 layers. This concludes, a model with 4 layers constitutes the best performing design out of the five.\\
Qualitatively the overall training for both rarefaction levels is very stable. Training and validation error do not diverge exessively and converge early in training. Separation of training and validation error occurs prominently for the hydrodynamic solution. This is thought to be connected to the emersion of sharp shock fronts towards the end of the simulation. This increases variance in the whole dataset and therefore also in the training- and validation set.\\
The number of epochs is now doubled to 4000 epochs because the lowest validation error was achieved towards the end of the training in the previous experiments. Again this is justifiable as one epoch takes less than 1s to finish and no prominent overfitting is observed. 
\begin{table}[htpb!]
	\centering
	\caption{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training, the \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Hidden units & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} & \multicolumn{2}{c}{Shrinkage factor}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		50 & \num{1.91e-8}  & \num{5.05e-8} & \num{0.0015}  & \num{0.0025} & 0.06  & 0.01\\ \hline
		40 & \num{2.65e-08} & \num{1.65e-8} & \num{0.0018}  & \num{0.0014} & 0.075 & 0.125\\ \hline
		30 & \num{1.77e-08} & \num{3.40e-8} & \num{0.0015}  & \num{0.0021} & 0.015 & 0.0167\\ \hline
		20 & \num{2.50e-08} & \num{5.25e-8} & \num{0.0017}  & \num{0.0027} & 0.1   & 0.25 \\ \hline
		10 & \num{5.11e-08} & \num{3.97e-7} & \num{0.0025}  & \num{0.0077} & 0.3   & 0.5\\ \hline
	\end{tabular*}\label{Tab:Width}
\end{table}
The width of the two remaining hidden layers is examined in the following. For both the hydrodynamic and the rarefied regime five experiments are conducted, lowering the hidden units of the hidden layers from fifty to ten. Note that the decoder is chosen to be structurally a reflection of the encoder. Therefore only one parameter is changed. Results for \(\hy\) and \(\rare\) are shown in \cref{Tab:Width}. Note that the contribution of over- and underfitting is negligible and therefore the training error is omitted. A model with 30 hidden units in encoder and decoder performs best with \(\hy\) and reaches a validation error of \(\num{1.77e-08}\). The corresponding \(\L2 = \num{1.5e-3}\) with a shrinkage factor of 0.015. Overall the loss of each experiment with \(\hy\) is quiet similar an ranges from \(\num{1.77e-8}\) to \(\num{5.11e-8}\). The \(\L2\) behaves in a similar fashion and is even equal for 50 and 30 layers. A model with 40 hidden units performs best for \(\rare\). The corresponding validation error is \(\num{1.65e-8}\) with \(\L2=\num{1.4e-3}\), which is smaller than \(\hy\). The shrinkage factor is 0.125. In all experiments a model with 10 hidden nodes performs worst. Training and validation error over all 4000 epochs for both experiments can be seen in \cref{Fig:Width}. The aforementioned separation of training- and validation error, that was observed solely for \(\hy\), is mitigated when moving away from 40 hidden units for encoder and decoder. Not shrinking the input in the first hidden layer only serves the performance when using \(\rare\).\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L2\) is also given but evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Batch Size & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		32& \num{5.40e-8} & \num{2.17e-8} & \num{0.0024}  & \num{0.0017}&4998&4992\\ \hline
		16& \num{1.95e-8} & \num{2.06e-8} & \num{0.0015}  & \num{0.0016}&4999&5000\\ \hline
		8 & \num{2.25e-8} & \num{1.03e-8} & \num{0.0017}  & \num{0.0012}&4965&4961\\ \hline
		4 & \num{1.52e-8} & \num{6.30e-9} & \num{0.0013}  & \num{0.0010}&3956&4534\\ \hline
		2 & \num{1.15e-8} & \num{9.18e-9} & \num{0.0012}  & \num{0.0013}&4956&4872\\ \hline
	\end{tabular*}\label{Tab:Batch}
\end{table}
Next the mini-batch size is analysed. Epochs are increased by 1000 epochs, as training- and validation error show potential to decrease further as seen in \cref{Fig:Width} for \(\rare\) with 40 hidden nodes. Results for $\hy$  and  $\rare$ are displayed in \cref{Tab:Batch}. Experiments are conducted with mini-batch sizes of 2, 4, 8, 16, 32. Batch sizes to the power of 2 are typically chosen as to fully exploit computational capabilities of a GPU i.e. aligning the batch size with the way memory is structured within a GPU. The smallest batch size of 2 yields the lowest validation error of \(\num{1.15e-8}\) with corresponding \(\L2 = 0.0012\) at epoch 4956 for \(\hy\). The lowest validation error with \(\num{6.30e-9}\) is achieved for \(\rare\) at epoch 4534 with a batch size of 4. The corresponding \(\L2 = 0.001\). Compared to a batch size of 16 in the previous experiments we can observe that small batch sizes have a regularizing effect on training as described in \cref{Ch:DimRedAl} and therefore are beneficial to generalization. At the same time, the lower the batch sizes are, the more unstable is the training as seen in \cref{Fig:Batch}. The oscillations which begin with batch sizes of 8 and lower, which make the training unstable, can be battled with a lower learning rate as soon as training starts to tremble. Additionally small batch sizes drastically increase training time which is why a batch size as low as 2 will not be used for the next experiments. In conclusion a batch size of 2 is omitted and for both input data a batch size of 4 is chosen. Furthermore a reduction of the learning rate from \(\num{1e-4}\) to \(\num{1e-5}\) is applied after the 3000th epoch.\\
Eight experiments with different activation functions namely ReLU, ELU, Tanh, SiLU and LeakyReLU are performed. The experiment designs and results are given in \cref{Tab:activations} for hidden and code activations. With \(\hy\) a combination of ELU and ELU for hidden and code activation yields the best results in validation error with \num{4.44e-9} and a corresponding \(\L2 = 0.0008\). These values are achieved at the last epoch. For \(\rare\) a combination of ReLU and ReLU for hidden and code activation produces a validation error of \num{7.18e-9} and a corresponding \(\L2 = 0.0009\). Both values are also reached close to the last epoch. Note that all models reach their lowest loss at or very close to the last epoch. The reason is the stable training after the 3000th epoch, where the learning rate is lowered to \num{1e-5} as seen in \cref{Fig:Activations}. This measure shows in all experiments an immediate success for learning. Both validation and training error fall at the 3001st epoch and only decrease slightly thereafter. This behavior clearly shows that the updates to the free parameters \(\frepar\) were too big which prohibitively slowed down or even prevented the learning process. Small updates to \(\frepar\) made all models quickly reach a minimum. Therefore 3000 epochs or even less could have been enough to reach similar results with a lower learning rate.\\
If the learning rate could have been reduced whilst producing similar results remains unanswered here as the results are satisfactory. Note that for \(\rare\) in the previous experiment a validation error of \num{6.30e-9} was achieved which is slightly lower than the current result. Nonetheless it is decided to take the current model as the final result.\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L2\). The \(\L2\) is evaluated with the models saved when the minimum validation error was achieved during training.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Activations hidden/code & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		ReLU/ReLU 	       & \num{9.79e-9} & \num{7.18e-9} & \num{0.0010}  & \num{0.0009}&5000 &4998\\ \hline
		ELU/ELU            & \num{4.44e-9} & \num{1.11e-8} & \num{0.0008}  & \num{0.0012}&5000 &5000\\ \hline
		Tanh/Tanh 	       & \num{7.83e-9} & \num{2.58e-8} & \num{0.0011}  & \num{0.0018}&5000 &5000\\ \hline
		SiLU/SiLU 	       & \num{7.69e-9} & \num{1.37e-8} & \num{0.0011}  & \num{0.0013}&5000 &5000\\ \hline
		LeakyReLU/LeakyReLU& \num{1.86e-8} & \num{9.39e-9} & \num{0.0015}  & \num{0.0010}&5000 &4997\\ \hline
		ELU/Tanh           & \num{5.49e-9} & \num{1.87e-8} & \num{0.0008}  & \num{0.0014}&5000 &5000\\ \hline
		LeakyReLU/Tanh     & \num{1.00e-8} & \num{1.42e-8} & \num{0.0010}  & \num{0.0012}&4997 &4992\\ \hline
		ELU/SiLU           & \num{8.11e-9} & \num{1.93e-8} & \num{0.0011}  & \num{0.0015}&5000 &5000\\ \hline
	\end{tabular*}\label{Tab:activations}
\end{table} 
The final hyperparameters for both input data are summarized below in \cref{Tab:Final}. From the initial models to the final models the decrease in validation error gained \(\approx \num{1.5e-7}\) for \(\hy\) and \(\approx \num{7.2e-8}\) for \(\rare\) which is 93\% of the initial values for both models.
\begin{table}[H]
	\centering
	\caption{Summary of the final hyperparameters for both input data.}
	\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Input data & Act. hidden/code & Batch size & Width & Depth & Learning rate & Epochs\\ [.5ex]
		\hline
		$\hy$ &  ELU/ELU & 4 & 30 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
		$\rare$ & ReLU/ReLU & 4 & 40 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
	\end{tabular*}\label{Tab:Final}
\end{table}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Depth/hydro_depth.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Depth/rare_depth.tex}
		\caption{Five experiments over the different depth with $\hy$ left and $\rare$ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.}
		\label{Fig:Depth}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Width/hydro_width.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Width/rare_width.tex}
		\caption{Five experiments over different width with $\hy$ left and $\rare$ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.}
		\label{Fig:Width}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/hydro_batch.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/rare_batch.tex}
		\caption{Five experiments over different batch sizes with $\hy$ left and $\rare$ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.}
		\label{Fig:batch}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act.tex}
	\end{figure}
\end{center}
\begin{figure}[H]
	\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act2.tex}
	\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act2.tex}
		\caption{Five experiments over different batch sizes with $\hy$ left and $\rare$ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.}
\end{figure}\label{Fig:Activations}
%\newpage
\chapter{Hyperparameters for the Convolutional Autoencoder}
\label{Ch:ApB}

The finding of appropriate hyperparameters for the convolutional autoencoder is described here. The hyperparameters include  batch size, non-linear activation functions, number of epochs for training and learning rate and number of layers i.e. depth. Depth comprises kernel size, stride width as well as number of channels per layer.\\
This analysis follows the prior finding of hyperparameters for the fully connected autoencoder, hence utilizes insights thereof. Additionally this analysis follows a slightly different scheme than before, as there are solely 40 examples for training and validation for both rarefaction levels \(\hy\) and \(\rare\). Hence it is tried to find a combined model that performs well on both rarefaction levels. With this measure datasets for both rarefaction levels are concatenated to one dataset of 80 examples for training and validation. Furthermore this approach gives an answer to how well a model can generalize about the BGK model for different rarefaction levels in Sod's shock tube.\\
Again we start with a small model which architecture and hyperparameters are summarized in \cref{Tab:small}.
\begin{table}[H]
\centering
\caption{Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num{e-4} to \num{e-4} at the 500th epoch as in \cref{Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.}
\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
	\toprule
	\multicolumn{4}{c}{Encoder} & \multicolumn{4}{c}{Decoder}\\ [.5ex] \hline
	Layer & Type & Channels & Output size & Layer & Type & Channels & Output size \\ 
	\hline
	1 & Conv  & 8  & $5\times 40$ & 4 & Lin.     & -  & 128            \\ \hline
	2 & Conv. & 16 & $1\times 8$  & 5 & Tr.Conv. & 16 & $1\times 8$    \\ \hline
	3 & Lin.  & -  & 5		      & 6 & Tr.Conv. & 8  & $25\times 200$ \\ 		
\end{tabular*}
\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
	\toprule
	Epochs & Act. hidden/code & Batch size & Depth & Kernel size & Stride width & Learning rate\\ [.5ex]
	\hline
	1000 &  ReLU/ReLU  & 2 & 4 & \(5\times 5\) & \(5\times 5\) & \num{e-4}/\num{e-5} \\ \hline
\end{tabular*}\label{Tab:small}
\end{table} 
The activations and learning rate stem from conclusions made in \cref{Ch:ApA}. Kernel size and stride width are chosen as to circumvent checkerboard artefacts as described in \cref{Ch:DimRedAl}. The number of channels is inspired by the architecture chosen in \cite{Carlberg}.\\
The number of available examples for training and validation limited as previously stated . Therefor the split in training and validation sets encompasses the risk of a bias in one of the sets. For instance, if out of the 40 examples, a few of them ,maybe 5 , contain considerable variance to the other examples and all of them are found in the training set, then the validation error could not estimate the model's ability to generalize. Therefore the k-fold algorithm described in \cite{Goodfellow} is adopted. The k-fold algorithm provides from the complete shuffled dataset, k independent splits into training and validation set with which the model is trained. By that, each example gets the chance to be either in the training or validation set once. For an 80/20 split as used in this thesis and described in \cref{Ch:DimRedAl}, five independent folds can be obtained. Hence with 80 available examples the training set \(\btch_{train}\) consists of 74 examples and the validation set \(\btch_{val}\) of the remaining 16 examples. The results of said preliminary experiment are summarized in \cref{Tab:kFold} and \cref{Fig:kFold}.
\begin{table}[H]
	\centering
	\caption{Results for the k-fold experiments. Minimum values of training- and validation error as well as the epoch of the minimum validation error and the \(\L2\) are given. The \(\L2\) is evaluated with the models saved when the minimum validation error was achieved during training. The mean and standard deviation of the validation error are \num{3.55e-5} and \num{2.13e-5} respectively with a corresponding variance of \num{4.56e-10}.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Fold & Minimum training error & Minimum validation error & \(\L2\) & Epoch\\ [.5ex]
		\hline
		 1   & \num{2.4e-5}           & \num{7.0e-5}             & 0.053   & 498  \\  
		\hline
		2    & \num{1.4e-5}           & \num{1.3e-5}             & 0.044   & 1000\\
		\hline
		3    & \num{1.7e-5}           & \num{5.0e-5}             & 0.057   & 1000\\
		\hline
		4    & \num{1.7e-5}           & \num{1.8e-5}             & 0.039   & 997\\
		\hline
		5    & \num{1.6e-5}           & \num{2.7e-5}             & 0.053  & 1000\\
		\hline
	\end{tabular*}\label{Tab:kFold}
\end{table}
The lowest validation error of \num{1.3e-5} is achieved with the second fold at the last epoch. The corresponding \(L2 = 0.044\). As training error is lower than validation error it is assumed, that the \say{difficult} examples are within the training set. Therefore the training set could be biased. The second lowest validation error of \num{1.8e-5} and a corresponding \(\L2=0.039\) is achieved with fold 4 at the 997th epoch. For both folds training and validation error evolve in unison and get instable even before the 500th epoch. The instability is eliminated by lowering the learning rate at said epoch, but the training does not improve thereafter as in \cref{Ch:ApA}. For the other folds a seperation of training and validation error can be observed and less instability. The mean validation error of \num{3.55e-5} gives an estimate of the models performance with a standard deviation of \num{2.13e-5} and variance of \num{4.56e-10}. For the continuation of experiments fold 4 is used, as it provides a balanced split in training- and validation split which also manifests in the lowest \(\L2\) of all folds.\\
Next the capacity of the model is increased by successively adding convolutional layers, thus obtaining two additional models. One encompasses three convolutional layers, the other four convolutional layers in encoder and decoder. An exact description of both architectures is summarized in \cref{Tab:Layer}. The maximum number of channels for this analysis is kept at 16. This measure decreases the growth rate of channels per layer, which is quadratic for the first model and now scales to linear in order to increase capacity rather smooth. Note that it is also possible to keep the growth rate constant while adding layers. Here this is done in the following steps.\\
Kernel size and stride width shrink the input as described in \cref{Ch:DimRedAl}. Hence the same kernel size and stride width of the previous model can only be adopted by using excessive zero padding between successive convolutional layers. To overcome this issue a balance between zero padding, which cannot be omitted for four convolutional layers with the given input dimensions, and kernel size and stride width needs to be found. Here both features are chosen to have sizes \(3\times 3\). Extra padding, which is needed for one to reduce the shrinkage rate per layer in the encoder and for the other to transpose the shrinkage of the encoder in the decoder, is summarized in \cref{Tab:Layer}. In \texttt{pytorch}'s implementation of transposed convolutional layers a handy parameter can be used called output padding. This parameter provides a measure to effectively increase the output size by one, left or at the bottom of the output, to resolve the ambiguity when \(\text{stride}>1\). Then the respective convolutional layer maps multiple input sizes to the same output size. Taking for example the second and sixth layer of the model with three convolutional layers in encoder and decoder. Here the convolutional layer maps a size of 23 to 8. But the respective transposed convolutional layer maps size 8 to size 22. In this case output padding resolves the issue by increasing the output size by one.         
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/kfold.tex}
		\label{Fig:kFold}
		\caption{Training- and validation error over 1000 epochs for five independent folds.}
	\end{figure}
\end{center}

\begin{table}
	\setlength{\tabcolsep}{1pt}
	\footnotesize
	\caption{Architecture for the models with three (left) and four (ricght) convolutional layers in encoder and decoder. Kernel size and stride width are \(3\times 3\) for both features and models.}
	\begin{minipage}{.5\textwidth}
		\begin{tabular*}{.9\textwidth}{ @{\extracolsep{\fill}} c c c c c @{} }
			\toprule
			\multicolumn{4}{c}{Encoder} \\ [.5ex]\hline
			Layer & Type & Channels & Padding & Output  \\ 
			\hline
			1 & Conv  & 4  & in: 1/1 & $9\times 67$  \\ \hline
			2 & Conv. & 8  & in: 0/1 & $3\times 23$  \\ \hline
			3 & Conv. & 16 & in: 0/1 & $1\times 8$   \\ \hline
			4 & Lin.  & -  & -   & 5		      	 \\ 
			\\
			\toprule
			\multicolumn{4}{c}{Decoder}		\\ [.5ex]\hline
			Layer & Type & Channels & Padding & Output \\
			\hline
			5 & Lin.     & -  & - & 128       	 		        \\ \hline
			6 & Tr.Conv. & 16 & in: 0/1 out: 0/1 & $1\times 8$  \\ \hline
			7 & Tr.Conv. & 8  & in: 0/1 out: 0/1 & $3\times 23$ \\ \hline
			8 & Tr.Conv. & 1  & in: 0/1 & $25\times 200$        \\ \hline
			\\
		\end{tabular*}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\begin{tabular*}{.9\textwidth}{ @{\extracolsep{\fill}} c c c c c @{} }
			\toprule
			\multicolumn{4}{c}{Encoder} \\ [.5ex]\hline
			Layer & Type & Channels & Padding & Out  \\ 
			\hline
			1 & Conv  & 2  & in: 3/2 & $10\times 68$ \\ \hline
			2 & Conv. & 4  & in: 3/2 & $5\times 24$  \\ \hline
			3 & Conv. & 8  & in: 3/2 & $3\times 9$   \\ \hline
			4 & Conv. & 16 & in: 3/3 & $1\times 3$   \\ \hline
			5 & Lin.  & -  & & 5		      \\  
			\toprule
			\multicolumn{4}{c}{Decoder}		\\ [.5ex]\hline
			Layer & Type & Channels & Padding & Out \\
			\hline
			6  & Lin.     & -  & - & 128       	 \\ \hline
			7  & Tr.Conv. & 16 & - & $3\times 9$   \\ \hline
			8  & Tr.Conv. & 8  & in: 2/2 out: 0/1 & $5\times 24$   \\ \hline
			9  & Tr.Conv. & 4  & in: 3/2 & $9\times 68$   \\ \hline
			10 & Tr.Conv. & 1  & in: 1/2 & $25\times 200$ \\ \hline   
		\end{tabular*}
	\end{minipage}
\end{table}
\begin{table}[H]
	\centering
	\caption{}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Layer & Minimum training error & Minimum validation error & \(\L2\) & Epoch\\ [.5ex]
		\hline
		3   & \num{1.4e-5}           & \num{1.5e-5}             & 0.044   & 1975  \\  
		\hline
		4    & \num{7.3e-4}           & \num{8.1e-4}             & 0.327   & 1995\\
		\hline
	\end{tabular*}\label{Tab:Layer}
\end{table}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/Layer.tex}
		\label{Fig:Layer}
		\caption{Training- and validation error over 2000 epochs for two models with three and four convolutional layers. Channels are chosen to reach a  maximum of 16 in the last layer of encoder. Kernel size and stride width are both \(3\times 3\).}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/Batch.tex}
		\label{Fig:Batch}
		\caption{Training- and validation error over 2000 epochs for the small model with two convolutional layers in encoder and decoder as well as the models with two three and four convolutional layers. All three models are trained with a batch size of 4.}
	\end{figure}
\end{center}

\begin{table}[H]
\centering
\caption{}
\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
	\toprule
	Layer & Minimum training error & Minimum validation error & \(\L2\) & Epoch\\ [.5ex]
	\hline
	2   & \num{6.0e-6}             & \num{8.0e-6}             & 0.030   & 1999  \\
	\hline  
	3   & \num{1.0e-5}              & \num{1.3e-5}            & 0.038   & 1965  \\  
	\hline
	4    & \num{6.0e-3}            & \num{6.9e-3}             & 0.94   	& 109\\
	\hline
\end{tabular*}\label{Tab:Batch4}
\end{table}

\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/Channels.tex}
		\label{Fig:Channels}
		\caption{Training- and validation error over 2000 epochs for the small model with two convolutional layers in encoder and decoder as well as the models with two three and four convolutional layers. All three models are trained with a batch size of 4.}
	\end{figure}
\end{center}
\begin{table}[H]
	\centering
	\caption{}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Layer & Minimum training error & Minimum validation error & \(\L2\) & Epoch \\ [.5ex]
		\hline
		2   & \num{6.8e-3}             & \num{7.6e-3}             & 1.0     & 0     \\
		\hline  
		3   & \num{9.0e6}              & \num{1.1e-5}             & 0.037   & 1985  \\  
		\hline
		4    & \num{7.0e-6}            & \num{9.0e-6}             & 0.033   & 1953  \\
		\hline
	\end{tabular*}\label{Tab:Channels}
\end{table}