% !TEX root = master.tex


\begin{center}
	{\sffamily \bfseries\Large Appendix}\\
\end{center}%\hspace*{\fill}\\[1.5cm]
\vspace{1cm}
\chapter{Hyperparameters for the Fully Connected Autoencoder}
\label{Ch:ApA}
%\pagenumbering{arabic}

The finding of appropriate hyperparameters for the fully connected autoencoder is described here. The hyperparameters include number of layers i.e. depth, number of nodes per hidden layer i.e. width, batch size and non-linear activation functions, number of epochs for training and learning rate. The experiments are evaluated through the validation error which estimates the model's ability to generalize, the training error which estimates the optimization to training data and the $\L2$ as described in \cref{Ch:ROM}. Both validation- and training error are described in \cref{Ch:DimRedAl} and provide information about under- and overfitting. Moreover the validation error is the essential metric for validating a model's performance. The $\L2$ on the other hand gives an estimate of how well the model performs on the whole dataset hence is used as a comparative metric against POD.\\ 
To start with a working model first a guess about the some initial hyperparameters is done, which are summarized in \Cref{Tab:First Guess}. These include a mini-batch size of 16, the width of the bottleneck layer is 3 and 5  for \(\hy\) and \(\rare\) respectively and a learning rate of 0.0001. Activation LeakyReLU is applied for the outputs of the input- and  any hidden layer which does not output the code, referred to as activations hidden. Tanh is applied for the output of the last hidden layer in the encoder which outputs the code, referred to as activation code. A visualization of the activation scheme is provided in \cref{Fig:ActScheme}. Moreover 2000 initial number of epochs are used. This might seem exaggerated but is justified by the little amount of input data and the small size of the network which yiels a fast training.
\begin{center}
	\begin{figure}[H]
		\centering
		\input{Figures/Parameterstudy/Fully_Connected/Activations/act_scheme.tex}
		\caption{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.}
		\label{Fig:ActScheme}
		\end{figure}
\end{center}
\begin{table}[H]
	\centering
	\caption{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Mini-batch size   & Intrinsic dimensions &   Epochs &Learning rate & Activations hidden/code \\   
		\hline
		16 		&	3/5 &     2000&	    0.0001 & LeakyReLU/Tanh\\
		\bottomrule
	\end{tabular*} \label{Tab:First Guess}
\end{table}
Five designs for finding an optimal number of layers i.e. depth are run. The hidden layers are designed to halve the input at each step. Not within this scope is the bottlenck layer which has a fixed size of 5 and 3 for $\rare$ and $\hy$ respectively and the first and last hidden layer (after the input layer and before the output layer). Those should provide an abstraction without shrinking the incoming data. Note that this design feature was validated in an initial exploration cycle which is not included in this thesis. The five designs range from 10 layers in (a) to 2 layers in (e) always taking one layer away from encoder and decoder hence decreasing by two layers and are as follows:     
\begin{enumerate}
	\item 10 layers with layer widths: 40, 40, 20, 10, 5, 3/5, 5, 10, 20, 40, 40.
	\item 8 layers with layer widths: 40, 40, 20 , 10, 3/5, 10, 20, 40, 40.
	\item 6 layers with layer widths: 40, 40, 20 , 3/5, 20, 40, 40.
	\item 4 layers with layer widths: 40, 40, 3/5, 40, 40.
	\item 2 layers with layer widths: 40, 3/5, 40.
	\end{enumerate}
The model's depth is determined in a primary step, as it sets a consequential part of the model's representational capacity and therefore can initiate over- and underfitting at an early stage in the hyperparameter search. The results of the experimentation are shown in \cref{Fig:Depth} and \cref{Tab:Depth} for both rarefaction levels.\\
\begin{table}[htp]
	\centering
	\caption{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training. The \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Depth & \multicolumn{2}{c}{Minimum training error} & \multicolumn{2}{c}{Minimum validation error} & \multicolumn{2}{c}{\(\L2\) }\\ [.5ex]
		 & \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		10& \num{1.53e-7} & \num{5.96e-7} & \num{2.22e-7} & \num{5.19e-7} & 0.0048 & 0.0091\\ \hline
		8 & \num{1.17e-7 }& \num{2.05e-7} & \num{1.58e-7} & \num{2.32e-7} & 0.0041 & 0.0054\\ \hline
		6 & \num{9.76e-8} & \num{1.40e-7} & \num{1.49e-7} & \num{1.72e-7} & 0.0038 & 0.0045\\ \hline
		4 & \num{6.29e-8} & \num{1.52e-7} & \num{7.74e-8} & \num{1.61e-7} & 0.0031 & 0.0048 \\ \hline
		2 & \num{1.29e-6} & \num{3.29e-6} & \num{1.37e-6} & \num{3.42e-6} & 0.0136 & 0.0217\\ \hline
	\end{tabular*}\label{Tab:Depth}
\end{table}\noindent
For \(\hy\) the lowest validation error of \num{7.74e-8} and an \(\L2\) of 0.0031 is reached with 4 layers, hence constitutes the best performing design out of the five. Additionally, as seen in \cref{Fig:Depth}(left), does a design exceeding 4 layers results in a slight overfitting after around 500 epochs. Less than 4 layers do not reach the validation error and \(\L2\) of the other designs, yielding the conclusion, that the capacity is too low. Overfitting occurs with 4 layers only after the 1000th epoch and is less than with the other three models that show overfitting.\\
For \(\rare\) the lowest validation error of \num{1.61e-7} is reached again with 4 layers. On the other hand the lowest \(\L2\) of 0.0031 and the lowest training error of \num{1.40e-7} are reached with 6 layers. Contrary to the afore discussed case the training error and \(\L2\) are of lower magnitude for 6 layers, except for the validation error. Looking at \cref{Fig:Depth}(right), we observe that the model with 6 layers starts to overfit after the 1500 epochs, yielding a decreasing training error and a stagnating validation error. Hence the model improved in the optimization task which additionally improves the \(\L2\). It's generalization ability, measured by the validation error, did not improve and is greater than the validation error reached with 4 layers. This concludes, a model with 4 layers constitutes the best performing design out of the five.\\
Qualitatively the overall training for both rarefaction levels is very stable. Training and validation error do not diverge exessively and converge early in training. Separation of training and validation error occurs prominently for the hydrodynamic solution. This is thought to be connected to the emersion of sharp shock fronts towards the end of the simulation. This increases variance in the whole dataset and therefore also in the training- and validation set.\\
The number of epochs is now doubled to 4000 epochs because the lowest validation error was achieved towards the end of the training in the previous experiments. Again this is justifiable as one epoch takes less than 1s to finish and no prominent overfitting is observed. 
\begin{table}[htpb!]
	\centering
	\caption{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training, the \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Hidden units & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} & \multicolumn{2}{c}{Shrinkage factor}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		50 & \num{1.91e-8}  & \num{5.05e-8} & \num{0.0015}  & \num{0.0025} & 0.06  & 0.01\\ \hline
		40 & \num{2.65e-08} & \num{1.65e-8} & \num{0.0018}  & \num{0.0014} & 0.075 & 0.125\\ \hline
		30 & \num{1.77e-08} & \num{3.40e-8} & \num{0.0015}  & \num{0.0021} & 0.015 & 0.0167\\ \hline
		20 & \num{2.50e-08} & \num{5.25e-8} & \num{0.0017}  & \num{0.0027} & 0.1   & 0.25 \\ \hline
		10 & \num{5.11e-08} & \num{3.97e-7} & \num{0.0025}  & \num{0.0077} & 0.3   & 0.5\\ \hline
	\end{tabular*}\label{Tab:Width}
\end{table}
The width of the two remaining hidden layers is examined in the following. For both the hydrodynamic and the rarefied regime five experiments are conducted, lowering the hidden units of the hidden layers from fifty to ten. Note that the decoder is chosen to be structurally a reflection of the encoder. Therefore only one parameter is changed. Results for \(\hy\) and \(\rare\) are shown in \cref{Tab:Width}. Note that the contribution of over- and underfitting is negligible and therefore the training error is omitted. A model with 30 hidden units in encoder and decoder performs best with \(\hy\) and reaches a validation error of \(\num{1.77e-08}\). The corresponding \(\L2 = \num{1.5e-3}\) with a shrinkage factor of 0.015. Overall the loss of each experiment with \(\hy\) is quiet similar an ranges from \(\num{1.77e-8}\) to \(\num{5.11e-8}\). The \(\L2\) behaves in a similar fashion and is even equal for 50 and 30 layers. A model with 40 hidden units performs best for \(\rare\). The corresponding validation error is \(\num{1.65e-8}\) with \(\L2=\num{1.4e-3}\), which is smaller than \(\hy\). The shrinkage factor is 0.125. In all experiments a model with 10 hidden nodes performs worst. Training and validation error over all 4000 epochs for both experiments can be seen in \cref{Fig:Width}. The aforementioned separation of training- and validation error, that was observed solely for \(\hy\), is mitigated when moving away from 40 hidden units for encoder and decoder. Not shrinking the input in the first hidden layer only serves the performance when using \(\rare\).\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L2\) is also given but evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Batch Size & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		32& \num{5.40e-8} & \num{2.17e-8} & \num{0.0024}  & \num{0.0017}&4998&4992\\ \hline
		16& \num{1.95e-8} & \num{2.06e-8} & \num{0.0015}  & \num{0.0016}&4999&5000\\ \hline
		8 & \num{2.25e-8} & \num{1.03e-8} & \num{0.0017}  & \num{0.0012}&4965&4961\\ \hline
		4 & \num{1.52e-8} & \num{6.30e-9} & \num{0.0013}  & \num{0.0010}&3956&4534\\ \hline
		2 & \num{1.15e-8} & \num{9.18e-9} & \num{0.0012}  & \num{0.0013}&4956&4872\\ \hline
	\end{tabular*}\label{Tab:Batch}
\end{table}
Next the mini-batch size is analysed. Epochs are increased by 1000 epochs, as training- and validation error show potential to decrease further as seen in \cref{Fig:Width} for \(\rare\) with 40 hidden nodes. Results for $\hy$  and  $\rare$ are displayed in \cref{Tab:Batch}. Experiments are conducted with mini-batch sizes of 2, 4, 8, 16, 32. Batch sizes to the power of 2 are typically chosen as to fully exploit computational capabilities of a GPU i.e. aligning the batch size with the way memory is structured within a GPU. The smallest batch size of 2 yields the lowest validation error of \(\num{1.15e-8}\) with corresponding \(\L2 = 0.0012\) at epoch 4956 for \(\hy\). The lowest validation error with \(\num{6.30e-9}\) is achieved for \(\rare\) at epoch 4534 with a batch size of 4. The corresponding \(\L2 = 0.001\). Compared to a batch size of 16 in the previous experiments we can observe that small batch sizes have a regularizing effect on training as described in \cref{Ch:DimRedAl} and therefore are beneficial to generalization. At the same time, the lower the batch sizes are, the more unstable is the training as seen in \cref{Fig:Batch}. The oscillations which begin with batch sizes of 8 and lower, which make the training unstable, can be battled with a lower learning rate as soon as training starts to tremble. Additionally small batch sizes drastically increase training time which is why a batch size as low as 2 will not be used for the next experiments. In conclusion a batch size of 2 is omitted and for both input data a batch size of 4 is chosen. Furthermore a reduction of the learning rate from \(\num{1e-4}\) to \(\num{1e-5}\) is applied after the 3000th epoch.\\
Eight experiments with different activation functions namely ReLU, ELU, Tanh, SiLU and LeakyReLU are performed. The experiment designs and results are given in \cref{Tab:activations} for hidden and code activations. With \(\hy\) a combination of ELU and ELU for hidden and code activation yields the best results in validation error with \num{4.44e-9} and a corresponding \(\L2 = 0.0008\). These values are achieved at the last epoch. For \(\rare\) a combination of ReLU and ReLU for hidden and code activation produces a validation error of \num{7.18e-9} and a corresponding \(\L2 = 0.0009\). Both values are also reached close to the last epoch. Note that all models reach their lowest loss at or very close to the last epoch. The reason is the stable training after the 3000th epoch, where the learning rate is lowered to \num{1e-5} as seen in \cref{Fig:Activations}. This measure shows in all experiments an immediate success for learning. Both validation and training error fall at the 3001st epoch and only decrease slightly thereafter. This behavior clearly shows that the updates to the free parameters \(\frepar\) were too big which prohibitively slowed down or even prevented the learning process. Small updates to \(\frepar\) made all models quickly reach a minimum. Therefore 3000 epochs or even less could have been enough to reach similar results with a lower learning rate.\\
If the learning rate could have been reduced whilst producing similar results remains unanswered here as the results are satisfactory. Note that for \(\rare\) in the previous experiment a validation error of \num{6.30e-9} was achieved which is slightly lower than the current result. Nonetheless it is decided to take the current model as the final result.\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L2\) all evaluated with the best performing model.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Activations hidden/code & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		ReLU/ReLU 	       & \num{9.79e-9} & \num{7.18e-9} & \num{0.0010}  & \num{0.0009}&5000 &4998\\ \hline
		ELU/ELU            & \num{4.44e-9} & \num{1.11e-8} & \num{0.0008}  & \num{0.0012}&5000 &5000\\ \hline
		Tanh/Tanh 	       & \num{7.83e-9} & \num{2.58e-8} & \num{0.0011}  & \num{0.0018}&5000 &5000\\ \hline
		SiLU/SiLU 	       & \num{7.69e-9} & \num{1.37e-8} & \num{0.0011}  & \num{0.0013}&5000 &5000\\ \hline
		LeakyReLU/LeakyReLU& \num{1.86e-8} & \num{9.39e-9} & \num{0.0015}  & \num{0.0010}&5000 &4997\\ \hline
		ELU/Tanh           & \num{5.49e-9} & \num{1.87e-8} & \num{0.0008}  & \num{0.0014}&5000 &5000\\ \hline
		LeakyReLU/Tanh     & \num{1.00e-8} & \num{1.42e-8} & \num{0.0010}  & \num{0.0012}&4997 &4992\\ \hline
		ELU/SiLU           & \num{8.11e-9} & \num{1.93e-8} & \num{0.0011}  & \num{0.0015}&5000 &5000\\ \hline
	\end{tabular*}\label{Tab:activations}
\end{table} 
The final hyperparameters for both input data are summarized below in \cref{Tab:Final}. From the initial models to the final models the decrease in validation error gained \(\approx \num{1.5e-7}\) for \(\hy\) and \(\approx \num{7.2e-8}\) for \(\rare\) which is 93\% of the initial values for both models.
\begin{table}[H]
	\centering
	\caption{Summary of the final hyperparameters for both input data.}
	\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Input data & Act. hidden/code & Batch size & Width & Depth & Learning rate & Epochs\\ [.5ex]
		\hline
		$\hy$ &  ELU/ELU & 4 & 30 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
		$\rare$ & ReLU/ReLU & 4 & 40 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
	\end{tabular*}\label{Tab:Final}
\end{table}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Depth/hydro_depth.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Depth/rare_depth.tex}
		\caption{Five experiments over the different depth with $\hy$ left and $\rare$ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.}
		\label{Fig:Depth}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Width/hydro_width.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Width/rare_width.tex}
		\caption{Five experiments over different width with $\hy$ left and $\rare$ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.}
		\label{Fig:Width}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/hydro_batch.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/rare_batch.tex}
		\caption{Five experiments over different batch sizes with $\hy$ left and $\rare$ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.}
		\label{Fig:batch}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act.tex}
	\end{figure}
\end{center}
\begin{figure}[H]
	\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act2.tex}
	\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act2.tex}
		\caption{Five experiments over different batch sizes with $\hy$ left and $\rare$ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.}
\end{figure}\label{Fig:Activations}
%\newpage
\chapter{Hyperparameters for the Convolutional Autoencoder}
\label{Ch:ApB}

The finding of appropriate hyperparameters for the convolutional autoencoder is described here. The hyperparameters include  batch size, non-linear activation functions, number of epochs for training and learning rate and number of layers i.e. depth. Depth comprises kernel size, stride width as well as number of channels per layer.\\
This analysis follows the prior finding of hyperparameters for the fully connected autoencoder, hence utilizes insights thereof. Additionally this analysis follows a slightly different scheme than before, as there are solely 40 examples for training and testing for both rarefaction levels \(\hy\) and \(\rare\). Hence it is tried to find a combined model that performs well on both rarefaction levels. With this measure datasets for both rarefaction levels are concatenated to one dataset of 80 examples for training and testing. Furthermore this approach gives an answer to how well a model can generalize about the BGK model for different rarefaction levels.\\
Again we start with a small model which comprises hyperparameters summarized in \cref{Tab:small}.
\begin{table}[H]
\centering
\caption{Initial selection of hyperparameters. The learning rate is switched from \num{e-4} to \num{e-4} at the 500th epoch. Depth counts the number of convolutional layers and excludes all fully connected layers.}
\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
	\toprule
	Epochs & Act. hidden/code & Batch size & Depth & Kernel size & Stride width & Learning rate\\ [.5ex]
	\hline
	1000 &  ReLU/ReLU & 2 & 4 & \(5\times 5\) & \(5\times 5\) &\num{e-4}/\num{e-5}\\ \hline
\end{tabular*}\label{Tab:small}
\end{table}   
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/kfold.tex}
		\label{Fig:Kfold}
		\caption{Five folds}
	\end{figure}
\end{center}

