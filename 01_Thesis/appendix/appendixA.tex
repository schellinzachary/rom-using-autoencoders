% !TEX root = master.tex


\begin{center}
	{\sffamily \bfseries\Large Appendix}\\
\end{center}%\hspace*{\fill}\\[1.5cm]
\vspace{1cm}
\chapter{Hyperparameters for the Fully Connected Autoencoder}
\label{Ch:ApA}
%\pagenumbering{arabic}

The finding of appropriate hyperparameters for the fully connected autoencoder is described here. The hyperparameters include number of layers i.e. depth, number of nodes per hidden layer i.e. width, batch size and non-linear activation functions, number of epochs for training. The experiments are evaluated through the validation error which estimates the model's ability to generalize, the training error which estimates the optimization to training data and the $\L2$ as described in \cref{Ch:ROM}. Both validation- and training error are described in \cref{Ch:DimRedAl} and provide information about under- and overfitting. Moreover the validation error is the essential metric for validating a model's performance. The $\L2$ on the other hand gives an estimate of how well the model performs on the whole dataset hence is used as a comparative metric against POD.\\ 
To start with a working model first a guess about the some initial hyperparameters is done, which are summarized in \Cref{Tab:First Guess}. These include a mini-batch size of 16, the width of the bottleneck layer is 3 and 5  for \(\hy\) and \(\rare\) respectively, a learning rate of 0.0001, activations LeakyReLU and Tanh for the hidden layers and code layer respectively as well as 2000 initial number of epochs. Note that 2000 epochs might seem exaggerated, but is justified by a fast training.
\begin{table}[H]
	\centering
	\caption{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Mini-batch size   & Intrinsic dimensions &   Epochs &Learning rate &activation code/rest \\   
		\hline
		16 		&	3/5 &     2000&	    0.0001 & Tanh/LeakyReLU\\
		\bottomrule
	\end{tabular*} \label{Tab:First Guess}
\end{table}
Five designs for finding an optimal number of layers i.e. depth are run. The hidden layers are designed to halve the input at each step. Not within this scope is the bottlenck layer which has a fixed size of 5 and 3 for $\rare$ and $\hy$ respectively and the first and last hidden layer (after the input layer and before the output layer). Those should provide an abstraction without shrinking the incoming data. Note that this design feature was validated in an initial exploration cycle which is not included in this thesis. The five designs range from 10 layers in (a) to 2 layers in (e) always taking one layer away from encoder and decoder hence decreasing by two layers and are as follows:     
\begin{enumerate}
	\item 10 layers with layer widths: 40, 40, 20, 10, 5, 3/5, 5, 10, 20, 40, 40.
	\item 8 layers with layer widths: 40, 40, 20 , 10, 3/5, 10, 20, 40, 40.
	\item 6 layers with layer widths: 40, 40, 20 , 3/5, 20, 40, 40.
	\item 4 layers with layer widths: 40, 40, 3/5, 40, 40.
	\item 2 layers with layer widths: 40, 3/5, 40.
	\end{enumerate}
The model's depth is determined in a primary step, as it sets a consequential part of the model's representational capacity and therefore can initiate over- and underfitting at an early stage in the hyperparameter search. The results of the experimentation are shown in \cref{Fig:Depth} and \cref{Tab:Depth} for both rarefaction levels.\\
\begin{table}[htp]
	\centering
	\caption{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Depth & \multicolumn{2}{c}{Minimum training error} & \multicolumn{2}{c}{Minimum validation error} & \multicolumn{2}{c}{\(\L2\) }\\ [.5ex]
		 & \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		10& \num{1.53e-7} & \num{5.96e-7} & \num{2.22e-7} & \num{5.19e-7} & 0.0048 & 0.0091\\ \hline
		8 & \num{1.17e-7 }& \num{2.05e-7} & \num{1.58e-7} & \num{2.32e-7} & 0.0041 & 0.0054\\ \hline
		6 & \num{9.76e-8} & \num{1.40e-7} & \num{1.49e-7} & \num{1.72e-7} & 0.0038 & 0.0045\\ \hline
		4 & \num{6.29e-8} & \num{1.52e-7} & \num{7.74e-8} & \num{1.61e-7} & 0.0031 & 0.0048 \\ \hline
		2 & \num{1.29e-6} & \num{3.29e-6} & \num{1.37e-6} & \num{3.42e-6} & 0.0136 & 0.0217\\ \hline
	\end{tabular*}\label{Tab:Depth}
\end{table}\noindent
For \(\hy\) the lowest validation error of \num{7.74e-8} and an \(\L2\) of 0.0031 is reached with 4 layers, hence constitutes the best performing design out of the five. Additionally, as seen in \cref{Fig:Depth}(left), does a design exceeding 4 layers results in a slight overfitting after around 500 epochs. Less than 4 layers do not reach the validation error and \(\L2\) of the other designs, yielding the conclusion, that the capacity is too low. Overfitting occurs with 4 layers only after the 1000th epoch and is less than with the other three models that show overfitting.\\
For \(\rare\) the lowest validation error of \num{1.61e-7} is reached again with 4 layers. On the other hand the lowest \(\L2\) of 0.0031 and the lowest training error of \num{1.40e-7} are reached with 6 layers. Contrary to the afore discussed case the training error and \(\L2\) are of lower magnitude for 6 layers, except for the validation error. Looking at \cref{Fig:Depth}(right), we observe that the model with 6 layers starts to overfit after the 1500 epochs, yielding a decreasing training error and a stagnating validation error. Hence the model improved in the optimization task which additionally improves the \(\L2\). It's generalization ability, measured by the validation error, did not improve and is greater than the validation error reached with 4 layers. This concludes, a model with 4 layers constitutes the best performing design out of the five.\\
Qualitatively the overall training for both rarefaction levels is very stable. Training and validation error do not diverge exessively and converge early in training. Separation of training and validation error occurs prominently for the hydrodynamic solution. This is thought to be connected to the emersion of sharp shock fronts towards the end of the simulation. This increases variance in the whole dataset and therefore also in the training- and validation set.\\
The number of epochs is now doubled to 4000 epochs because the lowest validation error was achieved towards the end of the training in the previous experiments. Again this is justifiable as one epoch takes less than 1s to finish and no prominent overfitting is observed. 
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Depth/hydro_depth.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Depth/rare_depth.tex}
		\caption{Five experiments over the different depth with $\hy$ left and $\rare$ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.}
		\label{Fig:Depth}
	\end{figure}
\end{center}
\begin{table}[htpb!]
	\centering
	\caption{Results for the variation of width. Given are minimum values of training and validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Hidden units & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} & \multicolumn{2}{c}{Shrinkage factor}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		50 & \num{1.91e-8} & \num{5.05e-8} & \num{1.5e-3}  & \num{2.5e-3} & 0.06  & 0.01\\ \hline
		40 & \num{2.65e-08} & \num{1.65e-8} & \num{1.8e-3} & \num{1.4e-3} & 0.075 & 0.125\\ \hline
		30 & \num{1.77e-08} & \num{3.40e-8} & \num{1.5e-3}  & \num{2.1e-3} & 0.015 & 0.0167\\ \hline
		20 & \num{2.50e-08} & \num{5.25e-8} & \num{1.7e-3}  & \num{2.7e-3} & 0.1   & 0.25 \\ \hline
		10 & \num{5.11e-08} & \num{3.97e-7} & \num{2.5e-3}  & \num{7.7e-3} & 0.3   & 0.5\\ \hline
	\end{tabular*}\label{Tab:Width}
\end{table}
The width of the two remaining hidden layers is examind in the following. For both the hydrodynamic and the rarefied regime five experiments are conducted, lowering the hidden units of the hidden layers from fifty to ten. Note that the decoder is chosen to be structurally a reflection of the encoder. Therefore only one parameter is changed. Results for \(\hy\) and \(\rare\) are shown in \cref{Tab:Width}. Note that the contribution of over- and underfitting is negligible and therefore is omitted. A model with 30 hidden units in encoder and decoder performs best with \(\hy\) and reaches a validation error of \(\num{1.77e-08}\). The corresponding \(\L2 = \num{1.5e-3}\) with a shrinkage factor of 0.015.\\
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Width/hydro_width.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Width/rare_width.tex}
		\label{Fig:Width}
		\caption{Five experiments over different width with $\hy$ left and $\rare$ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.}
	\end{figure}
\end{center}
Next the mini-batch size is analysed. Results are displayed in \cref{Tab:Batch Size Hydro} for $\hy$  and in \cref{Tab:Batch Size Rare} for $\rare$. Additionally \cref{Sec:AppendixA} shows the training for both in put data $\hy$ and $\rare$. For both input data experiments are first conducted with mini-batch sizes in the range of : \([2,4,8,16,32]\).\\ The results of the L2-Error show best performance between a mini-batch size of 8 and 16 for $\hy$. Likewise the results of the L2-Error for $\rare$ show best perfomance between mini-batch sizes of 4 and 16. Therefore additional experiments are conducted. Three additional experiments for $\hy$ with mini-batch sizes of: \([10,12,24]\). Two additional experiments for $\rare$ with mini-batch sizes of: \([6,10]\). The training reveals that smaller batch sizes lead to oscillating traing and test errors. This can be overcome by a smaller learning rate, but also leads to increased computational time as more epochs are needed to achieve a comparable training and test loss at the last epoch. A mini-batch size of 8 is bordering the zone where the oscillations of training and test loss subside. At the same time a mini-batch size of 8 to 10 indicate also a growth of the L2-Error. In conclusion a mini-batch size of 8 is chosen to continue to working with for $\hy$ and $\rare$. The osccilations which make the training instable can be battled with a lower learning rate as soon as training starts to tremble.\\
\begin{table}[htp]
	\centering
	\caption{Minimum values of training and validation error as well as the \(\L2\) defined in \cref{Ch:ROM}. The minima where reached around the last 50 epochs.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Batch Size & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		32 & \num{3.93e-8} & \num{5.05e-8} & \num{0.0030}  & \num{0.0017}\\ \hline
		16 & \num{2.25e-8} & \num{1.65e-8} & \num{0.0013} & \num{0.0017}\\ \hline
		14 & \num{1.06e-7} & \num{3.40e-8} & \num{0.0018}  & -\\ \hline
		12 & \num{5.23e-8} & \num{5.25e-8} & \num{0.0018}  & -\\ \hline
		10 & \num{3.05e-7} & \num{3.97e-7} & \num{0.0029}  & \num{0.0014}\\ \hline
		8 & \num{3.05e-7} & \num{3.97e-7} & \num{0.0011}  & \num{0.0012}\\ \hline
		4 & \num{3.05e-7} & \num{3.97e-7} & \num{0.0011}  & \num{0.0010}\\ \hline
		2 & \num{3.05e-7} & \num{3.97e-7} & \num{0.0011}  & \num{0.0014}\\ \hline
	\end{tabular*}\label{Tab:Batch}
\end{table}
Together with the mini-batch sizes, activations are examined. Hence experiments with different activation functions are performed with the innitially used mini-batch size of 16. Besides the mini-batch size, epochs are as well taken from the inital selection of hyperparameters as 2000 epochs. Unlike the previous two examinations the selection of activation functions shows early in training if a acceptable level of perfomance can be achieved. This is a personal observation made throughout working on this thesis. First five activations are applied to all the four layers. Second a combination of activation functions is studied, where the intrinsic variables is activated with a different function than the remaining layers. Results can be observed in \cref{Tab:Activations Hydro} and \cref{Tab:Activations Rare}.
\begin{table}[!htbp]\centering
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		Activation function & ReLU & ELU & Tanh & SiLU & LeakyReLU \\ [.5ex]
		\hline
		Error & 0.0028 & 0.0019 & 0.0036 & 0.002 & 0.0039\\ \hline
		Activation function & ELU/Tanh & LeakyReLU/Tanh & ELU/SiLU & & \\ [.5ex]
		\hline
		Error & 0.0019 & 0.0017 & 0.0019 &  & \\ \hline
	\end{tabular}
	\caption{L2-Error different activation functions and combinations for $\hy$.}
	\label{Tab:Activations Hydro}
\end{table}
\begin{table}[!htbp]\centering
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		Activation function & ReLU & ELU & Tanh & SiLU & LeakyReLU \\ [.5ex]
		\hline
		Error & 0.0048 & 0.0029 & 0.0037 & 0.0134 & 0.0.0034\\ \hline
		Activation function & ELU/Tanh & LeakyReLU/Tanh & ELU/SiLU & & \\ [.5ex]
		\hline
		Error & 0.0.0032 & 0.0030 & 0.0036 &  & \\ \hline
	\end{tabular}
	\caption{L2-Error different activation functions and combinations for $\rare$.}
	\label{Tab:Activations Rare} 
\end{table}
Both input data lead to different results this time. For that reason $\hy$ and $\rare$ are studied separately. Starting with $\hy$ and the values of the L2-Error, it can be observed that,
while ELU and SiLU stand out with the lowest value of the L2-Error, when applying the same activation function to all layers, all three coupled activations convince in the combination case. Applying different activations to the layers increases the representational capacity of the model since the model can feed on a greater variety of functions as explained in the previous subsection. The need for a great representational capacity reduces for $\hy$. The reasons are the linearity of that case as described in \cite{BGK}. Hence two activations (ELU and SiLU) from a similar family of functions perform well on $\hy$ without the need of a second actiation. Still adding another activation render as good results concerning the L2-Error. When observing the train -and test loss in \cref{Fig:Activations Hydro 1} one can notice that overfitting clusters the activations into two groups. While they all achieve a similar MSE-Loss for train -and test data only ELU and Tanh are not sowing overfitting. The combination of two activations functions yields an abundance of overfitting only for the combination of LeakyReLU with Tanh. In addition LeakyReLU with Tanh reaches together with ELU in combination with SiLU the lowest train -and test loss. The slight overfitting observed with the combination ELU and SiLU  give rise to choose for the final model LeakyReLU with Tanh.\\
To continue with $\rare$  the L2-Error is again observed in \cref{Tab:Activations Rare}, ELU stands out for the single activation. ELU with Tanh and LeakyReLU with Tanh meet the lowest L2-error for the combination of activations. Next observing the training for the three activations in \cref{Fig:Activations Rare} one finds that all three show little to no overfitting, while the combination of LeakyReLU and Tanh deviates from that slightly. From this point no clean decision can be made which activation or combination to take. Therefore another run is conducted for the three. Results are shown in \cref{Tab:Activations Rare 2nd}. Only LeakyReLU with Tanh and ELU alone convince with a lower L2-loss. The train -and test loss does not give any hint which of the two remaining activations/ combination of activations to take concerning overfitting or a profound gradient for the train -and test loss. Hence a third run is performed taking the parameters already produced for the two models and training them for another 2000 epochs. This warm-starting of the models finally gives an answer on which one to take. The results in \cref{Tab:Activations Rare 2nd} show that LeakyReLU with Tanh, just as for $\hy$, perform well on $\rare$ with the given model. In \cref{Fig:Activations Rare 2nd3rd} the train and test loss during training are provided for teh second and third run. 
\begin{table}[!htbp]\centering
	\begin{tabular}{ |c|c|c|c| }
		\hline
		Activation function & ELU & ELU/Tanh & LeakyReLU/TanH \\ 
		\hline
		Error 2nd run& 0.0034 & 0.0029 & 0.0025\\ \hline
		Error 3rd run& 		  & 0.0024 & 0.0019 \\ \hline
	\end{tabular}
	\caption{L2-Error different activation functions and combinations for $\hy$.}
	\label{Tab:Activations Rare 2nd}
\end{table}


\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/2.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/4.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/8.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/10.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/12.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/14.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/16.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Hydro/32.tex}
		\caption{Training with different mini-batch sizes for $\hy$. Train -and test loss is shown over 5000 epochs.}
		\label{}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/2.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/4.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/6.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/8.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/10.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/16.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/Rare/32.tex}
		\caption{Training with different mini-batch sizes for $\rare$. Train -and test loss is shown over 5000 epochs.}
		\label{}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/ReLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/ELU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/Tanh.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/SiLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/LeakyReLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/ELU_Tanh.tex}}
		\caption{balblabla}
		\label{Fig:Activations Hydro 1}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]\ContinuedFloat
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/LeakyReLU_Tanh.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Hydro/ELU_SiLU.tex}}
		\caption{}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ReLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ELU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/Tanh.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/SiLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/LeakyReLU.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ELU_Tanh.tex}}
		\caption{blablabla}
		\label{Fig:Activations Rare}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]\ContinuedFloat
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/LeakyReLU_Tanh.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ELU_SiLU.tex}}
		\caption{blablabla}
		\label{}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ELU_test.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/LeakyReLU_Tanh_test.tex}}
		\scalebox{.9}{\input{Figures/Parameterstudy/Fully_Connected/Activations/Rare/ELU_Tanh_test.tex}}
		\caption{blablabla}
		\label{Fig:Activations Rare 2nd3rd}
	\end{figure}
\end{center}
\newpage
\chapter{Hyperparameters for the Convolutional Autoencoder}
\label{Ch:ApB}

\begin{figure}[!htp]
	\input{Figures/Parameterstudy/Convolutional/Convolutions/Scheme_Rounds.tex}
\end{figure}
\begin{figure}
	\scalebox{.9}{\input{Figures/Parameterstudy/Convolutional/Layers/2Layer.tex}}
	\scalebox{.9}{\input{Figures/Parameterstudy/Convolutional/Layers/3Layer.tex}}
	\scalebox{.9}{\input{Figures/Parameterstudy/Convolutional/Layers/4Layer.tex}}
	\caption{Three convolutional networks with differing depth and with identical kernel evolution for $\rare$.}
\end{figure}
\begin{figure}
	\scalebox{1}{\input{Figures/Parameterstudy/Convolutional/Channels/L2R.tex}}\\
	\scalebox{1}{\input{Figures/Parameterstudy/Convolutional/Channels/L3R.tex}}\\
	\scalebox{1}{\input{Figures/Parameterstudy/Convolutional/Channels/L4R.tex}}
	\caption{Different Channel sizes for three convolutional networks with differing depth for $\rare$.}
\end{figure}
\subsection{Appendix B}\label{Sec:AppendixA}
