% !TEX root = master.tex


\begin{center}
	{\sffamily \bfseries\Large Appendix}\\
\end{center}%\hspace*{\fill}\\[1.5cm]
\vspace{1cm}
\chapter{Hyperparameters for the Fully Connected Autoencoder}
\label{Ch:ApA}
%\pagenumbering{arabic}

The finding of appropriate hyperparameters for the fully connected autoencoder is described here. The hyperparameters include number of layers i.e. depth, number of nodes per hidden layer i.e. width, batch size and non-linear activation functions, number of epochs for training and learning rate. The experiments are evaluated through the validation error which estimates the model's ability to generalize, the training error which estimates the optimization to training data and the $\L2$ as described in \cref{Ch:ROM}. Both validation- and training error are described in \cref{Ch:DimRedAl} and provide information about under- and overfitting. Moreover the validation error is the essential metric for validating a model's performance. The $\L2$ on the other hand gives an estimate of how well the model performs on the whole dataset hence is used as a comparative metric against POD.\\ 
To start with a working model first a guess about the some initial hyperparameters is done, which are summarized in \Cref{Tab:First Guess}. These include a mini-batch size of 16, the width of the bottleneck layer is 3 and 5  for \(\hy\) and \(\rare\) respectively and a learning rate of 0.0001. Activation LeakyReLU is applied for the outputs of the input- and  any hidden layer which does not output the code, referred to as activations hidden. Tanh is applied for the output of the last hidden layer in the encoder which outputs the code, referred to as activation code. A visualization of the activation scheme is provided in \cref{Fig:ActScheme}. Moreover 2000 initial number of epochs are used. This might seem exaggerated but is justified by the little amount of input data and the small size of the network which yiels a fast training.
\begin{center}
	\begin{figure}[H]
		\centering
		\input{Figures/Parameterstudy/Fully_Connected/Activations/act_scheme.tex}
		\caption{\footnotesize Scheme of a network with four layers showing where the activations are placed. Activation act. a activates the output of the input layer as well as the output of any other hidden layer except the output of the last hidden layer in the encoder which is activated through act.b and outputs the code.}
		\label{Fig:ActScheme}
		\end{figure}
\end{center}
\begin{table}[H]
	\centering
	\caption{Initial selection for batch size, bottleneck size, number of epochs, learning rate and applied activation functions.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Mini-batch size   & Intrinsic dimensions &   Epochs &Learning rate & Activations hidden/code \\   
		\hline
		16 		&	3/5 &     2000&	    0.0001 & LeakyReLU/Tanh\\
		\bottomrule
	\end{tabular*} \label{Tab:First Guess}
\end{table}
Five designs for finding an optimal number of layers i.e. depth are run. The hidden layers are designed to halve the input at each step. Not within this scope is the bottlenck layer which has a fixed size of 5 and 3 for $\rare$ and $\hy$ respectively and the first and last hidden layer (after the input layer and before the output layer). Those should provide an abstraction without shrinking the incoming data. Note that this design feature was validated in an initial exploration cycle which is not included in this thesis. The five designs range from 10 layers in (a) to 2 layers in (e) always taking one layer away from encoder and decoder hence decreasing by two layers and are as follows:     
\begin{enumerate}
	\item 10 layers with layer widths: 40, 40, 20, 10, 5, 3/5, 5, 10, 20, 40, 40.
	\item 8 layers with layer widths: 40, 40, 20 , 10, 3/5, 10, 20, 40, 40.
	\item 6 layers with layer widths: 40, 40, 20 , 3/5, 20, 40, 40.
	\item 4 layers with layer widths: 40, 40, 3/5, 40, 40.
	\item 2 layers with layer widths: 40, 3/5, 40.
	\end{enumerate}
The model's depth is determined in a primary step, as it sets a consequential part of the model's representational capacity and therefore can initiate over- and underfitting at an early stage in the hyperparameter search. The results of the experimentation are shown in \cref{Fig:Depth} and \cref{Tab:Depth} for both rarefaction levels.\\
\begin{table}[htp]
	\centering
	\caption{Results for the variation of depth. Given are minimum values of training and validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training. The \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Depth & \multicolumn{2}{c}{Minimum training error} & \multicolumn{2}{c}{Minimum validation error} & \multicolumn{2}{c}{\(\L2\) }\\ [.5ex]
		 & \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		10& \num{1.53e-7} & \num{5.96e-7} & \num{2.22e-7} & \num{5.19e-7} & 0.0048 & 0.0091\\ \hline
		8 & \num{1.17e-7 }& \num{2.05e-7} & \num{1.58e-7} & \num{2.32e-7} & 0.0041 & 0.0054\\ \hline
		6 & \num{9.76e-8} & \num{1.40e-7} & \num{1.49e-7} & \num{1.72e-7} & 0.0038 & 0.0045\\ \hline
		4 & \num{6.29e-8} & \num{1.52e-7} & \num{7.74e-8} & \num{1.61e-7} & 0.0031 & 0.0048 \\ \hline
		2 & \num{1.29e-6} & \num{3.29e-6} & \num{1.37e-6} & \num{3.42e-6} & 0.0136 & 0.0217\\ \hline
	\end{tabular*}\label{Tab:Depth}
\end{table}\noindent
For \(\hy\) the lowest validation error of \num{7.74e-8} and an \(\L2\) of 0.0031 is reached with 4 layers, hence constitutes the best performing design out of the five. Additionally, as seen in \cref{Fig:Depth}(left), does a design exceeding 4 layers results in a slight overfitting after around 500 epochs. Less than 4 layers do not reach the validation error and \(\L2\) of the other designs, yielding the conclusion, that the capacity is too low. Overfitting occurs with 4 layers only after the 1000th epoch and is less than with the other three models that show overfitting.\\
For \(\rare\) the lowest validation error of \num{1.61e-7} is reached again with 4 layers. On the other hand the lowest \(\L2\) of 0.0031 and the lowest training error of \num{1.40e-7} are reached with 6 layers. Contrary to the afore discussed case the training error and \(\L2\) are of lower magnitude for 6 layers, except for the validation error. Looking at \cref{Fig:Depth}(right), we observe that the model with 6 layers starts to overfit after the 1500 epochs, yielding a decreasing training error and a stagnating validation error. Hence the model improved in the optimization task which additionally improves the \(\L2\). It's generalization ability, measured by the validation error, did not improve and is greater than the validation error reached with 4 layers. This concludes, a model with 4 layers constitutes the best performing design out of the five.\\
Qualitatively the overall training for both rarefaction levels is very stable. Training and validation error do not diverge exessively and converge early in training. Separation of training and validation error occurs prominently for the hydrodynamic solution. This is thought to be connected to the emersion of sharp shock fronts towards the end of the simulation. This increases variance in the whole dataset and therefore also in the training- and validation set.\\
The number of epochs is now doubled to 4000 epochs because the lowest validation error was achieved towards the end of the training in the previous experiments. Again this is justifiable as one epoch takes less than 1s to finish and no prominent overfitting is observed. 
\begin{table}[htpb!]
	\centering
	\caption{Results for the variation of width. Given is the minimum value of validation error as well as the \(\L2\). The minima where reached around the last 50 epochs of the training, the \(\L2\) is evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Hidden units & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} & \multicolumn{2}{c}{Shrinkage factor}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		50 & \num{1.91e-8}  & \num{5.05e-8} & \num{0.0015}  & \num{0.0025} & 0.06  & 0.01\\ \hline
		40 & \num{2.65e-08} & \num{1.65e-8} & \num{0.0018}  & \num{0.0014} & 0.075 & 0.125\\ \hline
		30 & \num{1.77e-08} & \num{3.40e-8} & \num{0.0015}  & \num{0.0021} & 0.015 & 0.0167\\ \hline
		20 & \num{2.50e-08} & \num{5.25e-8} & \num{0.0017}  & \num{0.0027} & 0.1   & 0.25 \\ \hline
		10 & \num{5.11e-08} & \num{3.97e-7} & \num{0.0025}  & \num{0.0077} & 0.3   & 0.5\\ \hline
	\end{tabular*}\label{Tab:Width}
\end{table}
The width of the two remaining hidden layers is examined in the following. For both the hydrodynamic and the rarefied regime five experiments are conducted, lowering the hidden units of the hidden layers from fifty to ten. Note that the decoder is chosen to be structurally a reflection of the encoder. Therefore only one parameter is changed. Results for \(\hy\) and \(\rare\) are shown in \cref{Tab:Width}. Note that the contribution of over- and underfitting is negligible and therefore the training error is omitted. A model with 30 hidden units in encoder and decoder performs best with \(\hy\) and reaches a validation error of \(\num{1.77e-08}\). The corresponding \(\L2 = \num{1.5e-3}\) with a shrinkage factor of 0.015. Overall the loss of each experiment with \(\hy\) is quiet similar an ranges from \(\num{1.77e-8}\) to \(\num{5.11e-8}\). The \(\L2\) behaves in a similar fashion and is even equal for 50 and 30 layers. A model with 40 hidden units performs best for \(\rare\). The corresponding validation error is \(\num{1.65e-8}\) with \(\L2=\num{1.4e-3}\), which is smaller than \(\hy\). The shrinkage factor is 0.125. In all experiments a model with 10 hidden nodes performs worst. Training and validation error over all 4000 epochs for both experiments can be seen in \cref{Fig:Width}. The aforementioned separation of training- and validation error, that was observed solely for \(\hy\), is mitigated when moving away from 40 hidden units for encoder and decoder. Not shrinking the input in the first hidden layer only serves the performance when using \(\rare\).\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of batch sizes. Given is the minimum value of validation error as well as the the corresponding epoch. The \(\L2\) is also given but evaluated with the model at the last epoch.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Batch Size & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		32& \num{5.40e-8} & \num{2.17e-8} & \num{0.0024}  & \num{0.0017}&4998&4992\\ \hline
		16& \num{1.95e-8} & \num{2.06e-8} & \num{0.0015}  & \num{0.0016}&4999&5000\\ \hline
		8 & \num{2.25e-8} & \num{1.03e-8} & \num{0.0017}  & \num{0.0012}&4965&4961\\ \hline
		4 & \num{1.52e-8} & \num{6.30e-9} & \num{0.0013}  & \num{0.0010}&3956&4534\\ \hline
		2 & \num{1.15e-8} & \num{9.18e-9} & \num{0.0012}  & \num{0.0013}&4956&4872\\ \hline
	\end{tabular*}\label{Tab:Batch}
\end{table}
Next the mini-batch size is analysed. Epochs are increased by 1000 epochs, as training- and validation error show potential to decrease further as seen in \cref{Fig:Width} for \(\rare\) with 40 hidden nodes. Results for $\hy$  and  $\rare$ are displayed in \cref{Tab:Batch}. Experiments are conducted with mini-batch sizes of 2, 4, 8, 16, 32. Batch sizes to the power of 2 are typically chosen as to fully exploit computational capabilities of a GPU i.e. aligning the batch size with the way memory is structured within a GPU. The smallest batch size of 2 yields the lowest validation error of \(\num{1.15e-8}\) with corresponding \(\L2 = 0.0012\) at epoch 4956 for \(\hy\). The lowest validation error with \(\num{6.30e-9}\) is achieved for \(\rare\) at epoch 4534 with a batch size of 4. The corresponding \(\L2 = 0.001\). Compared to a batch size of 16 in the previous experiments we can observe that small batch sizes have a regularizing effect on training as described in \cref{Ch:DimRedAl} and therefore are beneficial to generalization. At the same time, the lower the batch sizes are, the more unstable is the training as seen in \cref{Fig:Batch}. The oscillations which begin with batch sizes of 8 and lower, which make the training unstable, can be battled with a lower learning rate as soon as training starts to tremble. Additionally small batch sizes drastically increase training time which is why a batch size as low as 2 will not be used for the next experiments. In conclusion a batch size of 2 is omitted and for both input data a batch size of 4 is chosen. Furthermore a reduction of the learning rate from \(\num{1e-4}\) to \(\num{1e-5}\) is applied after the 3000th epoch.\\
Eight experiments with different activation functions namely ReLU, ELU, Tanh, SiLU and LeakyReLU are performed. The experiment designs and results are given in \cref{Tab:activations} for hidden and code activations. With \(\hy\) a combination of ELU and ELU for hidden and code activation yields the best results in validation error with \num{4.44e-9} and a corresponding \(\L2 = 0.0008\). These values are achieved at the last epoch. For \(\rare\) a combination of ReLU and ReLU for hidden and code activation produces a validation error of \num{7.18e-9} and a corresponding \(\L2 = 0.0009\). Both values are also reached close to the last epoch. Note that all models reach their lowest loss at or very close to the last epoch. The reason is the stable training after the 3000th epoch, where the learning rate is lowered to \num{1e-5} as seen in \cref{Fig:Activations}. This measure shows in all experiments an immediate success for learning. Both validation and training error fall at the 3001st epoch and only decrease slightly thereafter. This behavior clearly shows that the updates to the free parameters \(\frepar\) were too big which prohibitively slowed down or even prevented the learning process. Small updates to \(\frepar\) made all models quickly reach a minimum. Therefore 3000 epochs or even less could have been enough to reach similar results with a lower learning rate.\\
If the learning rate could have been reduced whilst producing similar results remains unanswered here as the results are satisfactory. Note that for \(\rare\) in the previous experiment a validation error of \num{6.30e-9} was achieved which is slightly lower than the current result. Nonetheless it is decided to take the current model as the final result.\\
\begin{table}[H]
	\centering
	\caption{Results for the variation of activations for hidden-/code layers. Given is the minimum value of validation error as well as the the corresponding epoch and the \(\L2\). The \(\L2\) is evaluated with the models saved when the minimum validation error was achieved during training.}
	\begin{tabular*}{15.5cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Activations hidden/code & \multicolumn{2}{c}{Validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& \(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)&\(\hy\)&\(\rare\)\\
		\hline
		ReLU/ReLU 	       & \num{9.79e-9} & \num{7.18e-9} & \num{0.0010}  & \num{0.0009}&5000 &4998\\ \hline
		ELU/ELU            & \num{4.44e-9} & \num{1.11e-8} & \num{0.0008}  & \num{0.0012}&5000 &5000\\ \hline
		Tanh/Tanh 	       & \num{7.83e-9} & \num{2.58e-8} & \num{0.0011}  & \num{0.0018}&5000 &5000\\ \hline
		SiLU/SiLU 	       & \num{7.69e-9} & \num{1.37e-8} & \num{0.0011}  & \num{0.0013}&5000 &5000\\ \hline
		LeakyReLU/LeakyReLU& \num{1.86e-8} & \num{9.39e-9} & \num{0.0015}  & \num{0.0010}&5000 &4997\\ \hline
		ELU/Tanh           & \num{5.49e-9} & \num{1.87e-8} & \num{0.0008}  & \num{0.0014}&5000 &5000\\ \hline
		LeakyReLU/Tanh     & \num{1.00e-8} & \num{1.42e-8} & \num{0.0010}  & \num{0.0012}&4997 &4992\\ \hline
		ELU/SiLU           & \num{8.11e-9} & \num{1.93e-8} & \num{0.0011}  & \num{0.0015}&5000 &5000\\ \hline
	\end{tabular*}\label{Tab:activations}
\end{table} 
The final hyperparameters for both input data are summarized below in \cref{Tab:Final}. From the initial models to the final models the decrease in validation error gained \(\approx \num{1.5e-7}\) for \(\hy\) and \(\approx \num{7.2e-8}\) for \(\rare\) which is 93\% of the initial values for both models.
\begin{table}[H]
	\centering
	\caption{Summary of the final hyperparameters for both input data.}
	\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
		\toprule
		Input data & Act. hidden/code & Batch size & Width & Depth & Learning rate & Epochs\\ [.5ex]
		\hline
		$\hy$ &  ELU/ELU & 4 & 30 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
		$\rare$ & ReLU/ReLU & 4 & 40 & 4 & \num{e-4}/\num{e-5} & $\approx 3000$\\ \hline
	\end{tabular*}\label{Tab:Final}
\end{table}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Depth/hydro_depth.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Depth/rare_depth.tex}
		\caption{Five experiments over the different depth with $\hy$ left and $\rare$ right. The number of layers used for every experiment are given. Training and validation loss are shown over 2000 epochs.}
		\label{Fig:Depth}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Width/hydro_width.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Width/rare_width.tex}
		\caption{Five experiments over different width with $\hy$ left and $\rare$ right. The number of nodes used for every experiment are given. Training and validation loss are shown over 4000 epochs.}
		\label{Fig:Width}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/hydro_batch.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Batch_Size/rare_batch.tex}
		\caption{Five experiments over different batch sizes with $\hy$ left and $\rare$ right. The batch size used for every experiment is given. Training and validation loss are shown over 5000 epochs.}
		\label{Fig:batch}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act.tex}
		\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act.tex}
	\end{figure}
\end{center}
\begin{figure}[H]
	\input{Figures/Parameterstudy/Fully_Connected/Activations/hydro_act2.tex}
	\input{Figures/Parameterstudy/Fully_Connected/Activations/rare_act2.tex}
		\caption{Eight experiments with different combinations of activation functions for \(\hy\) (left) and \(\rare\) (right). Shown are training- and validation error over 5000 epochs.}
\end{figure}\label{Fig:Activations}
%\newpage
\chapter{Hyperparameters for the Convolutional Autoencoder}
\label{Ch:ApB}

The finding of appropriate hyperparameters for the convolutional autoencoder is described here. The hyperparameters include  batch size, non-linear activation functions, number of epochs for training and learning rate and number of layers i.e. depth. Depth comprises kernel size, stride width as well as number of channels per layer.\\
This analysis follows the prior finding of hyperparameters for the fully connected autoencoder, hence utilizes insights thereof. Additionally this analysis follows a slightly different scheme than before, as there are solely 40 examples for training and validation for both rarefaction levels \(\hy\) and \(\rare\). Hence it is tried to find a combined model that performs well on both rarefaction levels. With this measure datasets for both rarefaction levels are concatenated to one dataset of 80 examples for training and validation. Furthermore this approach gives an answer to how well a model can generalize about the BGK model for different rarefaction levels in Sod's shock tube.\\
Again we start with a small model which architecture and hyperparameters are summarized in \cref{Tab:small}.
\begin{table}[htbp!]
\centering
\caption{Initial selection of hyperparameters for training and architecture. The initial model compirses 6 layers, of which 4 are either convolutional (Conv.) or transposed convolutional (Tr.Conv.). The learning rate is switched from \num{e-4} to \num{e-4} at the 500th epoch as in \cref{Ch:ApA}. Depth counts the number of convolutional/transposed concolutional layers and excludes the fully connected layers.}
\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
	\toprule
	\multicolumn{4}{c}{Encoder} & \multicolumn{4}{c}{Decoder}\\ [.5ex] \hline
	Layer & Type & Channels & Output size & Layer & Type & Channels & Output size \\ 
	\hline
	1 & Conv  & 8  & $5\times 40$ & 4 & Lin.     & -  & 128            \\ \hline
	2 & Conv. & 16 & $1\times 8$  & 5 & Tr.Conv. & 16 & $1\times 8$    \\ \hline
	3 & Lin.  & -  & 5		      & 6 & Tr.Conv. & 8  & $25\times 200$ \\ 		
\end{tabular*}
\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c c c @{} }
	\toprule
	Epochs & Act. hidden/code & Batch size & Depth & Kernel size & Stride width & Learning rate\\ [.5ex]
	\hline
	1000 &  ReLU/ReLU  & 2 & 4 & \(5\times 5\) & \(5\times 5\) & \num{e-4}/\num{e-5} \\ \hline
\end{tabular*}\label{Tab:small}
\end{table} 
The activations and learning rate stem from conclusions made in \cref{Ch:ApA}. The application of activation functions follows the same sheme as described in \cref{Ch:ApA} seen in \cref{Fig:ActScheme}. Kernel size and stride width are chosen as to circumvent checkerboard artefacts as described in \cref{Ch:DimRedAl}. The number of channels is inspired by the architecture chosen in \cite{Carlberg}.\\
The number of available examples for training and validation limited as previously stated . Therefor the split in training and validation sets encompasses the risk of a bias in one of the sets. For instance, if out of the 40 examples, a few of them ,maybe 5 , contain considerable variance to the other examples and all of them are found in the training set, then the validation error could not estimate the model's ability to generalize. Therefore the k-fold algorithm described in \cite{Goodfellow} is adopted. The k-fold algorithm provides from the complete shuffled dataset, k independent splits into training and validation set with which the model is trained. By that, each example gets the chance to be either in the training or validation set once. For an 80/20 split as used in this thesis and described in \cref{Ch:DimRedAl}, five independent folds can be obtained. Hence with 80 available examples the training set \(\btch_{train}\) consists of 74 examples and the validation set \(\btch_{val}\) of the remaining 16 examples. The results of said preliminary experiment are summarized in \cref{Tab:kFold} and \cref{Fig:kFold}.
\begin{table}[htbp!]
	\centering
	\caption{Training of five independent folds. Summary of minimum training- and minimum validation error for a small model with two convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached. The mean and standard deviation of the validation error are \num{3.55e-5} and \num{2.13e-5} respectively with a corresponding variance of \num{4.56e-10}.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Fold & Minimum training error & Minimum validation error & \(\L2\) & Epoch\\ [.5ex]
		\hline
		 1   & \num{2.4e-5}           & \num{7.0e-5}             & 0.053   & 498  \\  
		\hline
		2    & \num{1.4e-5}           & \num{1.3e-5}             & 0.044   & 1000\\
		\hline
		3    & \num{1.7e-5}           & \num{5.0e-5}             & 0.057   & 1000\\
		\hline
		4    & \num{1.7e-5}           & \num{1.8e-5}             & 0.039   & 997\\
		\hline
		5    & \num{1.6e-5}           & \num{2.7e-5}             & 0.053  & 1000\\
		\hline
	\end{tabular*}\label{Tab:kFold}
\end{table}
The lowest validation error of \num{1.3e-5} is achieved with the second fold at the last epoch. The corresponding \(L2 = 0.044\). As training error is lower than validation error it is assumed, that the \say{difficult} examples are within the training set. Therefore the training set could be biased. The second lowest validation error of \num{1.8e-5} and a corresponding \(\L2=0.039\) is achieved with fold 4 at the 997th epoch. For both folds training and validation error evolve in unison and get instable even before the 500th epoch. The instability is eliminated by lowering the learning rate at said epoch, but the training does not improve thereafter as in \cref{Ch:ApA}. For the other folds a seperation of training and validation error can be observed and less instability. The mean validation error of \num{3.55e-5} gives an estimate of the models performance with a standard deviation of \num{2.13e-5} and variance of \num{4.56e-10}. For the continuation of experiments fold 4 is used, as it provides a balanced split in training- and validation split which also manifests in the lowest \(\L2\) of all folds.\\
Next the capacity of the model is increased by successively adding convolutional layers, thus obtaining two additional models. One encompasses three convolutional layers, the other four convolutional layers in encoder and decoder. An exact description of both architectures is summarized in \cref{Tab:Layer}. The maximum number of channels for this analysis is kept at 16. This measure decreases the growth rate of channels per layer, which is quadratic for the first model and now scales to linear in order to increase capacity rather smooth. Note that it is also possible to keep the growth rate constant while adding layers. Here this is done in the following steps.\\
Kernel size and stride width shrink the input as described in \cref{Ch:DimRedAl}. In the previous model shrinkage evolved with a pace of \(\frac{1}{5}\). This reduction rate is able to reduce the input size over two succesive layers to unity. Hence the same kernel size and stride width can only be adopted by using excessive zero padding between successive convolutional layers. To overcome this issue a balance between zero padding, which cannot be omitted for four convolutional layers with the given input dimensions, and kernel size and stride width needs to be found. Here both features are chosen to have sizes \(3\times 3\). Extra padding, which is needed for one to reduce the shrinkage rate per layer in the encoder and for the other to transpose the shrinkage of the encoder in the decoder, is summarized in \cref{Tab:Layer}. In \texttt{pytorch}'s implementation of transposed convolutional layers a handy parameter can be used called output padding. This parameter provides a measure to effectively increase the output size by one, left or at the bottom of the output, to resolve the ambiguity when \(\text{stride}>1\). Then the respective convolutional layer maps multiple input sizes to the same output size. Taking for example the second and sixth layer of the model with three convolutional layers in encoder and decoder. Here the convolutional layer maps a size of 23 to 8. But the respective transposed convolutional layer maps size 8 to size 22. In this case output padding resolves the issue by increasing the output size by one.         
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/kfold.tex}
		\label{Fig:kFold}
		\caption{Training- and validation error over 1000 epochs for five independent folds.}
	\end{figure}
\end{center}
Results for both models are summarized in \cref{Tab:Layer}. Training- and validation error for the model with three convolutinal layers achieve slightly better results as the small model. On the contrary does the \(\L2\) not achieve the resuls of the small model with \(\L2=0.044\). In \cref{Fig:Layer} the evolution of training- and validation error is depicted over 2000 epochs. The model with three convolutional layers shows an unstable training after the 500th epoch, which is ignored for now. It can be observed that the model with four convolutional layers is underfitting and does not achieve comparable results to the small model. 
\begin{table}[htbp!]
	\setlength{\tabcolsep}{1pt}
	\footnotesize
	\caption{Architecture for the models with three (left) and four (right) convolutional layers in encoder and decoder. Kernel size and stride width are \(3\times 3\) for both features and models.}
	\begin{minipage}{.5\textwidth}
		\begin{tabular*}{.9\textwidth}{ @{\extracolsep{\fill}} c c c c c @{} }
			\toprule
			\multicolumn{4}{c}{Encoder} \\ [.5ex]\hline
			Layer & Type & Channels & Padding & Output  \\ 
			\hline
			1 & Conv  & 4  & in: 1/1 & $9\times 67$  \\ \hline
			2 & Conv. & 8  & in: 0/1 & $3\times 23$  \\ \hline
			3 & Conv. & 16 & in: 0/1 & $1\times 8$   \\ \hline
			4 & Lin.  & -  & -   & 5		      	 \\ 
			\\
			\toprule
			\multicolumn{4}{c}{Decoder}		\\ [.5ex]\hline
			Layer & Type & Channels & Padding & Output \\
			\hline
			5 & Lin.     & -  & - & 128       	 		        \\ \hline
			6 & Tr.Conv. & 16 & in: 0/1 out: 0/1 & $1\times 8$  \\ \hline
			7 & Tr.Conv. & 8  & in: 0/1 out: 0/1 & $3\times 23$ \\ \hline
			8 & Tr.Conv. & 1  & in: 0/1 & $25\times 200$        \\ \hline
			\\
		\end{tabular*}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\begin{tabular*}{.9\textwidth}{ @{\extracolsep{\fill}} c c c c c @{} }
			\toprule
			\multicolumn{4}{c}{Encoder} \\ [.5ex]\hline
			Layer & Type & Channels & Padding & Out  \\ 
			\hline
			1 & Conv  & 2  & in: 3/2 & $10\times 68$ \\ \hline
			2 & Conv. & 4  & in: 3/2 & $5\times 24$  \\ \hline
			3 & Conv. & 8  & in: 3/2 & $3\times 9$   \\ \hline
			4 & Conv. & 16 & in: 3/3 & $1\times 3$   \\ \hline
			5 & Lin.  & -  & & 5		      \\  
			\toprule
			\multicolumn{4}{c}{Decoder}		\\ [.5ex]\hline
			Layer & Type & Channels & Padding & Out \\
			\hline
			6  & Lin.     & -  & - & 128       	 \\ \hline
			7  & Tr.Conv. & 16 & - & $3\times 9$   \\ \hline
			8  & Tr.Conv. & 8  & in: 2/2 out: 0/1 & $5\times 24$   \\ \hline
			9  & Tr.Conv. & 4  & in: 3/2 & $9\times 68$   \\ \hline
			10 & Tr.Conv. & 1  & in: 1/2 & $25\times 200$ \\ \hline   
		\end{tabular*}
	\end{minipage}
\end{table} 
\begin{table}[htbp!]
	\centering
	\caption{Results for increasing number of layers. Summary of minimum training- and minimum validation error for a model with three and a model with convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Layer & Minimum training error & Minimum validation error & \(\L2\) & Epoch\\ [.5ex]
		\hline
		3   & \num{1.4e-5}           & \num{1.5e-5}             & 0.044   & 1975  \\  
		\hline
		4    & \num{7.3e-4}           & \num{8.1e-4}             & 0.327   & 1995\\
		\hline
	\end{tabular*}\label{Tab:Layer}
\end{table} 
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/Layer.tex}
		\label{Fig:Layer}
		\caption{Increasing the number of layers. Shown are training- and validation error over 2000 epochs a model with two and a model with three convolutional layers in the encoder. Channels are chosen to reach a  maximum of 16 in the last layer of encoder. Kernel size and stride width are both \(3\times 3\).}
	\end{figure}
\end{center}
Next the batch size is increased from two to four. The initial batch size was chosen so small as the limited available data permits reasonable training time. The results for all three models are shown in \cref{Tab:Batch4}. The models with two and three convolutional layers benefit from increasing the batch size. The small model reaches the best training- and validation error with \num{6.0e-6} and \num{8.0e-6} respectively and \(\L2=0.03\). In \cref{Fig:Batch} it can be observed, that the increased batch size results in a stable training for both shallow models. The underfitting of the deep model with four convolutional layers in encoder and decoder is increased.
\begin{table}[htbp!]
	\centering
	\caption{Increasing the batch size to four. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Layer & Min. training error & Min. validation error & \(\L2\) & Epoch\\ [.5ex]
		\hline
		2   & \num{6.0e-6}             & \num{8.0e-6}             & 0.030   & 1999  \\
		\hline  
		3   & \num{1.0e-5}              & \num{1.3e-5}            & 0.038   & 1965  \\  
		\hline
		4    & \num{6.0e-3}            & \num{6.9e-3}             & 0.94   	& 109\\
		\hline
	\end{tabular*}\label{Tab:Batch4}
\end{table}
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/Batch.tex}
		\label{Fig:Batch}
		\caption{Increasing the batch size to four. Shown are training- and validation error over 2000 epochs for the models with two, three and four convolutional layers in the encoder.}
	\end{figure}
\end{center}
The underfitting of the deepest model is now tackled with increasing te capacity of the model by raising the channel growth rate to quadratic, as it is found in \cite{Carlberg}. The same is performed for the model with three convolutional layers in encoder and decoder. Additionally the channel sizes of the small model are increased. The results and available channel sizes in order of appearance in the encoder are summarized in \cref{Tab:Channels}. Note that the decoder mirrors the channel sizes of the encoder. The small model does not learn at all as seen in \cref{Fig:Channels}. The training was repeated several times, but no improvement could be produced. The reason for this behavior is not known and can't be classified as underfitting and overfitting, as there is no change in training- and validation error observed. On the other hand does the deep network with four convolutional layers achieve comparable results to the small model in the previous experiment. The model with three convolutional layers improves slightly. In conclusion, the quadratic growth rate of the channels starting from eight channels is suitable for the following experiments, for which the three layer model is discarded as the other models reach the lowest validation error so far. 
\begin{table}[htbp!]
	\centering
	\caption{Increasing the channel growth rate to quadratic. Summary of minimum training- and minimum validation error for the models with two, three and four convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached. Note that the channel sizes of the two layer model are only increased, the growth rate has already been quadratic.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c c c c c @{} }
		\toprule
		Layer & Channels  & Minimum training error & Minimum validation error & \(\L2\) & Epoch \\ [.5ex]
		\hline
		2    & 16,32   	  & \num{6.8e-3}           & \num{7.6e-3}             & 1.0     & 0     \\
		\hline  
		3    & 8,16,32    & \num{9.0e-6}            & \num{1.1e-5}             & 0.037   & 1985  \\  
		\hline
		4    & 8,16,32,64 & \num{7.0e-6}           & \num{9.0e-6}             & 0.033   & 1953  \\
		\hline
	\end{tabular*}\label{Tab:Channels}
\end{table}   
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/Channels.tex}
		\label{Fig:Channels}
		\caption{Increasing the channel growth rate to quadratic. Shown are training- and validation error over 2000 epochs for the model with two, three and four convolutional layers in the encoder.}
	\end{figure}
\end{center}
The variation of activation functions follows the same scheme as already outlined in \cref{Ch:ApA}. Eight experiments are conducted using ReLU, ELU, Tanh, SiLU and LeakyReLU in different combinations for the hidden convolutional layers and the code layer. Results are summarized in \cref{Tab:Activations}. Training- and validation error over 2000 epochs are displayed in \cref{Fig:ActivationsC}. For both models the validation error reaches the region of \num{e-6} using combinations of ELU, SiLU and Tanh. With applying rectifiers, the validation error stays in the region of \num{e-5}. Especially, with utilizing SiLU/SiLU for the four layer model, the validation error reaches a minimum of \num{6.0e-6}. Utilizing the combinations ELU/SiLU and ELU/Tanh for the two layer model also yields a minimum validation error of \num{6.0e-6}. In addition, this combination produces the smallest \(\L2\) with \(\L2=0.024\) applying ELU/SiLU for the four layer model and applying ELU/Tanh for the two layer model produces \(\L2=0.023\). These values correspond to the minimum training error which is \num{4.0e-6} and \num{3.0e-6} respectively. Here the four layer model overfits after the 500th epoch which produces, compared to the other models, a significant generalization gap. The same logic applies to the two layer model with ELU/Tanh, but yields a smaller generalization gap. In general, specifically the training of the four layer model is instable for all combinations of activation functions. The greater amount of free parameters, compared to the two layer model, requires a smaller learning rate or a greater batch size to stabilize the training. Finally, the combination of SiLU/SiLU is chosen for the four layer model and ELU/SiLU for the two layer model. Both choices lead to the lowest validation error, while maintaining the generalization gap small.\\
\begin{table}[htbp!]
	\setlength{\tabcolsep}{.01pt}
	\centering
	\caption{Variation of activations for hidden-/code layers. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached.}
	\begin{tabular*}{17cm}{ @{\extracolsep{\fill}} c c c c c c c c c @{} }
		\toprule
		Act. hid./code & \multicolumn{2}{c}{Min. training error}&\multicolumn{2}{c}{Min. validation error} & \multicolumn{2}{c}{$\L2$} &\multicolumn{2}{c}{Epoch}\\ [.5ex]
		& 2 Layer& 4 Layer& 2 Layer& 4 Layer& 2 Layer& 4 Layer\\
		\hline			%2			%4			%2                  %4     %2    %4
		ELU/ELU 	     &\num{5.0e-6} &\num{4.0e-6} &\num{7.0e-6} & \num{7.0e-6} & \num{0.026}  & \num{0.031}&1969  &1365\\ \hline
		ELU/SiLU         &\num{5.0e-6} &\num{4.0e-6} &\num{6.0e-6} & \num{8.0e-6} & \num{0.026}  & \num{0.024}&1991  &1808\\ \hline
		ELU/Tanh 	     &\num{3.0e-6} &\num{5.0e-6} &\num{6.0e-6} & \num{9.0e-6} & \num{0.023}  & \num{0.029}&1998  &1498\\ \hline
		Leaky/Leaky 	 &\num{6.0e-6} &\num{7.0e-6} &\num{1.1e-5} & \num{1.0e-5} & \num{0.032}  & \num{0.032}&1976  &1971\\ \hline
		Leaky/Tanh       &\num{5.0e-6} &\num{6.0e-6} &\num{7.0e-6} & \num{9.0e-6} & \num{0.030}  & \num{0.035}&1977  &1722\\ \hline
		ReLU/ReLU        &\num{8.0e-6} &\num{7.0e-6} &\num{1.3e-5} & \num{1.1e-5} & \num{0.036}  & \num{0.036}&1984  &1989\\ \hline
		SiLU/SiLU        &\num{6.0e-6} &\num{6.0e-6} &\num{8.0e-6} & \num{6.0e-6} & \num{0.030}  & \num{0.035}&1972  &1550\\ \hline
		Tanh/Tanh        &\num{8.0e-6} &\num{5.0e-6} &\num{8.0e-6} & \num{8.0e-6} & \num{0.033}  & \num{0.030}&1999  &975 \\ \hline
	\end{tabular*}\label{Tab:Activations}
\end{table}
A comparison with the validation loss of the fully connected model suggests, that the two- and three layer model gets stuck at a local minimum of  \(\min J(\frepar)\). Therefore the input data is augmentated for the following experiments. All examples in the training- and validation set are rotated around their center by \(180\degree\) and flipped about their central vertical axes. These methods add examples the input data without altering the information about the flow field present in each example. In \cref{Fig:DataAug} is an example in its original, rotated and flipped version. The flow field stays the same, just the direction in which it evlves has been altered. Using this method, available examples for training and testing triple from 80 to 240 examples.\\
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/ExpDatAug.tex}
		\caption{Rotated and flipped version of one original example from the dataset showing \(v\) over \(x\) and \(t\).}
		\label{Fig:DataAug}
	\end{figure}
\end{center}
Results for using augmented data are summarized in \cref{Tab:DataAug}. Two experiments are conducted for each model. One with learning rate adjustment, which decreases from \num{1e-4} to \num{1e-5} after the 1250th epoch, and one without. The minimum validation error ranges between \num{2.2e-5} and \num{1.4e-5}. Therefore data augmentation could not decrease the validation error further. Instead it increases. But, as seen in \cref{Fig:DatAug} and \cref{Fig:DatAugSched}, the training is stabilized compared to the previous experiments. In addition overfitting cannot be obeserved for the two layer model. The four layer model overfits after the around the 1150th epoch. Furthermore both models benefit from reducing the learning rate. A drop in training- and validation error can be observed after the 1250th epoch for both models.
\begin{table}[htbp!]
	\centering
	\caption{Data augmentation with and without learning rate adjustement after the 1250th epoch. Summary of minimum training- and minimum validation error for the models with two and four convolutional layers in the encoder as well as the corresponding \(\L2\) and the epoch in which those values are reached.}
	\begin{tabular*}{16cm}{ @{\extracolsep{\fill}} c c c c c @{} }
		\toprule
		Layer   & Min. training error & Min. validation error & \(\L2\) & Epoch \\ [.5ex]
		\hline
		2      & \num{1.5e-5}            & \num{1.4e-5}             & 0.048     & 2000  \\
		\hline  
		2 (lr adjusted)  & \num{2.2e-5}  & \num{2.2e-5}              & 0.064    & 2000  \\  
		\hline
		4       & \num{7.0e-6}           & \num{1.6e-5}              & 0.034    & 1991  \\
		\hline
		4 (lr adjusted)  & \num{1.1e-5}  & \num{1.9e-5}             & 0.045     & 2000  \\  
		\hline
	\end{tabular*}\label{Tab:DataAug}
\end{table}    
\begin{center}
	\begin{figure}[htbp!]
		\input{Figures/Parameterstudy/Convolutional/DatAug.tex}
		\caption{Using augumented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.}
		\label{Fig:DatAug}
		\vspace{-1cm}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/DatAugSchedule.tex}
		\caption{Learning rate adjustment with augmented data. Shown are training- and validation error over 2000 epochs for the model with two and four convolutional layers in the encoder.}
		\label{Fig:DatAugSched}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/2l_activations.tex}
		\input{Figures/Parameterstudy/Convolutional/4l_activations.tex}
	\end{figure}
\end{center}
\begin{center}
	\begin{figure}[H]
		\input{Figures/Parameterstudy/Convolutional/2l_2_activations.tex}
		\input{Figures/Parameterstudy/Convolutional/4l_2_activations.tex}
		\caption{Eight experiments with different combinations of activation functions for the model with two (left) and four (right) convolutional layers in encoder. Shown are training- and validation error over 2000 epochs.}
		\label{Fig:ActivationsC}
	\end{figure}
\end{center}
To this end the finding of hyperparameters for a convolutional autoencoder using both rarefaction levels as input data failed to produce comparable results to those obtained by the fully connected autoencoders using both rarefaction levels as separate datasets. The analysis stops here out of brevity, but is far from an end. Changing the kernel size and especially the stride width, which yields non overlapping kernel positions in all models, changing the loss function to for example \texttt{pyTorch}'s BCEWithLogitsLoss or even adding more layers and so forth are still not analyzed, but are proposed for further investigations. As a final model the two layer model trained without data augmentation is chosen. This model encompasses the smallest number of free parameters while achieving the lowest validation error.
\begin{table}[htbp!]
	\centering
	\caption{Final model.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c c c @{} }
		\toprule
		Layer & Channels  & Activations & Batch size & Learning rate & Num. Epochs \\ [.5ex]
		\hline
		2    & 8,16   	  & ELU/SiLU    & 4           & \num{1e-4} & 2000 \\
		\hline
	\end{tabular*}\label{Tab:FinalC}
\end{table} 
  