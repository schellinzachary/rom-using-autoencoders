% !TEX root = master.tex

\chapter{Model Order Reduction}
\label{Ch:ROM}
%\pagenumbering{arabic}


In this chapter model order reduction (MOR) of the BGK-model in the Sod shock tube will be introduced for which POD and in particular autoencoders are adopted to obtain a reduced basis (RB).\\
Model order reduction is a technique used for reducing the computational cost when evaluating PDE's \cite{Bernard}\cite{Carlberg}\cite{ohlberger2015reduced}. To achieve this, the solution to the PDE is being approximated by reducing one or more of it's dimensions. The reduction is performed by a mapping onto a low dimensional manifold. In our case the solution to the BGK-model is \(f(x,v,t) \in \mathbb{R}^3\). Now we could reduce i.e. \(v\) to \(n\), with \(o\) beeing the number of elements in \(v\) and \(p\) in \(n\). With \(p << o\) we obtain a reduced order model (ROM) \(q(x,n,t)\) of significantly lower dimension. In particular \(n\) is called the reduced basis or the intrinsic variables. In \cref{Ch:BGK} we saw that the BGK-model is a PDE which through discretization in the spatial dimension \(x\) and the velocity dimension \(v\) holds a system of \(KJ\) ODE's in time in 1D and \(K^3J^3\) ODE's in time in 3D.  By the reduction of \(v\) we arrive at \(nJ\) ODE's in time for 1D and \(nJ^3\) ODE's in time in 3D, though we discuss the 1D case only. This example should illustrate the amount of computations saved by MOR. The mapping from \(f(x,v,t)\) to \(q(x,n,t)\) is implemented through a reduction algorithm, which also performs a remapping back to \(\tilde{f}(x,v,t)\), such that the distance \(||f - \tilde{f}||\) is small.\\
To sum up, the idea that every high dimensional dynamical-state space \(W\), also called solution manifold, where in our case \(f(x,v,t) \in W\) is element of \(W\), can be mapped onto a state-space i.e. \(V\) of lower dimension with \(q_n(\mu_i) \in V\), is exploited within MOR \cite{ohlberger2015reduced}. Here \(i\) gives the number of variables that are left after the reduction and \(p\) beeing the number of so called intrinsic variables \(n\). The state space of lower dimension is called the intrinsic solution manifold \(V\) with \(q_n(\mu_i) \in V\) \cite{Carlberg}.\\
\begin{figure}[H]
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow.tex}
		\subcaption{Evolving the BGK-model in the Sod shock tube in time is generated through evaluating the FOM in space \(x\), velocity \(v\) and time \(t\), which yields the solution \(f(x,v,t)\). The operator \(Q\) is here the FOM described in \cref{Ch:BGK}.}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow_red.tex}
		\subcaption{Evolving the ROM of the BGK-model in the Sod shock tube in time through evaluating the ROM at \(n\) and \(\mu_i\), yields an approximation to the FOM solution \(\tilde{f}\). The operator \(Q\) is here either POD or a neural network described in \cref{Ch:DimRedAl}.}
	\end{subfigure}
	\caption{Outline of the correlation between the FOM solution and the approximation obtained from the ROM.}
\end{figure}
MOR is partitioned into two successive phases called the \textit{offline} - and the \textit{online} phase. During the offline phase \textit{snapshots} of a dynamical-system are generated through experiments or simulations of the full order model (FOM). The snapshots \(F = \{f(t1),...,f(t_n)\}\) are created once, each representing one moment in time of the dynamical system. Thus in our case one needs a snapshot database of solutions \(f(x,v,t)\) of the BKG-model in the Sod shock tube. Next a mapping \(Q\) is constructed such that \(\tilde{f} = Q(f)\), for which \(f(t_i) \approx \tilde{f}(t_i)\), reducing the dimensionality of the FOM solution as outlined before. During the online phase the reduced order model is evaluated and the error is estimated by 
\begin{equation}
	\L2 = \frac{||f - \tilde{f} ||_2}{||f||_2}
\end{equation}
which is called the relative \(\L2\)-Error norm. The abbreviation \(\L2\)-Error is used from here on. Therefore the online phase may be described as a stage of independence from the full order model.
Following \cite{ohlberger2015reduced} and \cite{Carlberg}, the success when building a ROM through linear reduction methods like the POD \cref{Sec: POD}, depends on a rapidly decaying Kolmogorov N-width. In particular advection dominated problems exhibit a slow decay of the Kolmogorov N-width. Thus yielding a need for non-linear methods like autoencoders described in \cref{Sec:AE}. The Kolmogorov N-width is given by
\begin{equation}
	d_{V}(W):= \sup_{f \in W} \inf_{\tilde{f} \in V} ||f-\tilde{f}||
\end{equation}
and gives the worst best approximation error for elements of \(W\). The convergence behaviour of the Kolmogorov N-width for advection dominated problems, especially when jump conditions are involved as in the Sod shock tube \cref{Ch:BGK}, decays with
\begin{equation}
	d_n(W) \leq \frac{1}{2} n^{-1/2},
	\label{Eq:KolmoAdv}
\end{equation}
where \(n\) denotes the number of RB or intrinsic variables. Note that hereafter we will solely use the term intrinsic variables\cite{ohlberger2015reduced}. The relevance of this contribution, using non-linear methods for MOR in rarefied gas dynamics, was already implied in the beginning and now takes shape. The BGK-model in the sod shock tube describes an advection problem with jump discontinuities \cref{Ch:BGK}. Thus linear reduction methods should fail. We will see in the following, that this is only partly true, because the advection character as well as the formation of sharp shock fronts heavily depends on the rarefaction of the gas as described in \cref{Ch:BGK}.
\section{Offline Phase}
\subsection{Full order BGK-model}\label{Sec: FOM}
The FOM is the 1D BGK-model in the Sod shock tube for two levels of rarefaction, gratefully provided by Julian K\"ollermeier and the Departement of Mathematics at the RWTH Aachen. The Sod shock tube is discretized in space \(x\) with 100 nodes, in velocity \(v\) with 40 nodes for 25 time steps in time \(t\), as presented in \cref{Tab:Setup} and \cref{Fig:SodProbSetup}. Details about the BGK-model and Sod's shock can be found in \cref{Ch:BGK}.
\begin{table}[htp]
	\centering
	\caption{Problem setup for the BGK model in the Sod shock tube. The diaphragm is positioned at \(x_d=0.5025\). For \(x<x_d\) the gas is present, for \(x\geq x_d\) particles are absent.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c @{} }
		\toprule
		Variable   & Number of nodes \(i\)&   Domain extension& Step size (uniform)\\   
		\hline
		\(x\) 		&	200&     [0.0025,0.9975]&	    0.00499\\
		\(v\)       &   40 &  		    [-10,10]&	    0.05128\\
		\(t\)   	&	25 &        	[0,0.12]&	      0.005\\
		\bottomrule
	\end{tabular*} \label{Tab:Setup}
\end{table}
One can be viewed as a slip flow \cite{schaaf} with \(\Kn=0.01\), hereafter referred to as \(\rare\). The dilution up to this level of rarefaction is little though leading to inaccuracies when employing the common Navier Stokes equations. Therefore the NFS-equations (Navier-Stokes-Fourrier) could be used \cite{NumaKUL}. The other is situated in the continuum flow regime with \(\Kn=0.00001\) for which the Navier-Stokes equations can be utilized without hesitation. A detailed description can be found in \cref{Ch:BGK}. Hereafter the continuum flow will be referred to as \(\hy\). Note that both \(\hy\) and \(\rare\) are three dimensional tensors containing \(f(x,v,t)\).\\
The reduction algorithms, introduced in \cref{Ch:DimRedAl}, require a distinct reshaping of the input data before they can be used. The preprocessed matrix for the FCNN, \(\mae\), is shown in \cref{Eq:AE_matrix}. Each row of \(\mae\) contains a sample shown to the FCNN. In turn we aquire \(x_it_i=5000\) samples. The preprocessed matrix for the CNN, \(\mconv\), is shown in \cref{Eq:CNN_matrix}. We obtain \(v_i=40\) samples, each containing a matrix \(\pi_{CNN}^{x_i\times t_i}\) holding the information about \(f(x,t)\) for a fixed point in \(v\). POD uses the \(\mae\) matrix or its transposition.\\
\begin{minipage}{.55\linewidth}
	\begin{equation}
	\mae = \begin{bmatrix}
	f(v_1,t_1,x_1)&\cdots &f(v_n,t_1,x_1) \\
	f(v_1,t_1,x_2)&\cdots &f(v_n,t_1,x_2) \\
	\vdots& \vdots & \vdots\\
	f(v_1,t_1,x_n)&\cdots &f(v_n,t_1,x_n)\\
	f(v_1,t_2,x_1)&\cdots &f(v_n,t_2,x_1)\\
	\vdots & \vdots & \vdots\\
	f(v_1,t_n,x_n)&\cdots &f(v_n,t_n,x_n)
	\end{bmatrix}
	\label{Eq:AE_matrix}
	\end{equation}
\end{minipage}%
\begin{minipage}{.45\linewidth}
	\begin{equation}
	\mconv= \begin{bmatrix}
	n_{Filters},&f(v_1,\textbf{t},\textbf{x})\\
	n_{Filters},&f(v_2,\textbf{t},\textbf{x})\\
	\vdots&\vdots\\
	n_{Filters},&f(v_n,\textbf{t},\textbf{x})
	\end{bmatrix}
	\label{Eq:CNN_matrix}
	\end{equation}
\end{minipage}\\\\
In the following a distinction between \(\mconv\) and \(\mae\) is omitted, when referring to the preprocessed matrices. However a distinction between the levels of rarefaction, namely \(\hy\) and \(\rare\), will be utilized as \(\mhy\) for the former and \(\mrare\) for the latter. This designation is mainly used in \cref{Ch:ApB} and \cref{Ch:ApA}.
\subsection{Contructing the mapping}
With the FOM solution at hand it is possible to construct a mapping \(Q\) such that \(Q(f)\approx \tilde{f}\). For \(Q\), POD and two autoencoders, based on a fully connected neural network (FCNN) and a convolutional neural network (CNN), are employed, see \cref{Ch:DimRedAl}. The architecture, selection of hyperparameters and training for the FCNN and the CNN is discussed in \cref{Sec:AppendixA}. Hence the existence of fully trained and tuned FCNN and CNN is given from now on. Since this thesis mainly focuses on methods from machine learning, POD serves as a comparative measure. To continue, the intrinsic variables obtained from POD, the FCNN and the CNN, will be referred as \(\idhy\) and \(\idrare\). The former describes the intrinsic variables when reducing \(\hy\) and the latter when reducing \(\rare\).\\
In order to choose the number \(p\) of intrinsic variables (sizes of \(\idhy\) and \(\idrare\)) we will utilized POD as a reference framework. Therefore the singular values \(\sigma\), as well as, the cumulative energy (cusum-e)\\
\noindent \begin{minipage}{.3\linewidth}
	\begin{equation}
	\textrm{cusum-e} = \frac{\textrm{cusum}}{\sum_i \sigma_i}
	\end{equation}
\end{minipage}%
\begin{minipage}{.7\linewidth}
	\begin{equation}
			\textrm{with} \qquad\qquad(\textrm{cusum})_i =(\text{cusum})_{i-1} + \sigma_{i}
	\end{equation}
\end{minipage}\\\\
over the singular values, are employed. A comparison for both levels of rarefaction is provided in \cref{Fig:CUSUM-e}. With a total of \(p=6\) intrinsic variables, a cumulative energy of over \(99\%\) can be achieved for \(\hy\). The sixth singular value measures to a value of \(\sigma_6 = 0.275\). The cumulative energy of the singular values of \(\rare\) arrives above \(99\%\) with \(p=4\) singular values. The forth value is at \(\sigma_4 = 0.706\). The rate at which the singular values drop is exponential in \(\rare\) and close to exponential  in \(\hy\), with a difference of \(\L2 = 0.025\) in the \(\L2\)-Error in the gradients of the first \(p=10\) singular values.\\
We can link the decay of the Kolmogorov n-width in \cref{Eq:KolmoAdv} for advection dominated problems with the slightly dissimilar decay in singular values, which manifests in the transition from \(\hy \rightarrow \rare \). Specifically on the spectrum between advection and diffusion, \(\hy\) is the advective one, as the compression is higher in this case. See \cref{Fig:KN} for a closer survey. Though, for both \(\hy\) and \(\rare\) the Kolmogorov n-width decays rapidly which in turn leads to the assumption that advection and sharp shock fronts do not appear predominantly.
\begin{figure}[!htbp]
	\begin{subfigure}{.45\textwidth}
		\input{Figures/SVD/CumSum_Hydro.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\hy\). A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Rare}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\input{Figures/SVD/CumSum_Rare.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\rare\).  A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Hydro}
	\end{subfigure}
	\caption{Comparison of singular variables \(\sigma\) and cumulative energy for \(hy\) and \(\rare\). The decay of the singular values can be used to estimate the decay of the Kolmogorov n-width.}
	\label{Fig:CUSUM-e}
\end{figure}
From a fluid mechanical point of view the number of intrinsic variables for \(\hy\) should be in total not more than three, as a slip-flow can be described in terms of three macroscopic quantities like density \(\rho\), macroscopic velocity \(u\) and total energy \(E_{tot}\)\cite{BGK}\cite{Bernard}. Therefore \(p=3\) is employed for \(\idhy\) and the autoencoders are arranged in that way. When considering \(\rare\) the picture becomes a little blurrier. The microscopic velocities determine the overall picture to an increasing extend congruent to \(p\) the number of intrinsic variables. Therefore the size of the bottleneck layer, which is the same as \(p\) the number intrinsic variables are varied in a next step.\\ To this end $p$ is varied for POD, FCNN and CNN over $p \in \{1,2,4,8,16,32\}$ with both $\hy$ and $\rare$. Note that the neural networks needed to be trained for these experiments and that by changing $p$ i.e. widening the bottleneck layer, a gain or loss of capacity occurs which can be connected to stability during training, see \cref{Ch:DimRedAl} and \cite{Goodfellow}.
\begin{figure}[htbp!]
	\input{Figures/Results/Var_iv.tex}
	\caption{Variation of the number of intrinsic variables \(p\) over the $\L2$-Error for POD, FCNN and CNN. Results for $\hy$ are displayed on the left and for $\rare$ on the right.}
	\label{Fig:IntVar}
\end{figure}
Shown in \cref{Fig:IntVar} is the outcome of said experiments. The design for \cref{Fig:IntVar} is taken from \cite{Carlberg}. The loss of information when applying the POD goes exponentially to zero with increasing $p$ which is not suprising when consulting the \textit{Eckard-Young Theorem} provided in \cref{Eq:EckardYoung} taken from \cite{Kutz}.
The left plot of \cref{Fig:IntVar} displays the results for $\hy$ with $p=3$ the estimated size of \(\idhy\), emphasized with a black line. Here a drop in the $\L2$-Error can be observed for both neural networks until $p = 3$. Stagnation is observed for the CNN afterwards. FCNN continues to improve slighly. However $p=3$ can be confirmed.\\
Moving forward to establish the value of \(p\) for $\rare$ we look at the right plot of \cref{Fig:IntVar}. A drop in the $\L2$-Error can be observed for the FCNN with increasing $p$ until $p = 5$, which is highlighted with a black line. A continued widening of the bottleneck layer results in an slightly increased $\L2$-Error. Overfitting due to increased capacity of the model can be though of as the culprit here. Contrarily the CNN performs best with \(p = 4\) and maintains a zig zag pace progression. The discrepancy between \(p=4\) and \(p = 5\) in the CNN and FCNN is negligible. For one the FCNN performs overall better than it's convolutional counterpart, additionally shifts of missing nodes i.e. weights and biases in the bottleneck layer to the weights and biases of other layers makes this \glqq method\grqq fairly inaccurate. This results in \(p=5\) for \(\rare\).
\begin{table}[!htbp]
	\centering
	\caption{}
	\begin{tabular*}{16.5cm}{ @{\extracolsep{\fill}} c c c c c c c c c c c @{} }
		\toprule
		Intrinsic variables&  2 & 3 &  4  &  5  & 6  & 7  & 8  & 9  & 10 \\   
		\hline
		\(\L2\)-Error for \(\rare\)& 0.0750 & 0.0205 &0.0081& 0.0030& 0.0013& 0.0006& 0.0002& 6.2\(e^{-5}\)& 2.7\(e^{-5}\) \\
		\(\L2\)-Error for \(\hy\)&0.0853 & 0.0327 & 0.0153 & 0.0087 & 0.0046 & 0.0021 & 0.0014 & 0.0005 & 0.0003\\
		\bottomrule
	\end{tabular*} \label{Tab:NumIntVar}
\end{table}

\subsection{Online Phase}