% !TEX root = master.tex

\chapter{Model Order Reduction}
\label{Ch:ROM}
%\pagenumbering{arabic}


In this chapter model order reduction (MOR) of the BGK-model in the Sod shock tube will be introduced for which POD and in particular autoencoders are adopted to obtain a reduced basis (RB).\\
Model order reduction is a technique used for reducing the computational cost when evaluating PDE's \cite{Bernard}\cite{Carlberg}\cite{ohlberger2015reduced}. To achieve this, the solution to a PDE is being approximated by reducing one or more of it's dimensions. The reduction is performed by a mapping onto a low dimensional manifold. In our case the solution to the BGK-model is a function \(f(x,v,t) \in \mathbb{R}^3\). Now we could reduce i.e. \(v\) to \(n\), where \(o\) represents the number of elements in \(v\) and \(p\) represents the number of elements in \(n\). With \(p << o\) we obtain a reduced order model (ROM) which we call \(q(x,n,t)\) of significantly lower dimension. In particular \(n\) is called the reduced basis or the intrinsic variables. In \cref{Ch:BGK} we saw that the BGK-model is a PDE which through discretization in the spatial dimension \(x\) and the velocity dimension \(v\) holds a system of \(KJ\) ODE's in time in 1D and \(K^3J^3\) ODE's in time in 3D.  By the reduction of \(v\) we arrive at \(nJ\) ODE's in time for 1D and \(nJ^3\) ODE's in time in 3D. This example illustrates the amount of computations that can be saved by MOR. The mapping from \(f(x,v,t)\) to \(q(x,n,t)\) can be performed by one of the reduction algorithms from \cref{Ch:DimRedAl}. The remapping back to \(\tilde{f}(x,v,t)\) is carried out by the same algorithm under the condition, that the distance \(||f - \tilde{f}||\) is small.\\
To sum up, the idea that every high dimensional dynamical-state space \(W\), also called solution manifold, where in our case \(f(x,v,t) \in W\), can be mapped onto a state-space i.e. \(V\) of lower dimension with \(q(n,\mu_i) \in V\), is exploited within MOR \cite{ohlberger2015reduced}. Here \(i\) counts through the variables that where omitted during reduction and \(n\) beeing the intrinsic variables. Again \(p\) counts through the intrinsic variables. The state space of lower dimension is called the intrinsic solution manifold \(V\) with \(q(n,\mu_i) \in V\) \cite{Carlberg}.\\
\begin{figure}[H]
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow.tex}
		\subcaption{Evolving the BGK-model in the Sod shock tube in time is generated through evaluating the FOM in space \(x\), velocity \(v\) and time \(t\), which yields the solution \(f(x,v,t)\). The operator \(Q\) is here the FOM described in \cref{Ch:BGK}.}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow_red.tex}
		\subcaption{Evolving the ROM of the BGK-model in the Sod shock tube in time through evaluating the ROM at \(n\) and \(\mu_i\), yields an approximation to the FOM solution \(\tilde{f}\). The operator \(Q\) is here either POD or a autoencoder described in \cref{Ch:DimRedAl}.}
	\end{subfigure}
	\caption{Outline of the correlation between the FOM solution and the approximation obtained from the ROM.}
\end{figure}
MOR is partitioned into two successive phases called the \textit{offline} - and the \textit{online} phase. During the offline phase \textit{snapshots} of a dynamical-system are generated through experiments or simulations of the full order model (FOM). The snapshots \(F = \{f(t_1),...,f(t_n)\}\) are created once, each representing one moment in time of the dynamical system. Thus in our case one needs a snapshot database of solutions \(f(x,v,t)\) of the BKG-model in Sod's shock tube. Next the mapping \(Q\) is constructed such that \(\tilde{f} = Q(f)\), for which \(f(t_i) \approx \tilde{f}(t_i)\), reducing the dimensionality of the FOM solution as outlined before. During the online phase the reduced order model is evaluated and the error is estimated by 
\begin{equation}
	\L2 = \frac{||f - \tilde{f} ||_2}{||f||_2}
\end{equation}
which is called the relative \(\L2\)-Error norm. The abbreviation \(\L2\)-Error is used from here on. Therefore the online phase may be described as a stage of independence from the full order model.
Following \cite{ohlberger2015reduced} and \cite{Carlberg}, the success when building a ROM through linear reduction methods like the POD \cref{Sec: POD}, depends on a rapidly decaying Kolmogorov N-width. In particular advection dominated problems exhibit a slow decay of the Kolmogorov N-width as described in \cite{ohlberger2015reduced}. Thus yielding a need for non-linear methods like autoencoders described in \cref{Sec:AE}. The Kolmogorov N-width is given by
\begin{equation}
	d_{V}(W):= \sup_{f \in W} \inf_{\tilde{f} \in V} ||f-\tilde{f}||
	\label{Eq:Kolmogorov}
\end{equation}
and gives the worst best approximation error for elements of \(W\). The convergence behaviour of the Kolmogorov N-width for advection dominated problems, especially when jump conditions are involved as in Sod's shock tube \cref{Ch:BGK}, decays with
\begin{equation}
	d_p(W) \leq \frac{1}{2} p^{-1/2},
	\label{Eq:KolmoAdv}
\end{equation}
where \(p\) denotes the number of RB or intrinsic variables. Further insight in provided in \cite{ohlberger2015reduced}.\\
Note that hereafter we will solely use the term intrinsic variables. The relevance of using non-linear reduction methods for MOR is often formulated in terms of a slow decaying Kolmogorov N-width. And even tough the BGK-model in Sod's shock tube describes an advection problem with jump discontinuities implies that linear reduction methods should fail, we will see in the following that this is only partly true in this case.
\section{Offline Phase}\label{Sec: FOM}
The FOM is the 1D BGK-model in Sod's shock tube for two levels of rarefaction, gratefully provided by Julian K\"ollermeier and the Departement of Mathematics at the RWTH Aachen. Sod's shock tube is discretized in space \(x\) with 100 nodes, in velocity \(v\) with 40 nodes for 25 time steps in time \(t\), as presented in \cref{Tab:Setup} and \cref{Fig:SodProbSetup}.
\begin{table}[htp]
	\centering
	\caption{Problem setup for the BGK model in Sod's shock tube. The diaphragm is positioned at \(x_d=0.5025\). For the initial condition with \(t=0\) the gas is present at \(x<x_d\) and absent for \(x\geq x_d\).}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c @{} }
		\toprule
		Variable   & Number of nodes \(i\)&   Domain extension& Step size (uniform)\\   
		\hline
		\(x\) 		&	200&     [0.0025,0.9975]&	    0.00499\\
		\(v\)       &   40 &  		    [-10,10]&	    0.05128\\
		\(t\)   	&	25 &        	[0,0.12]&	      0.005\\
		\bottomrule
	\end{tabular*} \label{Tab:Setup}
\end{table}
One level can be viewed as a slip flow \cite{schaaf} with \(\Kn=0.01\), hereafter referred to as \(\rare\). The dilution up to this level of rarefaction is little though leading to inaccuracies when employing the common Navier Stokes equations. Therefore the NFS-equations (Navier-Stokes-Fourrier) could be used \cite{NumaKUL}. The other is situated in the continuum flow regime with \(\Kn=0.00001\) for which the Navier-Stokes equations can be utilized without hesitation. An in-depth description of the BGK-model and Sod's shock can be found in \cref{Ch:BGK}.\\
Hereafter the continuum flow will be referred to as \(\hy\). Note that both \(\hy\) and \(\rare\) are three dimensional tensors comprising solutions \(f(x,v,t)\).\\
The reduction algorithms, introduced in \cref{Ch:DimRedAl}, require a distinct reshaping of the input data before they can be used. The preprocessed matrix for the FCNN as one batch  \(\btchfc\), is shown in \cref{Eq:AE_matrix}. Each row of \(\btchfc\) represents \(\btchfc_{,l}=f(v,t_i,x_j)\), an example, that can be fed to the FCNN. Hence \(x_jt_i=5000\) samples can be acquired.\\
The preprocessed matrix for the CNN, \(\btchcn\), is shown in \cref{Eq:CNN_matrix} with \(\btchcn_{,l} = f(x,t,v_k)\). Hence \(v_k=40\) examples can be obtained to be fed into the FCNN. POD can use \(\btchfc\) as well as \(P^T_{FCNN}\) it's transposition as input. In the following, a distinction between \(\btchcn\) and \(\btchfc\) is omitted, when referring to the preprocessed matrices\\
\begin{minipage}{.45\linewidth}
	\begin{gather}
	\btchfc = \begin{bmatrix}
	f(v_1,t_1,x_1)&\cdots &f(v_n,t_1,x_1) \\
	f(v_1,t_1,x_2)&\cdots &f(v_n,t_1,x_2) \\
	\vdots& \vdots & \vdots\\
	f(v_1,t_1,x_n)&\cdots &f(v_n,t_1,x_n)\\
	f(v_1,t_2,x_1)&\cdots &f(v_n,t_2,x_1)\\
	\vdots & \vdots & \vdots\\
	f(v_1,t_n,x_n)&\cdots &f(v_n,t_n,x_n)
	\end{bmatrix}
	%\raisetag{15pt}
	\label{Eq:AE_matrix}
	\end{gather}
\end{minipage},\qquad%
\begin{minipage}{.35\linewidth}
	\begin{gather}
	\btchcn= \begin{bmatrix}
	n_{Filters},&f(v_1,\textbf{t},\textbf{x})\\
	n_{Filters},&f(v_2,\textbf{t},\textbf{x})\\
	\vdots&\vdots\\
	n_{Filters},&f(v_n,\textbf{t},\textbf{x})
	\end{bmatrix}
	\label{Eq:CNN_matrix}
	\end{gather}
\end{minipage}\textrm{.}\\\\
%However a distinction between the levels of rarefaction, namely \(\hy\) and \(\rare\), will be utilized as \(\mhy\) for the former and \(\mrare\) for the latter. This designation is mainly used in \cref{Ch:ApB} and \cref{Ch:ApA}.
\subsection{Contructing the mapping}
With the FOM solution at hand it is possible to construct a mapping \(Q\) such that \(Q(f)\approx \tilde{f}\). Again, for \(Q\), POD and two autoencoders, based on a fully connected neural network (FCNN) and a convolutional neural network (CNN), are employed, see \cref{Ch:DimRedAl}. With considering \cref{Ch:ApA} and \cref{Ch:ApB}, the selection of hyperparameters and training for the FCNN and the CNN is already discussed. Hence the availability of fully trained and tuned FCNN and CNN is given from now on.\\
Note that since this thesis focuses on the use of neural networks, POD acts as a benchmark. Consequently, in oder to contrast the number \(p\) of intrinsic variables of the autoencoders (sizes of the code layer) we will utilize POD as a reference framework. The intrinsic variables obtained from POD, the FCNN and the CNN, will be referred as \(\idhy\) and \(\idrare\), where the former describes the intrinsic variables when reducing \(\hy\) and the latter when reducing \(\rare\).\\
A first step is to perform a POD with \(\hy\) and \(\rare\) distinctively. The obtained singular values \(\sigma\), as well as the cumulative energy (cusum-e), which is defined as\\
\noindent \begin{minipage}{.3\linewidth}
	\begin{equation}
	\textrm{cusum-e} = \frac{\textrm{cusum}}{\sum_i \sigma_i}
	\end{equation}
\end{minipage}%
\begin{minipage}{.7\linewidth}
	\begin{equation}
			\textrm{with} \qquad\qquad(\textrm{cusum})_i =(\text{cusum})_{i-1} + \sigma_{i}
	\end{equation}
\end{minipage},\\\\
over the singular values, are shown in \cref{Fig:CUSUM-e}. With a total of \(p=4\) intrinsic variables, a cumulative energy of over \(99\%\) can be achieved for \(\hy\). The fourth singular value measures to a value of \(\sigma_4 = 0.706\). The cumulative energy of the singular values of \(\rare\) arrives above \(99\%\) with \(p=6\) singular values. The sixth value is at \(\sigma_6 = 0.275\). Thus a slight difference can be observed for both datasets when emplying POD.\\
We can link the decay of the Kolmogorov N-width in \cref{Eq:Kolmogorov} with the decay of the singular values, which invalidates the application of \cref{Eq:KolmoAdv} for the FOM solutions.  The rate at which the singular values drop is approximately exponential in \(\hy\) and in \(\rare\) and in turn indicates a rapid decay of the Kolmogorov N-width. Hence supports the  assumption that advection and sharp shock fronts do not appear predominantly. Moreover the eventough small, but dissimilar, decay rate at which cusum-e drops is a manifestation of the different rarefaction levels. With a decreasing number of particles present, the lesser the probability of a single bulk macroscopic velocity emerges as seen in the full survey of the BGK model in \cref{Ch:BGK}. 
\begin{figure}[H]
	\begin{subfigure}{.45\textwidth}
		\input{Figures/Chapter_4/CumSum_Hydro.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\hy\). A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Rare}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\input{Figures/Chapter_4/CumSum_Rare.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\rare\).  A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Hydro}
	\end{subfigure}
	\caption{Comparison of singular variables \(\sigma\) and cumulative energy for \(\hy\) and \(\rare\). The decay of the singular values can be used to estimate the decay of the Kolmogorov n-width.}
	\label{Fig:CUSUM-e}
\end{figure}
From a fluid mechanical point of view the number of intrinsic variables for \(\hy\) should be in total not more than three, as a slip-flow can be described in terms of three macroscopic quantities like density \(\rho\), macroscopic velocity \(u\) and total energy \(E_{tot}\)\cite{BGK}\cite{Bernard}. Therefore \(p=3\) is employed for \(\idhy\) and the autoencoders are arranged in that way. When considering \(\rare\) the picture becomes a little blurrier, as outlined before. More than one Maxwellian describe the microscopic velocities which increases \(p\). To shed light into the size we want to choose for \(\idrare\), the size of the bottleneck layer of the autoencoders, which is the same as \(p\) is varied in a next step.\\
To this end $p$ is varied for POD, FCNN and CNN over $p \in \{1,2,4,8,16,32\}$ with both $\hy$ and $\rare$. Note that the neural networks needed to be trained for these experiments and that by changing $p$ i.e. widening the bottleneck layer, a gain or loss of capacity occurs which can be connected to stability during training, see \cref{Ch:DimRedAl} and \cite{Goodfellow}.
\begin{figure}[htbp!]
	\input{Figures/Chapter_4/Var_iv.tex}
	\caption{Variation of the number of intrinsic variables \(p\) over the $\L2$-Error for POD, FCNN and CNN. Results for $\hy$ are displayed on the left and for $\rare$ on the right.}
	\label{Fig:IntVar}
\end{figure}
Shown in \cref{Fig:IntVar} is the outcome of said experiments. The design for \cref{Fig:IntVar} is taken from \cite{Carlberg}. The loss of information when applying the POD goes exponentially to zero with increasing $p$ which is not suprising when consulting the \textit{Eckard-Young Theorem} provided in \cref{Eq:EckardYoung} taken from \cite{Kutz}.
The left plot of \cref{Fig:IntVar} displays the results for $\hy$ with $p=3$ the estimated size of \(\idhy\), emphasized with a black line. Here a drop in the $\L2$-Error can be observed for both neural networks until $p = 3$. Stagnation is observed for the CNN afterwards. FCNN continues to improve slighly. However the choice for $p=3$ can be confirmed.\\
Moving forward to establish the value of \(p\) for $\rare$ we look at the right plot of \cref{Fig:IntVar}. A drop in the $\L2$-Error can be observed for the FCNN with increasing $p$ until $p = 5$, which is highlighted with a black line. A continued widening of the bottleneck layer results in an slightly increased $\L2$-Error. Overfitting due to increased capacity of the model can be though of as the culprit here. Contrarily the CNN performs best with \(p = 4\) and maintains a zig zag pace progression. The discrepancy between \(p=4\) and \(p = 5\) in the CNN and FCNN is negligible. For one the FCNN performs overall better than it's convolutional counterpart, additionally shifts of missing nodes i.e. weights and biases in the bottleneck layer to the weights and biases of other layers makes this \glqq method\grqq{} fairly inaccurate. Thus we decide for \(p=5\) in \(\rare\).\\
A detailed survey of the reconstruction \(\tilde{f}\) obtained from POD, the FCNN and the CNN  using \(p\) is provided in \cref{Ch:Results}. 
\subsection{Online Phase}
In the online phase new states of the FOM solution are calculated. Moreover new states need to be evaluated. With POD one usually exploits the intrinsic variables within a Galerkin framework as in \cite{Bernard} and \cite{Carlberg} to produce new states, which won't discussed in this thesis. The same can be done with the intrinsic variables obtained from autoencoders as in \cite{Carlberg}. In this contribution new states are produced employing simple interpolation in time \(t\) of \(\idhy\) and \(\idrare\). This approach can be thought of as the ability of the autoencoder to generalize, an idea that stems from deep learning. Therefore the generalization ability of the proposed autoencoder architectures will be analyzed in \cref{Ch:Results}.   