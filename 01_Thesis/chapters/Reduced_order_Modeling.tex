% !TEX root = master.tex

\chapter{Model Order Reduction}
\label{Ch:ROM}
%\pagenumbering{arabic}

This chapter introduces model order reduction (MOR) of the BGK-model in the Sod's shock tube, for which POD and in particular autoencoders are adopted to obtain a reduced basis (RB).\\
Model order reduction is a technique used for reducing the computational cost when evaluating PDEs \cite{Bernard}\cite{Carlberg}\cite{ohlberger2015reduced}. To achieve this, the solution to a PDE is approximated by reducing one or more of its dimensions. The reduction performs a mapping onto a low-dimensional manifold. In this case the solution to the BGK model is a function \(f(x,v,t) \in \mathbb{R}^3\). For example,  \(v\) could be reduced to \(n\), where \(k\) represents the number of elements in \(v\) and \(p\) represents the number of elements in \(n\). With \(p << k\) we obtain a reduced order model (ROM) which we call \(q(x,n,t)\) of significantly lower dimension. In particular, \(n\) is called the reduced basis or the intrinsic variable. \Cref{Ch:BGK} shows that the BGK-model is a PDE which through discretization in the spatial dimension \(x\) and the velocity dimension \(v\) holds a system of \(KJ\) ODE's in time in 1D and \(K^3J^3\) ODE's in time in 3D.  By the reduction of \(v\), we arrive at \(nJ\) ODE' in time for 1D and \(nJ^3\) ODE's in time in 3D. This example illustrates the number of computations that can be saved by MOR. The mapping from \(f(x,v,t)\) to \(q(x,n,t)\) can be performed by one of the reduction algorithms from \cref{Ch:DimRedAl}. The remapping back to \(\tilde{f}(x,v,t)\) is carried out by the same algorithm under the condition, that the distance \(||f - \tilde{f}||\) is small.\\
\begin{figure}[H]
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow.tex}
		\subcaption{Evolving the BGK model in Sod's shock tube in time is generated through evaluating the FOM in space \(x\), velocity \(v\) and time \(t\), which yields the solution \(f(x,v,t)\). The operator \(Q\) is here the FOM described in \cref{Ch:BGK}.}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\centering
		\input{Figures/MOR/flow_red.tex}
		\subcaption{Evolving the ROM of the BGK model in Sod's shock tube in time through evaluating the ROM at \(n\) and \(\mu_i\) yields an approximation to the FOM solution \(\tilde{f}\). The operator \(Q\) is here either POD or an autoencoder described in \cref{Ch:DimRedAl}.}
	\end{subfigure}
	\caption{Outline of the correlation between the FOM solution and the approximation obtained from the ROM.}
\end{figure}
To summarize, the idea that every high dimensional dynamical-state space \(W\), also called solution
manifold, can be mapped onto a state-space i.e. V of lower dimension, is exploited within MOR \cite{ohlberger2015reduced}. Thus, \(f(x, v, t) \in W\) and \(q(n,\mu_i) \in V\), where \(\mu_i\) gives the variables omitted by the reduction and \(n\) the intrinsic variables. Again, p counts through the intrinsic variables. The state-space of the lower dimension is called the intrinsic solution manifold \(V\) with \(q(n, \mu_i) \in V\)\cite{Carlberg}.
MOR is partitioned into two successive phases called the \textit{offline} - and the \textit{online} phase. During the offline phase, experiments or simulations of the full order model (FOM) generate \textit{snapshots} of a dynamical system. The snapshots \(F = \{f(t_1),...,f(t_n)\}\) are created once, each representing one moment in time of the dynamical system. Thus a snapshot database of solutions \(f(x,v,t)\) of the BKG model in Sod's shock tube is required. Next the mapping \(Q\) is constructed such that \(\tilde{f} = Q(f)\), for which \(f(t_i) \approx \tilde{f}(t_i)\), reducing the dimensionality of the FOM solution as outlined before. During the online phase, the reduced-order model is evaluated and the error is estimated by
\begin{equation}
	\L2 = \frac{||f - \tilde{f} ||_2}{||f||_2}
\end{equation}
which is called the relative \(\L2\)-Error norm. Note, the abbreviation \(\L2\)-Error is used from here on. Therefore the online phase may be described as a stage of independence from the full order model. Following \cite{ohlberger2015reduced} and \cite{Carlberg}, the success when building a ROM through linear reduction methods like the POD, depends on a rapidly decaying Kolmogorov N-width. In particular, advection-dominated problems exhibit a slow decay of the Kolmogorov N-width as described in \cite{ohlberger2015reduced}, thus yielding the need for non-linear methods like autoencoders. The Kolmogorov N-width is given by
\begin{equation}
	d_{V}(W):= \sup_{f \in W} \inf_{\tilde{f} \in V} ||f-\tilde{f}||
	\label{Eq:Kolmogorov}
\end{equation}
and measures the worst best-approximation error for elements of \(W\). The convergence behavior of the Kolmogorov N-width for advection-dominated problems, especially when jump conditions are involved as in Sod's shock tube, decays with
\begin{equation}
	d_p(W) \leq \frac{1}{2} p^{-1/2},
	\label{Eq:KolmoAdv}
\end{equation}
where \(p\) denotes the number of RB or intrinsic variables. Further insight is provided in \cite{ohlberger2015reduced}.\\
Note, that hereafter the term intrinsic variables will be used solely. The relevance of using non-linear reduction methods for MOR is often formulated in terms of a slow decaying Kolmogorov N-width. And even though the BGK-model in Sod's shock tube describes an advection problem with jump discontinuities implies that linear reduction methods should fail, the following will show that this is only partly true for this case.

\section{Offline phase and number of intrinsic variables}\label{Sec: FOM}

The FOM is the 1D BGK-model in Sod's shock tube for which solutions \(f(x,v,t)\) for two levels of rarefaction are gratefully provided by Julian K\"ollermeier and the Departement of Mathematics at the RWTH Aachen.\\
One level with \(\Kn=0.01\) is a slip flow as explained in \cite{schaaf}. The dilution up to this level is little. However, the boundary to view this flow as a continuum is reached, leading to inaccuracies when employing the common Navier Stokes equations. Still, the NFS-equations (Navier-Stokes-Fourier) could be used in addition to the BGK-model \cite{NumaKUL}. The solution with this level of rarefaction is hereafter referred to as \(\rare\). The other solution is situated in the continuum flow regime with \(\Kn=0.00001\), for which the Navier-Stokes equations can be utilized without hesitation. Hereafter, the solution for this flow will be referred to as \(\hy\). A description of the BGK model and Sod's shock tube is available in \cref{Ch:BGK}.\\
Note, that both \(\hy\) and \(\rare\) are three dimensional tensors comprising \(f(x,v,t)\).\\
Sod's shock tube is discretized in space \(x\) with 100 nodes, in velocity \(v\) with 40 nodes for 25 time steps \(t\), presented in \cref{Tab:Setup} and \cref{Fig:SodProbSetup}.
\begin{table}[htp]
	\centering
	\caption{Problem setup for the BGK model in Sod's shock tube. The diaphragm is positioned at \(x_d=0.5025\). For the initial condition with \(t=0\) the gas is present at \(x<x_d\) and absent for \(x\geq x_d\).}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c c @{} }
		\toprule
		Variable   & Number of nodes \(i\)&   Domain extension& Step size (uniform)\\   
		\hline
		\(x\) 		&	200&     [0.0025,0.9975]&	    0.00499\\
		\(v\)       &   40 &  		    [-10,10]&	    \(\approx\) 0.51282051\\
		\(t\)   	&	25 &        	[0,0.12]&	      0.005\\
		\bottomrule
	\end{tabular*} \label{Tab:Setup}
\end{table}

The reduction algorithms, introduced in \cref{Ch:DimRedAl}, require a distinct reshaping of the input data before they can be used. The preprocessed matrix for the FCNN as one batch  \(\btchfc\), is shown in \cref{Eq:AE_matrix}. Each row of \(\btchfc\) represents \(\btchfc_{,l}=f(v,t_i,x_j)\), an example, that can be fed to the FCNN. Hence \(x_jt_i=5000\) samples can be acquired. POD can use \(\btchfc\) as well as \(P^T_{FCNN}\) its transposition as input.\\
The preprocessed matrix for the CNN, \(\btchcn\), is shown in \cref{Eq:CNN_matrix} with \(\btchcn_{,l} = f(x,t,v_k)\). Hence \(v_k=40\) examples can be obtained to be fed into the CNN. Because of the little number of available examples per rarefaction level, it is decided to combine \(\hy\) and \(\rare\) to one dataset, yielding 80 available examples for training the CNN. This measure leads to one model, that potentially generalizes about the BGK model for a variety of rarefaction levels in Sod's shock tube. Details are presented in \cref{Ch:ApB}. Note, the following omits a distinction between \(\btchcn\) and \(\btchfc\), when referring to the preprocessed matrices\\
\begin{minipage}{.45\linewidth}
	\begin{gather}
	\btchfc = \begin{bmatrix}
	f(v_1,t_1,x_1)&\cdots &f(v_n,t_1,x_1) \\
	f(v_1,t_1,x_2)&\cdots &f(v_n,t_1,x_2) \\
	\vdots& \vdots & \vdots\\
	f(v_1,t_1,x_n)&\cdots &f(v_n,t_1,x_n)\\
	f(v_1,t_2,x_1)&\cdots &f(v_n,t_2,x_1)\\
	\vdots & \vdots & \vdots\\
	f(v_1,t_n,x_n)&\cdots &f(v_n,t_n,x_n)
	\end{bmatrix}
	%\raisetag{15pt}
	\label{Eq:AE_matrix}
	\end{gather}
\end{minipage},\qquad%
\begin{minipage}{.35\linewidth}
	\begin{gather}
	\btchcn= \begin{bmatrix}
	n_{Filters},&f(v_1,\textbf{t},\textbf{x})\\
	n_{Filters},&f(v_2,\textbf{t},\textbf{x})\\
	\vdots&\vdots\\
	n_{Filters},&f(v_n,\textbf{t},\textbf{x})
	\end{bmatrix}
	\label{Eq:CNN_matrix}
	\end{gather}
\end{minipage}\textrm{.}\\\\
With the FOM solution at hand, it is possible to construct a mapping \(Q\) such that \(Q(f)\approx \tilde{f}\). Again, for \(Q\), POD and two autoencoders, the FCNN and the CNN are employed. With considering \cref{Ch:ApA} and \cref{Ch:ApB}, the selection of hyperparameters and training for the FCNN and the CNN is discussed, providing fully trained and tuned FCNNs and a CNN, which are given from now on.\\
Note that since this thesis focuses on the use of neural networks, POD acts as a benchmark. Consequently, to contrast the number \(p\) of intrinsic variables of the autoencoders (sizes of the code layer), POD will be utilized as a reference framework. The intrinsic variables obtained from POD, the FCNNs, and the CNN, will be referred to as \(\idhy\) and \(\idrare\), where the former describes the intrinsic variables when reducing \(\hy\) and the latter when reducing \(\rare\).\\
The first step is to perform a POD with \(\hy\) and \(\rare\) distinctively. The obtained singular values \(\sigma\), as well as the cumulative energy (cusum-e) defined as\\
\noindent \begin{minipage}{.3\linewidth}
	\begin{equation}
	\textrm{cusum-e} = \frac{\textrm{cusum}}{\sum_i \sigma_i}
	\end{equation}
\end{minipage}%
\begin{minipage}{.7\linewidth}
	\begin{equation}
			\textrm{with} \qquad\qquad(\textrm{cusum})_i =(\text{cusum})_{i-1} + \sigma_{i}
	\end{equation}
\end{minipage},\\\\
over the singular values, are shown in \cref{Fig:CUSUM-e}. With a total of \(p=4\) intrinsic variables, a cumulative energy of over \(99\%\) can be achieved for \(\hy\). The fourth singular value measures to a value of \(\sigma_4 = 0.706\). The cumulative energy of the singular values of \(\rare\) arrives above \(99\%\) with \(p=6\) singular values. The sixth value is at \(\sigma_6 = 0.275\). Thus a slight difference can be observed for both datasets when emplying POD.\\
The Kolmogorov N-width in \cref{Eq:Kolmogorov} linked to the decay of the singular values, invalidates the application of \cref{Eq:KolmoAdv} for the FOM solutions. The rate at which the singular values drop is approximately exponential in \(\hy\) and \(\rare\). Consequently, a rapid decay of the Kolmogorov N-width is indicated. This supports the assumption, that advection and sharp shock fronts do not appear predominantly. The even though small, but dissimilar, decay rate of the singular values is a manifestation of the different rarefaction levels. With a decreasing number of particles present in Sod's shock tube, the lesser the probability of a single bulk macroscopic behavior emerges as seen in the full survey of the BGK model in \cref{Ch:BGK}. Thus leading to an expected increase in the number of intrinsic variables necessary to achieve similar \(\L2\)-errors.  
\begin{figure}[H]
	\begin{subfigure}{.45\textwidth}
		\input{Figures/Chapter_4/CumSum_Hydro.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\hy\). A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Rare}
	\end{subfigure}\hfill
	\begin{subfigure}{.45\textwidth}
		\input{Figures/Chapter_4/CumSum_Rare.tex}
		\subcaption{Sigular values \(\sigma\) over \(k\) number of singular values (left) and cumulative energy, here labeled as \glqq cusum-e\grqq{} over \(k\) (right) for \(\rare\).  A black cross marker corresponds to over 99\% cumulative energy.}
		\label{Fig:CumSum_Hydro}
	\end{subfigure}
	\caption{Comparison of singular variables \(\sigma\) and cumulative energy for \(\hy\) and \(\rare\). The decay of the singular values can be used to estimate the decay of the Kolmogorov n-width.}
	\label{Fig:CUSUM-e}
\end{figure}
From a fluid mechanical point of view, the number of intrinsic variables for \(\hy\) in theory suffices with \(p=3\), as a slip-flow can be described in terms of three macroscopic quantities i.e. density \(\rho\), macroscopic velocity \(u\) and total energy \(E_{tot}\) as in \cref{Eq:Conservation1} to \cref{Eq:Conservation3} and in \cite{BGK}\cite{Bernard}. As mentioned previously, requires \(\rare\) a larger number for \(p\) because more than a single Maxwellian describes the microscopic velocities. Therefore, \(\idhy\) with \(p=3\) is employed for the FCNN as a starting value for the hyperparameter search. A starting value of \(p=5\) on the other hand is chosen for \(\idrare\). Note that this implies, that the CNN, which is trained with both rarefaction levels simultaneously, initially uses \(p=5\). The same value for \(p\) is chosen for the FCNN and \(\rare\).\\
The following variation of \(p\), sheds light on the performance of the autoencoders with different bottleneck layer sizes.\\
To this end, $p$ is varied for POD, the FCNNs and the CNN over $p \in \{1,2,4,8,16,32\}$ for both rarefaction levels. Note that the neural networks are trained again for these experiments. By changing $p$ i.e. widening the bottleneck layer, a gain or loss of capacity occurs that can be connected to stability during training, see \cref{Ch:DimRedAl} and \cite{Goodfellow}.
\begin{figure}[htbp!]
	\input{Figures/Chapter_4/Var_iv.tex}
	\caption{The $\L2$-Error over the variation of \(p\), the number of intrinsic variables using POD, the FCNNs and, the CNN. Results for $\hy$ are displayed on the left and for $\rare$ on the right.}
	\label{Fig:IntVar}
\end{figure}
Shown in \cref{Fig:IntVar} is the outcome of said experiments. The design for \cref{Fig:IntVar} is taken from \cite{Carlberg}. The loss of information when applying POD goes exponentially to zero with increasing $p$ which is not surprising when consulting the \textit{Eckard-Young Theorem} provided in \cref{Eq:EckardYoung} taken from \cite{Kutz}.
The left plot of \cref{Fig:IntVar} displays the results for $\hy$ with $p=3$, the estimated size of \(\idhy\), emphasized with a black line. The \(\L2\)-error of the FCNN first drops until $p = 3$ reaching \(\L2 = 0.0008\) to then drop further until \(p=8\) with \(\L2=0.00026\). Afterwards the \(\L2\) stagnates with a best value of \(\L2=0.00019\) at \(p=16\). Interestingly, the biggest improvement can be observed until \(p=3\).\\
The \(\L2\)-error of the CNN, that was trained with both rarefaction levels, drops until \(p=4\) reaching \(\L2=0.028\). Afterwards, the \(\L2\)-error stagnates reaching it's best value of \(\L2=0.023\) at \(p=16\) and \(p=32\). It is assumed that the value of the \(\L2\)-error is approximately the same for \(p=3\) and \(p=4\), as it is for the FCNN. Therefore, the CNN as well as the FCNN, seem to reflect the assumptions for \(p\) considering \(\hy\). While the CNN and POD are congruent for \(p=1\) and \(p=2\) with POD outperforming the CNN afterward, the FCNN is only outperformed by POD after \(p=8\), where the \(\L2\)-error of both algorithms meet.\\ 
Moving forward, to assess the value of \(p\) for $\rare$, consider the right plot of \cref{Fig:IntVar}. Once again, \(p=5\) is highlited with a black line to indicate it's assumed value. The $\L2$-Error of the FCNN begins as the highest of all three algorithms with \(\L2=0.58\) at \(p=1\). Afterwards the \(\L2\)-error plummets outperforming POD and the CNN until \(p=8\). After \(p=8\), where POD and the FCNN meet with \(\L2=0.012\) and \(\L2=0.014\) respectively, POD outperforms both algorithms. From \(p=4\) to \(p=5\) the \(\L2\)-error of the FCNN drops from \(\L2=0.0029\) to \(\L2=0.0009\). The increase in \(\L2\)-error moving from $p=5$ to $p=8$ is not a result of overfitting as seen in \cref{Ch:ApB}, and therefore can only be explained with a bad initialization point of the networks free parameters $\frepar$. A continued widening of the bottleneck layer results in the lowest error of \(\L2=0.00035\) at \(p=16\) for the FCNN.\\
Resulting \(\L2\)-error values of POD and the CNN match for \(p=1\) and \(p=2\). Thereafter, the error drops to \(\L2=0.03\) and \(\L2=0.026\) for \(p=4\) and \(p=5\) respectively. Continuing to widen the bottleneck layer, it can be observed that the \(\L2\)-error stagnates. Nevertheless, the lowest error is reached with \(\L2=0.022\) at \(p=16\) and \(p=32\).\\
The variation of \(p\) shows that for the assumed values of \(p=3\) and \(p=5\), the FCNN outperforms POD. Additionally, the performance of both autoencoders increases up to those points, which accentuates the previously made assumptions for \(p\). Nonetheless, those are not the lowest values the FCNN, and with some limitations also the CNN can reach. Until \(p=8\) the FCNN reaches its limitations for surpassing POD. Therefore, depending on \(p\), either POD or the FCNN can surpass each other. In order to evaluate the interpretability of all three algorithms, \(p=3\) and \(p=5\) is chosen for sizes of \(\idhy\) and \(\idrare\) respectively.\\

The following chapter covers the discussion of results obtained from POD, the FCNN, and the CNN. Additionally, the evaluation of new states of the FOM is offered, which can be viewed as the online phase of MOR. 
