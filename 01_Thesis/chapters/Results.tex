% !TEX root = master.tex

\chapter{Results}
\label{Results}
%\pagenumbering{arabic}

During the offline phase solutions obtained from the FOM, in this case $\hy$ and $\rare$, are reduced to their intrinsic dimension, as discussed in \cref{Ch:ROM}. From the thereby obtained intrinsic variables which define a reduced basis namely $\idhy$ and $\idrare$, the solution can be reconstructed yielding a loss of information. Hence, before buiding a ROM, the reconstruction of $\hy$ and $\rare$ needs to be evaluated. If the reduction and subsequent reconstruction fails to sustain "important" information, then the reduction algorithm is not suited for building a ROM. What these "important" qualities for the BGK model in the test case are and how they can be measured, as well as methods to compare the FOM solution against it's reconstruction is discussed in this section. For the dimensionality reduction the proper orthogonal decomposition (POD), a fully connected neural network (FCNN) and a convolutional neural network (CNN) is used.\\

A first measure of how much information gets lost is obtained through one of our permanence metrics during neural network training: the MSE over the validation set. Yet, this metric solely applies to neural networks. To be able to compare the POD to it's neural network counterparts another metric is in use: the error over the $L2$-Norm here referred to as thr $\L2$-Error , already introduced in \cref{Ch:DimRedAl}. In \cref{Tab:L2}, the $L2$-Error for all three algorithms on both input data is provided.

\begin{table}[htp]
	\centering
	\caption{Comparison of the $\L2$-Error over the reconstructions obtained by POD, FCNN and CNN. The CNN was trained using the k-fold algorithm, therefore the mean $\mu$ over $k=5$ folds, marked with superscript asterix, is given. Variance $\sigma^2$ for $\hy$ is $\sigma^2_{\hy}=2.25\times 10^{-5}$ and for $\rare$ is $\sigma^2_{\rare}=3.61 \times 10^{-5}$. Parenthesised values are obtained from the best fold.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c @{} }
		\toprule
		Algorithm       &$\L2$-Error for $\hy$     &$\L2$-Error for $\rare$  \\   
		\hline
		POD             &0.0205   &0.0087 \\
		FCNN 			&0.0017   &0.0019 \\
		CNN   			&0.0142* (0.0095)   &0.0178* (0.0097) \\
		\bottomrule
	\end{tabular*} \label{Tab:L2}
\end{table}
The FCNN performs best out of the three both for $\hy$ and $\rare$ and even drops about one decade compared to POD and CNN for $\hy$. Slightly better is the result for $\hy$ than for $\rare$ using both neural network designs. Yet the performance of FCNN and CNN doesn't seem to change a lot with changing $\hy$ and $\rare$. In contrast, the POD performs better for $\rare$ than for $\hy$. The reasons will be discussed hereafter. Note that the CNN is trained with the k-fold algorithm due to a lack of training samples as suggested in \cite{Goodfellow}. The k-fold algorithm is provided in \cref{Ch:ApB}. The mean $\mu$ over all folds gives an estimate of the models performance. Nonetheless the best performing fold is used in the subsequent analysis, yielding a better performance of the CNN.\\
Next a step is taken in attempting to estimate the intrinsic dimension $\pstar$ for $\rare$, which is not set as discussed in \cref{Ch:BGK}. To this end the number of intrinsic variables $\pstar$ is varied for POD, FCNN and CNN over $\pstar \in \{1,2,4,8,16,32\}$ with both $\hy$ and $\rare$. Note that the neural networks needed to be trained again for these experiments and that by changing $\pstar$ i.e. widening the bottleneck layer, a gain or loss of capacity occurs which can be connected to stability during training, see \cref{Ch:DimRedAl} and \cite{Goodfellow}. The stability of the POD, in contrast, is not altered when changing the value of $\pstar$.
\begin{figure}[htbp!]
	\input{Figures/Results/Var_iv.tex}
	\caption{Variation of intrinsic variables over the $\L2$-Error for POD, FCNN and CNN. Results for $\hy$ are displayed on the left and for $\rare$ on the right.}
	\label{Fig:IntVar}
\end{figure}
Shown in \cref{Fig:IntVar} is the outcome of said experiments. The design for \cref{Fig:IntVar} is taken from \cite{Carlberg}. The cumulative energy of the POD modes i.e. $\pstar$ in \cref{Ch:ROM} \cref{Fig:CumSum_Hydro} and \cref{Fig:CumSum_Rare} already revealed the performance of the POD with increasing $\pstar$. The loss of information when applying the POD goes linearly to zero with increasing $\pstar$ which is not suprising when consulting the \textit{Eckard-Young Theorem} provided in \cref{Eq:EckardYoung} taken from \cite{Kutz}.
The left plot of \cref{Fig:IntVar} displays the results for $\hy$ with $\pstar=3$ the known dimension of the intrinsic solution manifold for a simple gas which can be viewed as a continuum, emphasized with a black line. Here a sharp drop in the $\L2$-Error can be observed for both neural networks until $\pstar = 3$. Afterwards a stagnation is observed for CNN. FCNN continues to improve slighly. However the logically deducted and by POD proposed value of $\pstar=3$ can be confirmed.\\
Moving forward to measuring the size of the intrinsic solution manifold \(\pstar\) for $\rare$ we look at the right plot of \cref{Fig:IntVar}. Again, emphasized  with a black line, is the from POD proposed value for $\pstar$ with $\pstar=5$. See again \cref{Ch:ROM} for details. A drop in the $\L2$-Error can be observed for both neural networks with increasing $\pstar$ until they reach $\pstar = 5$. Note, the CNN already arrives there with \(\pstar = 4\). The continued widening of the bottleneck layer results in an slightly increased $\L2$-Error for both neural networks. Overfitting due to increased capacity of the model is the reason here. The discrepancy between \(\pstar=4\) and \(\pstar = 5\) in the CNN and FCNN occurs since neural networks are capable of shift missing nodes i.e. weights and biases in the bottleneck layer to the weights and biases of other layers. Hence a determination of \(\pstar\) by one variable is not exact with this method. As a backbone for determining $\pstar$ suits the FCNN, because of it's stable training, as seen in \cref{Ch:ApA}, and the cumulative energy of the POD modes, as seen in \cref{Ch:ROM}.\\
The "important" qualities, mentioned in the beginning, become visible when looking at the worst reconstructions. Those can be determined with the $\L2$-Error over time for all three models as seen in \cref{Fig:ErrTime}.
\begin{figure}[htbp!]
	\input{Figures/Results/ErrTime.tex}
	\caption{Relative Error over time for POD, FCNN and CNN. Results for $\hy$ are displayed on the left, the results for $\rare$ are displayed on the right.}
	\label{Fig:ErrTime}
\end{figure}
For POD and the CNN the last timestep at $t=0.12s$ in both cases $\hy$ and $\rare$ is the most rich in the $L2$-Error. While performing best, the FCNN in contrast has troubles in the beginning around $t=0.01s$ for $\hy$ and $\rare$. 
\begin{figure}[htbp!]
	\input{Figures/Results/ErrWorst.tex}
	\caption{Comparison of FOM with three reconstructions obtained from POD, FCNN and CNN. The BGK-model in Sod's shock tube at time \(t=0.12s\) and \(x\in [75\textrm{cm}, 150\textrm{cm}]\) of the tube, is most difficult to reconstruct by the aforementioned algorithms. Case $\hy$ is displayed in the top row, $\rare$ in the bottom row. }
	\label{Fig:ErrWorst}
\end{figure}
For a detailed comparison of the models, reconstructions of $f(x,v)$ at $t=0.12s$ and $x \in [75cm,150cm]$ are given in \cref{Fig:ErrWorst}.
A presentation of the FOM in $f(x,v)$ was already introduced in \cref{Ch:BGK}.
For clarity, $f(x_0,v)$ is a probability distribution for a gas having a velocity $v$ at point $x_0$ in space at one moment $t_i$ in time.\\
Top row in \cref{Fig:ErrWorst} show reconstructions for $\hy$ and the bottom row for $\rare$ with FOM solution as paradigm. Starting with \(\hy\) one observers, that a restored \(f(x,v)\) from\(x=120\), the point around which dilution initiates, gets defective for POD and the CNN. Here the probability distribution is thinner as the original with POD and smeared around the borders with the CNN. This in turn leads errors in velocities gas particles can have once passing \(x=120\). The FCNN reproduces the FOM solution relatively correct.\\
Continuing with a row further down of \cref{Fig:ErrWorst} and therefore \(\rare\), in place of a pronounced separation in a dense and rare region, stands bifurcation of \(f(x,v)\) into two probability distribution functions as outlined in \cref{Ch:BGK}. POD and the FCNN reproduce the FOM solution without any visible drawback. On the other hand the CNN reproduces smeared corners as in \(\hy\).\\    
\begin{figure}[htbp!]
	\input{Figures/Results/MacroError.tex}
	\caption{Matching of macroscopic quantities \(\rho\), \(\rho u\) and \(E\) reproduced by POD, FCNN, and CNN with FOM macroscopic quantities. Top row shows results for \(\hy\), bottom row for \(\rare\). CNN is displayed with marks only because of trembles in the signal.}
	\label{Fig:ErrMacro}
\end{figure}
Loss of information described above can unfold in severe mistakes in \(\rho\), \(\rho u\) and \(E\), the macroscopic quantities, as displayed in \cref{Fig:ErrMacro}. In the following descriptions of features of the macroscopic quantities are expressed in terms of rarefaction wave, contact discontinuity and height as well as position of the shockfront. For details see \cref{Ch:BGK}. Following the structure in the preceding figures, macroscopic quantities of \(\hy\) are displayed in the top row and for \(\rare\) in the bottom row of \cref{Fig:ErrMacro}. First the reproduction of the macroscopic quantities \(\rho\), \(\rho u\) and \(E\), obtained by the FCNN is exact for both cases \(\hy\) and \(\rare\). In particular the density \(\rho\) matches with results from the FOM exactly for all three algorithms in both cases \(\hy\) and \(\rare\), aside using POD for \(?y\). Second the CNN produces trembles in \(\rho u\) and especially in \(E\) which is why it's shown with marks only. Results from the CNN are similar for \(\hy\) and \(\rare\). The momentum \(\rho u\) holds errors for the tail of the rarefaction wave as well as the contact discontinuity and the position of the shockwave. One reason of the lacking ability of the CNN is the small number of samples, as described in \cref{Ch:ApB} and \cref{Ch:ROM}. The resulting trembles of the signal when calculating the macroscopic quantities is due the kernel approach of the CNN. During reconstruction the resolution of the output image is bounded by the size of the kernel, which leads to pixelation. Third POD performs better on \(\rare\), holding only small deviations in the contact discontinuity and position of shockwave in the momentum \(\rho u\) and the total energy \(E\). Distinct deviations from the FOM solution occur using POD on \(\hy\). The density \(\rho\) holds errors in the  height of the shockwave. The momentum \(\rho u\) holds errors in the tail of the rarefaction wave, the contact discontinuity and the height of the shockwave. Hence in the total energy \(E\) errors occur in the rarefaction wave, especially the tail, the contact discontinuity and the height of the shockwave. What we see here is the known drawback of POD. Sharp fronts and especially advection dominated problems lead to a fast decaying kogolomorov n-width. These problems need a nonlienar ansatz, as described in \cref{Ch:ROM}.
\begin{figure}[hbp!]
	\input{Figures/Results/Conservation.tex}
	\caption{Conservation}
\end{figure}
\begin{figure}
	\input{Figures/Results/Hy_Int.tex}
	\caption{Interpoaltionin in time for \(\hy\) from 25 snapshots to 241 snapshots with cubic splines using the FCNN.}
\end{figure}


\begin{figure}[!htbp]
	\scalebox{1}{\input{Figures/Results/Hydro/MacroCode10.tex}}
	\caption{Code variables \(c_1\), \(c_2\) and \(c_3\) (dashed lines - -) and macroscopic quantities \(\rho\), \(E\), \(\rho u\) (full lines --) for \(t=0.05s\).}
\end{figure}
\begin{figure}[!htbp]
	\scalebox{1}{\input{Figures/Results/Hydro/MacroCode20.tex}}
	\caption{Code variables \(c_1\), \(c_2\) and \(c_3\) (dashed lines - -) and macroscopic quantities \(\rho\), \(E\), \(\rho u\) (full lines --) for \(t=0.099s\).}
\end{figure}
\begin{figure}[!htbp]
	\scalebox{.6}{\input{Figures/Results/Hydro/PODConvCode.tex}}
	\caption{Comparison of the intrinsic variables generated by POD \(\gamma_1\), \(\gamma_2\) and \(\gamma_3\) with the intrinsic variables of the convolutional autoencoder \(\beta_1\), \(\beta_2\) and \(\beta_3\).}
\end{figure}

\begin{figure}[!hp]
	\scalebox{0.9}{\input{Figures/Results/Rare/Code.tex}}
	\caption{Intrinsic Variables of $\rare$}
\end{figure}
\subsection{Discussion and Outlook}