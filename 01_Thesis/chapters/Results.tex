% !TEX root = master.tex

\chapter{Results}
\label{Results}
%\pagenumbering{arabic}

During the offline phase solutions obtained from the FOM, in this case $\hy$ and $\rare$, are reduced to their intrinsic dimension, as discussed in \cref{Ch:ROM}. From the thereby obtained intrinsic variables which define a reduced basis namely $\idhy$ and $\idrare$, the solution can be reconstructed yielding a loss of information. Hence, before buiding a ROM, the reconstruction of $\hy$ and $\rare$ needs to be evaluated. If the reduction and subsequent reconstruction fails to sustain "important" information, then the reduction algorithm is not suited for building a ROM. What these "important" qualities for the BGK model in the test case are and how they can be measured, as well as methods to compare the FOM solution against it's reconstruction is discussed in this section. For the dimensionality reduction the proper orthogonal decomposition (POD), a fully connected neural network (FCNN) and a convolutional neural network (CNN) is used.\\

A first measure of how much information gets lost is obtained through one of our permanence metrics during neural network training: the MSE over the validation set. Yet, this metric solely applies to neural networks. To be able to compare the POD to it's neural network counterparts another metric is in use: the error over the $L2$-Norm here referred to as thr $\L2$-Error , already introduced in \cref{Ch:DimRedAl}. In \cref{Tab:L2}, the $L2$-Error for all three algorithms on both input data is provided.

\begin{table}[htp]
	\centering
	\caption{Comparison of the $\L2$-Error over the reconstructions obtained by POD, FCNN and CNN. The CNN was trained using the k-fold algorithm, therefore the mean $\mu$ over $k=5$ folds, marked with superscript asterix, is given. Variance $\sigma^2$ for $\hy$ is $\sigma^2_{\hy}=2.25\times 10^{-5}$ and for $\rare$ is $\sigma^2_{\rare}=3.61 \times 10^{-5}$. Parenthesised values are obtained from the best fold.}
	\begin{tabular*}{15cm}{ @{\extracolsep{\fill}} c c c @{} }
		\toprule
		Algorithm       &$\L2$-Error for $\hy$     &$\L2$-Error for $\rare$  \\   
		\hline
		POD             &0.0205   &0.0087 \\
		FCNN 			&0.0017   &0.0019 \\
		CNN   			&0.0142* (0.0095)   &0.0178* (0.0097) \\
		\bottomrule
	\end{tabular*} \label{Tab:L2}
\end{table}
The FCNN performs best out of the three both for $\hy$ and $\rare$ and even drops about one decade compared to POD and CNN for $\hy$. Slightly better is the result for $\hy$ than for $\rare$ using both neural network designs. Yet the performance of FCNN and CNN doesn't seem to change a lot with changing $\hy$ and $\rare$. In contrast, the POD performs better for $\rare$ than for $\hy$. The reasons will be discussed hereafter. Note that the CNN is trained with the k-fold algorithm due to a lack of training samples as suggested in \cite{Goodfellow}. The k-fold algorithm is provided in \cref{Ch:ApB}. The mean $\mu$ over all folds gives an estimate of the models performance. Nonetheless the best performing fold is used in the subsequent analysis, yielding a better performance of the CNN.\\
Next a step is taken in attempting to estimate the intrinsic dimension $\pstar$ for $\rare$, which is not set as discussed in \cref{Ch:BGK}. To this end the number of intrinsic variables $\pstar$ is varied for POD, FCNN and CNN over $\pstar \in \{1,2,4,8,16,32\}$ with both $\hy$ and $\rare$. Note that the neural networks needed to be trained again for these experiments and that by changing $\pstar$ i.e. widening the bottleneck layer, a gain or loss of capacity occurs which can be connected to stability during training, see \cref{Ch:DimRedAl} and \cite{Goodfellow}. The stability of the POD, in contrast, is not altered when changing the value of $\pstar$.
\begin{figure}[htbp!]
	\input{Figures/Results/Var_iv.tex}
	\caption{Variation of intrinsic variables over the $\L2$-Error for POD, FCNN and CNN. Results for $\hy$ are displayed on the left and for $\rare$ on the right.}
	\label{Fig:IntVar}
\end{figure}
Shown in \cref{Fig:IntVar} is the outcome of said experiments. The design for \cref{Fig:IntVar} is taken from \cite{Carlberg}. The cumulative energy of the POD modes i.e. $\pstar$ in \cref{Ch:ROM} \cref{Fig:CumSum_Hydro} and \cref{Fig:CumSum_Rare} already revealed the performance of the POD with increasing $\pstar$. The loss of information when applying the POD goes linearly to zero with increasing $\pstar$ which is not suprising when consulting the \textit{Eckard-Young Theorem} provided in \cref{Eq:EckardYoung} taken from \cite{Kutz}.
The left plot of \cref{Fig:IntVar} displays the results for $\hy$ with $\pstar=3$ the known dimension of the intrinsic solution manifold for a simple gas which can be viewed as a continuum, emphasized with a black line. Here a sharp drop in the $\L2$-Error can be observed for both neural networks until $\pstar = 3$. Afterwards a stagnation is observed for CNN. FCNN continues to improve slighly. However the logically deducted and by POD proposed value of $\pstar=3$ can be confirmed.\\
Moving forward to measuring the size of the intrinsic solution manifold \(\pstar\) for $\rare$ we look at the right plot of \cref{Fig:IntVar}. Again, emphasized  with a black line, is the from POD proposed value for $\pstar$ with $\pstar=5$. See again \cref{Ch:ROM} for details. A drop in the $\L2$-Error can be observed for both neural networks with increasing $\pstar$ until they reach $\pstar = 5$. Note, the CNN already arrives there with \(\pstar = 4\). The continued widening of the bottleneck layer results in an slightly increased $\L2$-Error for both neural networks. Overfitting due to increased capacity of the model is the reason here. The discrepancy between \(\pstar=4\) and \(\pstar = 5\) in the CNN and FCNN occurs since neural networks are capable of shift missing nodes i.e. weights and biases in the bottleneck layer to the weights and biases of other layers. Hence a determination of \(\pstar\) by one variable is not exact with this method. As a backbone for determining $\pstar$ suits the FCNN, because of it's stable training, as seen in \cref{Ch:ApA}, and the cumulative energy of the POD modes, as seen in \cref{Ch:ROM}.\\
The "important" qualities, mentioned in the beginning, become visible when looking at the worst reconstructions. Those can be determined with the $L2$-Error over time for all three models as seen in \cref{Fig:ErrTime}.
\begin{figure}[htpb!]
	\input{Figures/Results/ErrTime.tex}
	\caption{Relative Error over time for POD, FCNN and CNN. Results for $\hy$ are displayed on the left, the results for $\rare$ are displayed on the right.}
	\label{Fig:ErrTime}
\end{figure}
For POD and the CNN the last timestep at $t=0.12s$ in both cases $\hy$ and $\rare$ is the most rich in the $L2$-Error. While performing best, the FCNN in contrast has troubles in the beginning around $t=0.01s$ for $\hy$  and $t=0.3s$ for $\rare$. For a detailed comparison of the models, reconstructions for $f(x,v)$ at $t=0.12s$ and $x \in [75cm,150cm]$ are given in \cref{Fig:ErrWorst}. Representations of the FOM as $f(x,v)$ were already introduced in \cref{Ch:BGK}.
\begin{figure}[hpb!]
	\input{Figures/Results/ErrWorst.tex}
	\caption{Comparison of FOM with three reconstructions obtained from POD, FCNN and CNN. The BGK-model in Sod's shock tube at time \(t=0.12s\) and \(x\in [75\textrm{cm}, 150\textrm{cm}]\) of the tube, is most difficult to reconstruct by the aforementioned algorithms. Case $\hy$ is displed in the top row, $\rare$ in the bottom row. }
	\label{Fig:ErrWorst}
\end{figure}
For clarity, $f(x_0,v)$ is a probability distribution for a gas having a velocity at point $x_0$ in space at one moment $t_i$ in time. Top row in \cref{Fig:ErrWorst} show reconstructions for $\hy$ and the bottom row for $\rare$ with FOM solution as paradigm. Starting with \(\hy\) one observers, that a restored \(f(x,v)\) up to \(x=120\), the point around which dilution initiates, gets defective for POD and the CNN. Here the probability distribution is thinner with POD and thicker as well as smeared with the CNN. This in turn leads errors in velocities gas particles can have once passing \(x=120\). The FCNN seems to be less faulty on width of \(f(x,v)\), but also produces smeared characteristics similar to the CNN.\\
Continuing with a row further down of \cref{Fig:ErrWorst} and therefore \(\rare\), in place of a pronounced separation in a dense and rare region, stands bifurcation of \(f(x,v)\) into two probability distribution functions as outlined in \cref{Ch:BGK}. Neither POD, nor the CNN are able to recover this information during compression and reconstruction. Results of POD seem to average out the bifurcation. Results of the CNN seem to reproduce results as in case \(\hy\). Again the FCNN however reproduces this characteristic well enough.\\      
\begin{figure}[htbp!]
	\input{Figures/Results/MacroError.tex}
	\caption{Matching of macroscopic quantities \(\rho\), \(\rho u\) and \(E\) reproduced by POD, FCNN, and CNN with FOM macroscopic quantities. Top row shows results for \(\hy\), bottom row for \(\rare\). CNN is displayed with marks only because of tremblings in the signal.}
	\label{Fig:ErrMacro}
\end{figure}
Loss of information described above unfold in severe mistakes in \(\rho\), \(\rho u\) and \(E\), the macroscopic quantities, as displayed in \cref{Fig:ErrMacro}.  
\begin{figure}
	\input{Figures/Results/Conservation.tex}
	\caption{Conservation}
\end{figure}
\begin{figure}
	\input{Figures/Results/Hy_Int.tex}
	\caption{Interpoaltionin in time for \(\hy\) from 25 snapshots to 241 snapshots with cubic splines using the FCNN.}
\end{figure}
\subsection{Hydrodynamic Regime}

\begin{figure}[!htbp]
	\scalebox{1}{\input{Figures/Results/Hydro/MacroCode10.tex}}
	\caption{Code variables \(c_1\), \(c_2\) and \(c_3\) (dashed lines - -) and macroscopic quantities \(\rho\), \(E\), \(\rho u\) (full lines --) for \(t=0.05s\).}
\end{figure}
\begin{figure}[!htbp]
	\scalebox{1}{\input{Figures/Results/Hydro/MacroCode20.tex}}
	\caption{Code variables \(c_1\), \(c_2\) and \(c_3\) (dashed lines - -) and macroscopic quantities \(\rho\), \(E\), \(\rho u\) (full lines --) for \(t=0.099s\).}
\end{figure}
\begin{figure}[!htbp]
	\scalebox{.6}{\input{Figures/Results/Hydro/PODConvCode.tex}}
	\caption{Comparison of the intrinsic variables generated by POD \(\gamma_1\), \(\gamma_2\) and \(\gamma_3\) with the intrinsic variables of the convolutional autoencoder \(\beta_1\), \(\beta_2\) and \(\beta_3\).}
\end{figure}

\begin{figure}[!hp]
	\scalebox{0.9}{\input{Figures/Results/Rare/Code.tex}}
	\caption{Intrinsic Variables of $\rare$}
\end{figure}
The number of intrinsic variables defining the rarefied regime is unknown. Therefore experiments varying the number of intrinsic variables from two to ten are performed. \Cref{Tab:Intrinsic units} shows the results of two runs. From seven up to ten intrinsic varibales onward the the L2-Error reaches values around \(1.2e^{-3}\). From five and six intrinsic variables a shift happens to a value for the L2-Error around \(2.3e^{-3}\). A lowering of the number of intrinsic variables up to two increases the L2-Error further. A comparison between results obtained from the SVD and the autoencoder, show that both algorithms perform nearly the same with eight intrinsic variables with an L2-Error of \(1.4e^{-3}\). With more than eight intrinsic variables  
\begin{table}[!htbp]\centering
	\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
		\hline
		Intrinsic variables  & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ [.5ex]
		\hline
		Error 1st run & 0.0048 & 0.0025 & 0.0026 & 0.0027 & 0.0013 & 0.0014 & 0.0013 & 0.0009\\ \hline
		Error 2nd run & 0.0038 & 0.0024 & 0.0016 & 0.0015 & 0.0011 & 0.0010 & 0.0012 & 0.0010\\\hline
	\end{tabular}
	\caption{L2-Error for different numbers of intrinsic variables for the rarefied gas flow. Experiments with two intrinsic variables are also performed, but not shown here because two intrinsic variables is considered trivial.}
	\label{Tab:Intrinsic units}
\end{table}
\subsection{Discussion and Outlook}