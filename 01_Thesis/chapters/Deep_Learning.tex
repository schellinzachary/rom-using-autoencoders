% !TEX root = master.tex

\chapter{Dimensionality reduction algorithms}
\label{Ch:DimRedAl}
%\pagenumbering{arabic}

In this chapter dimensionality reduction algorithms which will be applied to solutions of the BGK model in Sod's shock tube are introduced: Proper orthogonal Decomposition (POD) and Autoencoders (AEs).\\
We will start off with a short introducing into POD, continue with autoencoders and finish with a detailed description of deep learning. Note that the main focus of this thesis lies in the application of autoencoders for which POD serves as a comparative method.\\
Dimensionality reduction algorithms lie at the heart of any reduced order model (ROM) which is described in the following chapter. As input serve datasets such as solutions of a full order model (FOM) or experimental data. These datasets may contain the dynamics of a spatio-temporal problem. The output is an approximation to the input. It is reconstructed from a low-dimensional representation that captured the underlying dynamics of the input problem. 
\section{Proper orthogonal decomposition (POD)}
\label{Sec: POD}
The solution of PDEs, precisely \(f(x,v,t)\), can be approximated either through a discretization into a system of ODEs as described in \cref{Ch:BGK}, or alternatively through a separation of variables ansatz
\begin{equation}
	f(t,v,x) = \sum_n a_n(t)\Phi_n(x,v)\mathrm{,}
\end{equation}  
as descried in \cite{Kutz}. Temporal dependence rendered through \(a_n(t)\) is independent from the spatial information carried in \(\Phi_n(x,v)\). Here \(\Phi_n(x,v)\) is called the \(n\)-th basis mode. With increasing \(n\), the accuracy of the solution consequently increases as well, which is similar to increasing the spatial resolution in finite difference methods outlined in \cref{Ch:BGK}. The essence of dimensionality reduction algorithms is now to find an optimal basis \(\Phi_n(x,v)\). Optimality here means capturing the dynamic answer to a given geometry and inital conditions thus permitting to exploit a minimal number \(n\) of basis modes to reconstruct the dynamics.\\
An optimal basis can be provided by POD. It leverages a physically interpretable spatio-temporal decomposition of the input data \cite{Kutz}. At first this data needs to be preprocessed in a fashion, that seperates the temporal and spatial axis. Each temporal state is called snapshot and is stacked in a matrix \(\Pi\) such that
\begin{equation}
	\Pi=\left[ \mathbf{u}_1,\mathbf{u}_2,\dots,\mathbf{u}_i \right] \qquad \mathrm{with}\qquad \mathbf{u}_i=\left[f(v_1,x_1),\dots,f(v_k,x_1),f(v_1,x_2)\dots,f(v_k,x_j)\right]^T \mathrm{,}
\end{equation}
where \(i\) is the number of available snapshots. Afterwards \(\Pi\) is decomposed using the singular value decomposition (SVD) which solves the left and right singular value problem leveraging
\begin{equation}
\Pi = U\Sigma V^*\mathrm{,}
\end{equation}
where \(U\) is a unitary matrix containing the left singular vectors of \(\Pi\) in it's columns and \(V\), also a unitary matrix, containing the right singular vectors in it's columns. Here superscribt asterix denotes the complex conjugate transpose. Furthermore \(\Sigma\) is a sparse matrix with the singular values in descending order on it's diagonal \cite{Kutz}. Note that the SVD always produces as much singular values and in turn singular vectors as there are elements on the shortest axis of the input matrix. Hence when preprocessing the input data as described above one gets as many singular values -and vectors as there are snapshots, given that the resolution in time is smaller than the spatial resolution.\\
Next, by applying the Eckard-Young theorem, that can be looked up in \cite{Kutz}, it is possible to solely harness the first leading singular values and corresponding vectors to approximate \(\Pi\) to a desired accuracy. The theorem states that the the optimal rank-r approximation to $\Pi$, in a least-squares sense, is given by the rank-r SVD truncation \(\tilde{\Pi}\):
\begin{equation}
\underset{\tilde{\Pi}, s.t. rank(\tilde{\Pi})=r}{\operatorname{argmin}} \| 
\Pi -\tilde{\Pi}\|_F
=\tilde{U}\tilde{\Sigma}\tilde{V}^*\mathrm{.}
\label{Eq:EckardYoung}
\end{equation} 
Here \(\tilde{U}\) and \(\tilde{V}\) denote the first \(r\) leading columns of \(\mathbf{U}\) and \(\mathbf{V}\), and \(\tilde{\Sigma}\) contains the leading \(r \times r\) sub-block of \(\Sigma\). \(\|\cdot\|_F\) is the Frobenius norm \cite{Kutz}.\\
When decomposing a matrix that contains snapshots of a dynamical system, the columns of \(\tilde{U}\) and \(\tilde{V}\) contain dominant patterns that describe the dynamical system. Moreover they provide a hierarchical set of modes, that characterize the observed attractor on which we may project a low-dimensional dynamical system to obtain reduced order models. That being said, we use the left singular values of \(\tilde{U}\) as optimal basis modes such that
\begin{equation}
	\tilde{U} = \Phi = \left[\Phi_1,\Phi_2,\dots,\Phi_r\right]\mathrm{.}
\end{equation}
Vectors in \(\Phi\) are orthogonal to each other and in that way provide a coordinate transformation from the high dimensional input space onto the low dimensional pattern space.[PLS CHECK AGAIN, sentence is stolen from KUTZ]
\section{Autoencoders}
\label{Sec:AE}

Another way to obtain an optimal basis is to employ a machine learning architecture called autoencoders. An autoencoder is a feed forward neural network, that is trained to learn salient features of it's input by compressing it and successively reconstructing it back from the compressed version. In that way one could say that an autoencoder simply copies it's input to it's output \cite{Goodfellow}. \Cref{Fig:Autoencoder} shows a schematic representation of the architecture for a deep undercomplete autoencoder and the necessary terminology used to describe it's components. For the moment we ignore the difference between an undercomplete and overcomplete autoencoder, and come back to that when talking about regularization.\\
The biggest allocation unit in autoencoders is the distinction of a decoder and a encoder part, which are seperated at central code layer. The encoder is a mapping \(h\). The encoder maps the input \(\Pi\) to the code \(\Phi\) while compressing it, which is written as \(h(\Pi)=\Phi\). Hence the encoder consists of the input layer, a variable number of hidden layers and the code layer, which is the left side of the schematic autoencoder in \cref{Fig:Autoencoder}. The decoder mirrors the encoder, which is not a necessity but often chosen that way, see the right side of \cref{Fig:Autoencoder}. Hence it comprises the same code layer, a variable number of hidden layers and the output layer. Similarily it is a mapping \(g\) which maps \(\Phi\) to \(\tilde{\Pi}\), an approximation to the initial \(\Pi\), and is written as \(g(\Phi)=\tilde{\Pi}\). The second largest allocation unit in autoencoders (and in general neural networks) are the layers. Each layer is a vector, a matrix or a three dimensional tensor. For simplicity we choose each layer to be vector valued in the following. Therefore the input layer can be an input vector like \(\mathbf{f}(t_1,\mathbf{v},x_1)\subseteq \Pi\) with \(\mathbf{f}(t_1,\mathbf{v},x_1) \in \mathbb{R}^2\). The vectors in the hidden layers are abstractions of the input. The term hidden stems from the fact, that one usually does not look at them as the interest mainly lies in the code -and output layer. The code layer is of main interest, as is holds the compressed abstraction of the input. Coming to the smallest allocation unit, the nodes. Each node refers to an entry in the vector/layer and is dispalyed as a circle in \cref{Fig:Autoencoder}. Two nodes are connected in a forward pass through solving \(O=I*w+b\), which is represented as a computational graph in \cref{Fig:Autoencoder}. Here the input is \(I\), which is multiplied with a weight \(w\) and by adding a bias \(b\), one obtains the output \(O\). Hence every connection between nodes contains two free parameters: a weight \(w\) and a bias \(b\). All of the aforementioned can be found in \cite{Goodfellow}.\\
\begin{figure}
	\centering
	\input{Figures/Reduced_Order_Algorithms/AE_scheme.tex}
	\caption{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi)=\tilde{\Pi}\), taking \(\Phi\) as input and outputs an approximation \(\tilde{\Pi}\) of \(\Pi\). Similarily the encoder is a mapping \(h(\Pi)=\Phi\), taking \(\Phi\) as input and outputs the code \(\Phi\). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The content of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), content of a node right of the inital node.}
	\label{Fig:Autoencoder}
\end{figure}
\subsection{Training}
\begin{figure}
	\input{Figures/Reduced_Order_Algorithms/Training.tex}
	\caption{\footnotesize training}
	\label{Fig:Training}
\end{figure}
In order to satisfy the objective of learning an optimal basis in the code layer, autoencoders need to be regularized. Regularization is defined in \cite{Goodfellow} as any modification made to a learning algorithm that is intended to reduce its generalization error but not it's training error. As already mentioned Autoencoders have a central hidden layer between the equally sized input -and output layer. In undercomplete autoencoders the central hidden layer is also called bottleneck layer, because it's size is smaller than the input. Thus the undercomplete autoencoder is forced to decide which information to copy and by that measure is regularized. Overcomplete autoencoders on the other hand have the central hidden layer greater or of the same size as the input, hence making it indispencable to use additional regularization, which, necessary to add, can be also fruitful in undercomplete autoencoders. Note that an undercomplete autoencoder as shown in \cref{Fig:Autoencoder} will be used in this thesis. Regularization is always advantageous when the capacity of an autoencoder is too big. The capacity of any neural network can be altered through the number of free parameters available for the model to fit the input data.\\

%The term deep learning situated around the much broader field of artificial intelligence stems from the use of deep feed forward networks also called multi layer perceptrons (MLP) or feed forward neural networks. Deep recurrent neural networks (RNN) are also used in this field but won't be covered in this thesis. In contrast to RNNs, information flows forward through these networks, which explains the name feed foward. In the following I will use the abbreviation MLP when talking about the aforementioned algorithm. Network refers to the typical composition of many different functions.\\The task of any MLP is to approximate a function \(f^\star(\mathbf{x};\mathbf{\Theta}) \approx f(\mathbf{x})\) through learning the values of the parameters \(\mathbf{\Theta}\). As mentioned before  \(f^\star\) is a composition of functions eg. \cref{Eq. Composition}.
%\begin{equation}
%	f^\star(\mathbf{x};\mathbf{\Theta})= f^3(f^2(f^1(\mathbf{x};\mathbf{\Theta}^1),\mathbf{\Theta}^2),\mathbf{\Theta}^3)
%	\label{Eq. Composition}
%\end{equation}
%In \cref{Eq. Composition} each function \(f^i\) is called layer. In this example \(f^1\) is called the input layer, \(f^3\) the output layer and \(f^2\) a hidden layer. Hence a layer is a vector-to-vector function. In this context a unit describes the corresponding vector-to-scalar functions of one layer. The width of a layer is referred to as the dimension of the vector valued input. The depth of the network describes the number of composed functions. In autoencoders the dimensions of input and output layer are identical.
\newpage
Obviously deep learning emphasizes the focus on the depth of a model. This is because linear models with just one layer can only approximate linear functions. Adding a non-linear activation function to the output of the proposed model wouldn't be sufficient in modeling any nonlinear behavior of the function. However the universal approximation theorem \cite{Hornik1989} states that MLPs with at least one hidden layer and any non-linear activation function can approximate any function given that enough hidden layers can be provided. In conclusion, MLPs are universal approximators \cite{Goodfellow}.\\
There are several types of layers, that can be used in MLPs. For this thesis it is sufficient to treat so called \textit{fully connected layers} and \textit{convolutional layers}.
Fully connected layers are called \texttt{Linear}\cite{bibid} in \texttt{PyTorch}\cite{bibid} because they compute a linear transformation of the input \cref{Eq. Linear Transformation}, where \(x\) is the input vector, \(A\) is the weight matrix and \(b\) is a bias vector. This is the forward pass of a linear layer. The learnable parameters \(\Theta\) are in this case the values in \(A\) and \(b\). For a linear layer which takes a vector of size \(i\) as input and outputs a vector of size \(o\) there are \(l = i \times o + o\) learnable parameters. 
\begin{equation}
	y = xA^T + b \label{Eq. Linear Transformation}
\end{equation}
Continuing with the forward pass in convolutional layers, which are called \texttt{Conv2D}\cite{bibid} in \texttt{Pytorch}. Convolutional layers are usually applied when the input data has a known grid-like topology \cite{Goodfellow}.While for fully connected layers, the input size is fixed, convolutional layers can be applied to inputs of various sizes. Furthermore, they are sparse by construction and share parameters making them equivariant \cite{Goodfellow}.  Even tough the name implies the use of the convolutional operation in \cref{Eq. Convolution}, \texttt{PyTorch} and many other neural network libraries instead use the cross correlation, which is an operation closely related to a convolution\cite{Goodfellow}\cite{Pytorch website}. The convolution in \ref{Eq. Convolution} operates on two functions \(x\) and \(w\), where the latter is a weighting function of the former which makes \(s\) the weighted average of the input \(x\). In \cref{Eq. Discrete Convolution} \(x\) is represented by \(I(m,n)\), and \(w\) by \(K(m,n)\) respectively. Note that while \cref{Eq. Convolution} is scalar valued and \cref{Eq. Discrete Convolution} is two dimensional, only for illustrative reasons. 
\begin{align}
	s(t) &= (x * w)(t) = \int x(a)w(t-a)\,da \label{Eq. Convolution}\\
	S(i,j) &= (I * K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n)
	\label{Eq. Discrete Convolution}
\end{align}
One drawback in implementing the discrete convolution \cref{Eq. Discrete Convolution} is that there can be invalid values for \(m\) and \(n\). This can be solved by exploiting the commutative property of the convolution, which results for the discrete convolution in a flipped kernel relative to the input, \cref{Eq. Discrete Convolution Flip}. Without the need of flipping the kernel which is not always possible, i.e. exploiting the commutative property, cross correlation is adopted \cref{Eq. Discrete Cross Correlation}.
\begin{align}
	S(i,j) &= (K * I)(i,j) = \sum_{m}\sum_{n}I(i-m,j-n)K(m,n) 
	\label{Eq. Discrete Convolution Flip}\\
	S(i,j) &= (I * K)(i,j) = \sum_{m}\sum_{n}I(i+m,j+n)K(m,n) 
	\label{Eq. Discrete Cross Correlation}
\end{align}
The given equations illustrate the movement of the kernel over a two dimensional input, but leaving out two basic accompanying designs. First is the so called stride of the kernel, which results in a increased down sampling with increased stride size. Considering strides of the kernel along one dimension of the input results in a shrinkage of that dimension by 
\begin{equation}\label{Eq:Downsampling}
	o = \frac{i -k}{s} + 1.
\end{equation} 
Here \(o\), \(i\), \(s\) and \(k\) are the output, input, stride and kernel size of one dimension respectively \cite{dumoulin2018guide}. Second are the so called channels, which allow convolutional layers to extract a different feature for every channel from the same input. For a two dimensional input like images this could be different features for the same location on the image, like edges and color in the RGB color space \cite{Goodfellow}. Adding strides and kernel to \cref{Eq. Discrete Cross Correlation} yields
\begin{equation}
	\mathsf{S}_{i,j,k} = c(\mathsf{K},\mathsf{I},s)_{i,j,k}\sum_{l,m,n}\left[\mathsf{I}_{l,(j-1)\times s+m,(k -1)\times s+n}\mathsf{K}_{i,l,m,n}\right]. \label{Eq. Cross correlation stride channel}
\end{equation}
The kernel \(\mathsf{K}\) is a four dimensional tensor with \(i\) indexing into the output channels of \(S\), \(l\) indexing into the input channels of \(I\), \(m\) and \(n\) indexing into the rows and colums. The input \(\mathsf{I}\) and output \(S\) are three dimensional tensors with \(j\) and \(k\) indexing into the rows and columns. Note that in \texttt{PyTorch} \cref{Eq. Cross correlation stride channel} is a so called valid cross correlation (valid convolution) \cite{bibid} meaning the kernel will only move over input units for which  all \(m\) and \(n\) are inside the rows and columns of the input. Zero padding the input can prevent the kernel from omitting corners in that case.
\\
For autoencoders the necessity to perform a transposition of the applied layers arises, which is straight forward for fully connected layers. Convolutional layers with strides greater than unity on the other hand  the transposition needs the kernel to be padded with zeros to realize an upsampling of the input data \cite{dumoulin2018guide}. Note that the padding of the kernel with zeros is only used to illustrate how the upsampling works. In \cref{Eq: Transposed convolution} taken from \cite{Goodfellow} multiplications with zero are omitted. The size of one dimension during upsampling can be calculated with the transposition of \cref{Eq:Downsampling}.
\begin{equation}\label{Eq: Transposed convolution}
	t(\mathsf{K},\mathsf{H},s)_{i,j,k} = \sum_{\substack{l,m\\s.t\\(l-1)\times s+m=j}}
												  \sum_{\substack{n,p\\s.t\\(n-1)\times s+p=k}}
												  \sum_q \mathsf{K}_{q,i,m,p}\mathsf{H}_{q,l,n} 
\end{equation}
\noindent
Forward-propagation is then referred to as the compositional evaluation of each layer. For the example in \cref{Eq. Composition} the outputs of each layer would then be:  \(f^1(\mathbf{x},\mathbf{\Theta}^1) = \mathbf{p}\), \(f^2(\mathbf{p},\mathbf{\Theta}^2) = \mathbf{q}\) and \(f^3(\mathbf{q},\mathbf{\Theta}^3) = \mathbf{\hat{y}}\).\\
Subsequently the cost function \(J(\Theta)\), which will be discussed later on, can be computed. Afterwards back-propagation returns the gradients w.r.t. the layer parameters \(\Theta\) to finally compute updated parameters \(\mathbf{\Theta}\).  The name back-propagation refers to the use of the chain rule of calculus to obtain the gradients of each layer. Assuming again the example in \cref{Eq. Composition} back-propagation would be equations \cref{Eq. Backpropagation1} to \cref{Eq. Backpropagation3}:
\begin{align}
	\frac{\partial J}{\partial \mathbf{\Theta}^3} &= \frac{\partial J}{\partial\hat{y}_i}\frac{\partial\hat{y}}{\partial \Theta^3}
\label{Eq. Backpropagation3}\\
	\frac{\partial J}{\partial \Theta^2} &= \frac{\partial J}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial q}\frac{\partial q}{\partial \Theta^2}
\label{Eq. Backpropagation2}\\
	\frac{\partial J}{\partial \mathbf{\Theta}^1} &= \frac{\partial J}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial q}\frac{\partial q}{\partial p}\frac{\partial p}{\partial \Theta^1}
\label{Eq. Backpropagation1}
\end{align}
While the term back-propagation is solely used for the method to compute the gradients for each layer in a backward fashion, meaning form the last layer to the first, the update of the parameters is done in an optimization step.
-L2-error as perfomance metrics
\subsection{Autoencoders}
Autoencoders have many hyperparameters determining their capability for compression and subsequent reconstruction. These parameters include : \textit{depth}, \textit{width of layers}, \textit{activation functions}, \textit{batch-size}, \textit{learning rate}, \textit{number of filters}, \textit{stride width}, \textit{kernel size}. Their finding is discussed in this section, for both $\hy$ and $\rare$.\\
For the fully connected autoencoder the order of determining the hyperparameters is as follows:
Depth $\rightarrow$ Hidden Units $\rightarrow$ Batch Size $\rightarrow$  Activation Fuctions.
For the convolutional autoencoder the order of determining the hyperparameters is as follows:
Depth $\rightarrow$ Channels $\rightarrow$ Batch Size $\rightarrow$ Activation Functions.
Both models are evaluated on the course of the training. Figures showing the training process for all experiments are provided in the appendix. After training the L2-Error\cref{labellist} is evaluated on the unshuffled, complete dataset as described in \cref{Sec: Data Sampling}.\\
Important to mention is that the difficulty of finding the right set of hyperparameters for MLPs led to an intensive search prior to the creation of this contribution. The difficulty lies in the fact that there is little systematic knowledge about how the hyperparameters interact in the model and answer to a variety of input data. Goodfellow et al. point out that with a combination of intuition, certain methods and first of all experience practitioners find hyperparameters that work well \cite{Goodfellow}. As for this thesis a list of the prior search is provided in \cref{Sec:AppendixA}. This list does not claim to cover the complete search and can by no means give enough insight to enable reproducability by another person.\\
For this reason an insight into the methods employed to find the set of hyperparameters used in this contribution are described in the following. 

The convolutional autoencoder architecture 1.1 is a composition of six convolutional and three fully conected layers, \cref{f_Conv}.
\begin{equation}
y_p = f_{C}^9(f_{C}^8(f_{C}^7(f_{F}^6(f_{F}^5(f_{F}^4(f_{C}^3(f_{C}^2(f_{C}^1(y_0)))))))))
\label{f_Conv}
\end{equation}