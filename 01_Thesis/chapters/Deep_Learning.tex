% !TEX root = master.tex

\chapter{Dimensionality reduction algorithms}
\label{Ch:DimRedAl}
%\pagenumbering{arabic}

In this chapter dimensionality reduction algorithms which will be applied to solutions of the BGK model in Sod's shock tube are introduced: Proper orthogonal Decomposition (POD) and Autoencoders (AEs).\\
We will start off with a short introducing into POD, continue with autoencoders and finish with a detailed description of deep learning. Note that the main focus of this thesis lies in the application of autoencoders for which POD serves as a comparative method.\\
Dimensionality reduction algorithms lie at the heart of any reduced order model (ROM) which is described in the following chapter. As input serve datasets such as solutions of a full order model (FOM) or experimental data. These datasets may contain the dynamics of a spatio-temporal problem. The output is an approximation to the input. It is reconstructed from a low-dimensional representation that captured the underlying dynamics of the input problem. 
\section{Proper orthogonal decomposition (POD)}
\label{Sec: POD}
%POD
The solution of PDEs, precisely \(f(x,v,t)\), can be approximated either through a discretization into a system of ODEs as described in \cref{Ch:BGK}, or alternatively through a separation of variables ansatz
\begin{equation}
	f(t,v,x) = \sum_{i=1}^n a_i(t)\Phi_i(x,v)\mathrm{,}
\end{equation}  
as descried in \cite{Kutz}. Temporal dependence rendered through \(a_i(t)\) is independent from the spatial information carried in \(\Phi_i(x,v)\). Here \(\Phi_i(x,v)\) is called the \(i\)-th basis mode. With increasing \(i\), the accuracy of the solution consequently increases as well, which is similar to increasing the spatial resolution in finite difference methods outlined in \cref{Ch:BGK}. The essence of dimensionality reduction algorithms is now to find optimal basis modes \(\Phi_i(x,v)\). Optimality here means capturing the dynamic answer to a given geometry and inital conditions thus permitting to exploit a minimal number \(n\) of basis modes to reconstruct the dynamics.\\
An optimal basis can be provided by POD. It leverages a physically interpretable spatio-temporal decomposition of the input data \cite{Kutz}. At first this data needs to be preprocessed in a fashion, that seperates the temporal and spatial axis. Each temporal state is called snapshot and is stacked in a matrix \(\btch\) such that
\begin{equation}
	\btch=\left[ \mathbf{u}_{1},\mathbf{u}_2,\dots,\mathbf{u}_n \right] \qquad \mathrm{with}\qquad \mathbf{u}_i=\left[f(v_1,x_1),\dots,f(v_k,x_1),f(v_1,x_2),\dots,f(v_k,x_j)\right]^T \mathrm{,}
	\label{Eq:PiPOD}
\end{equation}
where \(n\) is the number of available snapshots and \(\mathbf{u}_i\) the \(i\)-th snapshot with \(\mathbf{u}_i \in \mathbb{R}^{j\times k}\). Afterwards \(\btch\) is decomposed using the singular value decomposition (SVD) which solves the left and right singular value problem leveraging
\begin{equation}
\btch = U\Sigma V^*\mathrm{,}
\end{equation}
where \(U\) is a unitary matrix containing the left singular vectors of \(\btch\) in it's columns and \(V\), also a unitary matrix, containing the right singular vectors in it's columns. Here superscribt asterix denotes the complex conjugate transpose. Furthermore \(\Sigma\) is a sparse matrix with the singular values in descending order on it's diagonal \cite{Kutz}. Note that the SVD always produces as much singular values and in turn singular vectors as there are elements on the shortest axis of the input matrix. Hence when preprocessing the input data as described above one gets as many singular values -and vectors as there are snapshots, given that the resolution in time is smaller than the spatial resolution.\\
Next, by applying the Eckard-Young theorem, that can be looked up in \cite{Kutz}, it is possible to solely harness the first leading singular values and corresponding vectors to approximate \(\btch\) to a desired accuracy. The theorem states that the the optimal rank-r approximation to $\btch$, in a least-squares sense, is given by the rank-r SVD truncation \(\tilde{\btch}\):
\begin{equation}
\underset{\tilde{\btch}, s.t. rank(\tilde{\btch})=r}{\operatorname{argmin}} \| 
\btch -\tilde{\btch}\|_F
=\tilde{U}\tilde{\Sigma}\tilde{V}^*\mathrm{.}
\label{Eq:EckardYoung}
\end{equation} 
Here \(\tilde{U}\) and \(\tilde{V}\) denote the first \(r\) leading columns of \(\mathbf{U}\) and \(\mathbf{V}\), and \(\tilde{\Sigma}\) contains the leading \(r \times r\) sub-block of \(\Sigma\). \(\|\cdot\|_F\) is the Frobenius norm \cite{Kutz}.\\
When decomposing a matrix that contains snapshots of a dynamical system, the columns of \(\tilde{U}\) and \(\tilde{V}\) contain dominant patterns that describe the dynamical system. Moreover they provide a hierarchical set of modes, that characterize the observed attractor on which we may project a low-dimensional dynamical system to obtain reduced order models. That being said, we use the left singular values of \(\tilde{U}\) as optimal basis modes such that
\begin{equation}
	\tilde{U} = \Phi = \left[\Phi_1,\Phi_2,\dots,\Phi_r\right]\mathrm{.}
\end{equation}
Vectors in \(\Phi\) are orthogonal to each other and in that way provide a coordinate transformation from the high dimensional input space onto the low dimensional pattern space.[PLS CHECK AGAIN, sentence is stolen from KUTZ]
\section{Autoencoders}
\label{Sec:AE}
%Autoencoder

Another way to obtain an optimal basis is to employ a machine learning architecture situated in the field of deep learning called autoencoders. An autoencoder is a feed forward neural network, that is trained to learn salient features of it's input by compressing it and successively reconstructing it back from the compressed version. In that way one could say that an autoencoder simply copies it's input to it's output \cite{Goodfellow}. \Cref{Fig:Autoencoder} shows a schematic representation of the architecture for a deep undercomplete autoencoder and the necessary terminology used to describe it's components. For the moment we ignore the difference between an undercomplete and overcomplete autoencoder, and come back to that when talking about regularization. \\
The biggest allocation unit in autoencoders is the distinction of a decoder and a encoder part, which are seperated at central code layer. The encoder is a mapping \(h\). The encoder maps the input \(\btch\) to the code \(\Phi\) while compressing it, which is written as \(h(\btch)=\Phi\). Hence the encoder consists of the input layer, a variable number of hidden layers and the code layer, which is the left side of the schematic autoencoder in \cref{Fig:Autoencoder}. The decoder mirrors the encoder, which is not a necessity but often chosen that way, see the right side of \cref{Fig:Autoencoder}. It comprises the same code layer, a variable number of hidden layers and the output layer. Similarily it is a mapping \(g\) which maps \(\Phi\) to \(\tilde{\btch}\), an approximation to the initial \(\btch\), and is written as \(g(\Phi)=\tilde{\btch}\). The number of hidden layers in encoder and decoder is the first of the so called hyperparameters that can be tuned to improve performance of the autoencoder. Note that we refer to autoencoders with more than one hidden layer, the code layer, as deep autoencoders. The reverse is a shallow autoencoder. The second largest allocation unit in autoencoders (and in general neural networks) are the layers. Each layer can be vector, a matrix or a three dimensional tensor. For the time beeing we choose each layer to be vector valued in the following. Therefore the input layer can be an input vector like \(\mathbf{u_1}\subseteq \btch\). The vectors in the hidden layers are abstractions of the input. The term hidden stems from the fact, that one usually does not look at them as the interest mainly lies in the code -and output layer. The code layer is of main interest, as is holds the compressed abstraction of the input. Coming to the smallest allocation unit, the nodes. Each node refers to an entry in the vector/layer and is dispalyed as a circle in \cref{Fig:Autoencoder}. The number of nodes per hidden layer is a second hyperparameter. Two nodes are connected in a forward pass through solving \(O=I*w+b\), which is represented as a computational graph in \cref{Fig:Autoencoder}. Here the input is \(I\), which is multiplied with a weight \(w\) and by adding a bias \(b\), one obtains the output \(O\). Hence every connection between nodes contains two free parameters: a weight \(w\) and a bias \(b\). Hence the whole network holds a set of parameters which we call \(\frepar \in \mathbb{R}\) in the following. A forward pass in this sense refers to the flow of information from the left side of the encoder to the right side of the decoder in \cref{Fig:Autoencoder}. All of the aforementioned can be found in \cite{Goodfellow}.\\
\begin{figure}
	\centering
	\input{Figures/Reduced_Order_Algorithms/AE_scheme.tex}
	\caption{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi)=\tilde{\btch}\), taking \(\Phi\) as input and outputs an approximation \(\tilde{\btch}\) of \(\btch\). Similarily the encoder is a mapping \(h(\btch)=\Phi\), taking \(\Phi\) as input and outputs the code \(\Phi\). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), input of a node right of the inital node.}
	\label{Fig:Autoencoder}
\end{figure}
\subsection{Training}
%Training

In a feed forward neural network, as the autoencoder used in this thesis, the information flows forward from input to output layer when we want to perform an evaluation as in \(AE(\btch) = g(h(\btch)) =\tilde{\btch }\). In that way the layer structure in \cref{Fig:Autoencoder} can be viewed as evaluating a composition of functions
\begin{equation}
	l^{(4)}(l^{(3)}(l^{(2)}(l^{(1)}(l^{(0)}(\btch))))) = g(h(\btch)) = AE(\btch) = \tilde{\btch}\mathrm{,}
	\label{Eq:Composition}
\end{equation}
where every function represents one layer. Here \(l_{(0)}\) and \(l_{(4)}\) are input layer and output layer, \(l_{(1)}\) and \(l_{(3)}\) the hidden layers in encoder and decoder with \(l_{(2)}\) the bottleneck layer. The evaluation of one layer or function is a linear transformation of the incoming data with
\begin{equation}
\tilde{\mathbf{u}}_1 = \mathbf{u}_1W + \mathbf{b}\mathrm{.} \label{Eq:Linear Transformation}
\end{equation}
Here \(\mathbf{u}_1\) is as in \cref{Eq:PiPOD}, \(W\) is the weight matrix and \(\mathbf{b}\) a bias vector, which is the same equation as shown in the computational graph in \cref{Fig:Autoencoder}. The weight matrices and bias vectors of each layer are collected in the set \(\frepar \in \mathbb{R}\).  It is self evident, that \(\frepar\) does not minimize the cost function \(J(\frepar)\) from the start with
\begin{equation}
	J(\frepar) := \frac{1}{n}\sum_{i=1}^n (\mathbf{u}_i - AE(\frepar;\mathbf{u}_i))^2\mathrm{,}
	\label{Eq:Cost}
\end{equation}
and needs to be found through a learning process, which is called training. The training will be discussed in the following.\\
Note that the cost function \(J(\theta)\) comprises the mean squared error over all snapshots \(\mathbf{u}_i\), which we will write as \(L(\btch,\tilde{\btch})\) the so called loss with
\begin{equation}
	L(\btch,\tilde{\btch}) := \frac{1}{n}\sum_{i=1}^n(\mathbf{u}_i - \tilde{\mathbf{u}}_i)^2 = E \mathrm{,}
\end{equation}
when specifically referring to \(E\) the distance between \(\btch\) and $\tilde{\btch}$. In order to minimize \(J(\frepar)\) we compute the gradient of \(J(\frepar)\) with respect to the free parameters \(\frepar\) written as \(\nabla_{\frepar}J\). Specifically we need to compute the derivative of the network with respect to the free parameters as seen in \cref{Eq:Cost}. Then we move in the opposite direction of the gradient, which yields an update of \(\frepar\) as
\begin{equation}
	\frepar \leftarrow \frepar - \epsilon\grd\mathrm{,} \qquad\mathrm{where}\qquad \grd:= \nabla_{\frepar}J\mathrm{.}
	\label{Eq: update}
\end{equation}
\begin{wrapfigure}{r}{.2\textwidth}
	\centering
	\vspace{-10pt}
	\input{Figures/Reduced_Order_Algorithms/Sing_expample.tex}
	\vspace{-5pt}
	\caption{\footnotesize A single node network with one hidden node between input and output node.}
	\label{Fig:Sing_example}
\end{wrapfigure}
\textbf{Backpropagation:} The computation of the gradient \(\grd\) is done in the so called backpropagation. As the name implies the information flows backwards through the network, from output to input layer, and propagates \(E\) backwards through the network. Responsible for this backward motion is the recursive application of the chain rule of calculus. Considering as a simple illustrative example \cref{Fig:Sing_example} with \(y=g(z,b)=g(f(x,a))\), a single node connection from input \(x\) to output \(y\) with a single hidden node. Then \(a\) is a weighting constant of the first node and \(b\) defines a weighting consant of the hidden node. Backpropagating the distance \(E\) with \(E = \frac{1}{2}(y_0 -y)^2\) between the output and the ground truth \(y_0\) though the network yields
\begin{align}
	\frac{\partial E}{\partial a} &= -(y_0 - y)\frac{\partial y}{\partial z}\frac{\partial z}{\partial a}\qquad \mathrm{and}\\
	\frac{\partial E}{\partial b} &= -(y_0 - y)\frac{\partial y}{\partial b}\mathrm{,}
\end{align}
which requires \(\frac{\partial E}{\partial a} = -(y_0 - y)\frac{\partial y}{\partial z}\frac{\partial z}{\partial a} = 0\) to minimize \(E\). The update rule is then given by
\begin{equation}
	a_{i+1} = a_i + \epsilon \frac{\partial E}{\partial a_i} \qquad\mathrm{and}\qquad
	b_{i+1} = b_i + \epsilon \frac{\partial E}{\partial b_i}\mathrm{.}
\end{equation}
This example is taken from \cite{Kutz} and shows the manner in which the chain rule of calculus provides the backwards direction when deriving the gradients of each layer with respect to the free parameters.\\
\textbf{Generalization:}
At this point the objective for training a neural network is specified. The difference between pure optimization and learning lies in their differing objectives. In pure optimization a cost function like \(J(\frepar)\) is minimized in order to fit \(\btch\) to \(\tilde{\btch}\). This is called the objective of the optimization task. Learning on the other hand uses the same objective as the optimization task, but equally cares about the so called generalization task. In machine learning generalization refers to the ability of the learning algorithm to not only fit the input data, but also to fit data it has not seen before. It does so by using salient features of the input data which enables generalization. This can only be achieved to a certain extend and is sometimes intractable. But how is the generalizing performance measured? We randomly shuffle the input data \(\btch\) and split it into a training and a validation set in a 80/20 fashion with \(\btch=\{\btch_{train},\btch_{val}\}\). The random shuffling is performed as to eliminate any bias on both sets. Then we minimize \(J(\frepar)\) using only \(\btch_{train}\) which minimizes \(L(\btch_{train},\tilde{\btch}_{train})\) and check if indirectly \(L(\btch_{val},\tilde{\btch}_{val})\) is beeing minimized as well. The indirect minimization of \(L(\btch_{val},\tilde{\btch}_{val})\) is called the generalization task. The loss over the validation set is called the validation error and the loss over the training set is called the train error. Note that in learning both tasks are equally valued. As mentioned before, we want to learn an optimal basis \(\base\) which specifically contains salient features of \(\btch\). With minimizing only the optimization task, we acquire the same basis as in POD. With learning we try to find an even more powerful basis \(\base\) that enables the network to generalize.\\ 
\textbf{Adam:}
The algorithm to compute the updates to \(\frepar\) is called Adam, an upgraded version of the classic stochastic gradient descent algorithm (SGD) with moments. The name stems from adaptive moment estimation, which refers to the adaptation of moments which in combination with the learning rate \(\epsilon\) apply scaled, directional updates to \(\frepar\) during training. Moments and how they are adapted are discussed a bit further on. The steps of Adam are shown in \cref{Alg:Adam} and explained successively. There we initially define hyperparameters which are the step size/learning rate \(\epsilon\), exponential decay rates \(d_1\) and \(d_2\) in \([0,1)\) and a small constant \(\delta\) for numerical stability. We can use the default values for all constants with \(\epsilon=1e-2\), \(d_1=0.9\), \(d_2=0.999\) and \(\delta=1e-8\).  Despite the learning rate \(\epsilon\), Adam is fairly robust w.r.t. changing it's hyperparameters. Hence solely the hyperparameter \(\epsilon\) requires tuning.
\begin{algorithm}[hbp!]
	\begin{algorithmic}
		\SetAlgoLined
		\REQUIRE Step size $\epsilon$
		\REQUIRE Exponential decay rates from moment estimates $d_1$ and $d_2$ in \([0,1)\)\\
		(Suggested defaults: 0.9 and 0.999 respectively)
		\REQUIRE Small constant $\delta$ used for numerial stabilization (Suggested default: $10^{-8}$)
		\REQUIRE Initial parameters $\frepar$\\
		Initialize 1st and 2nd moment variables \(\mathbf{s}=0\) and \(\mathbf{r}=0\)\;
		Initialize time step i = 0\;
		\While{stopping criterion not met}{
			Sample a minibatch $\minib_i$ containing  \(m\) examples from the training set \(\btch_{train}\)\;
			Compute gradient: \(\grd\ \leftarrow \frac{1}{m} \nabla_{\frepar} \sum_m L(\minib_i - AE(\frepar;\minib_i)\)\;
			\(t\leftarrow t + 1\)\;
			Update biased first moment estimate: $\mathbf{s} \leftarrow d_1\mathbf{s} + (1-d_1)\grd$\;
			Update biased second moment estimate: $\mathbf{r} \leftarrow d_2\mathbf{r} + (1-d_2) \grd\circ\grd$\;
			Correct bias in first moment: $\hat{\mathbf{s}} \leftarrow \frac{\mathbf{s}}{1-d_1^i}$\;
			Correct bias in second moment: $\hat{\mathbf{r}} \leftarrow \frac{\mathbf{r}}{1-d_2^i}$\;
			Compute update: $\Delta\frepar = -\epsilon \frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}}+\delta}$ (operations applied element-wise)\;
			Apply update: $\frepar \leftarrow \frepar + \Delta\frepar$
		}
	\end{algorithmic}
	\caption{The Adam algorithm}
	\label{Alg:Adam}
\end{algorithm} 
Next we initialize the 1st and 2nd moments \(\mathbf{s}\) and \(\mathbf{r}\) to zero, in contrast to \(\frepar\), which is initialized as follows.\\
The selection of starting values for \(\frepar\) is of crucial importance for the success of any neural network to learn something useful in a reasonable time. Moreover it strongly affects if the learning algorithm converges at all. This is partly because with the starting point we introduce a strong bias on the network considering that the SGD algorithm only makes marginal contributions to \(\frepar\) at every update. Even if \(J(\frepar)\) reaches a minimum, different initial values can lead to a variety of generalization errors. Specifically this connection between initial parameters and generalization is not well understood. Note that the topic of parameter initialization is despite it's importance, only sketched out briefly in this thesis. For more details see \cite{Goodfellow} for an overview, \cite{LeCun98} for the default initialization in \texttt{pytorch}, which is also used in this thesis and \cite{he2015} for very deep networks with more than eight layers and rectifying non-linear activations.\\
Usually the biases can be chosen to a heuristically constant value, while the weights need to be initialized so that they break symmetry between nodes. Imagine two hidden nodes with the same activation function connected to the same input. The same gradient would lead to a symmetric evolution during training, making one of the nodes redundant. Hence weights are initialized randomly often drawn from a Gaussian distribution. Next, finding the right scale for the distribution, which in turn scales the initial weights, is tackled. The spectrum of initial weight sizes ranges from large initial weights which are beneficial for breaking any symmetry, to small initial weights or even a sparse initialization. Now with this in mind, if we scale the distribution to produce large weights, we induce a prior, which states that all nodes are connected and how they are connected. Small initial weights on the other hand induce a prior, that says that it is more likely for nodes to not be connected and that the learning algorithm can choose which nodes to connect and how strong.\\
There exist several heuristics for the scale of the initial weights, which try to preserve the norm of the weigh matrix of each layer, to stay close to or below unity. Envisage that during forward as well as backward propagation we perform matrix multiplications from one layer to the next. Keeping the norm of each layer from exceeding unity, we prevent the development of exploding values and gradients in the respective last layer. A preventive measure is to scale the distribution for one layer with respect to it's non-linear activation with a factor called gain. Gains for non-linear activations can be found in \cite{torch_ini} along with several sampling methods. The effects described above become severe with large layers or equally with networks of a certain depth, which is not the case for the autoencoders used in this thesis allowing to comfortably choose the default initialization in \texttt{pytorch} for linear -and convolutional layers where we draw the initial weights from a normal distribution with \(U(-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}})\) as suggested in \cite{LeCun98}. The number of input nodes, also called fan-in, per layer is \(m\).\\
SGD is derived from the common gradient decent method, where the whole batch \(\btch_{train}\) is used per iteration to compute the gradient for an update of \(\frepar\) in the opposite direction of the gradient. Hence gradient descent is a so called batch gradient method or deterministic gradient method. SGD is a stochastic method. It approaches updates for \(\frepar\) as in \cref{Eq: update} in a stochastic manner, using only small portions of \(\btch_{train}\) per update. By randomly sampling only a small number of examples from \(\btch_{train}\) and taking the average gradient over those examples we compute an estimate of the true gradient which is much cheaper. Therefore SGD trains with so called minibatches of a certain size, which yields \(\btch_{train} = \{\btch_{t_i},\dots,\btch_{t_n}\}\). Here \(i\) counts through all the minibatches and \(n\) is the total number of minibatches. The size of one minibatch can be obtained through dividing the dimension of your input, on which you split the batch, by \(n\) and is called \(\mbs\) in the following and is another hyperparameter. Minibatch sizes \(\mbs\) are influenced by the following observations. As one minibatch can be processed in parallel to compute an update, extremely small \(\mbs\) underutilize multicore architectures. Extremely large \(\mbs\) on the other hand can be limited by available memory to be solved in parallel. When utilizing a graphics processing unit (GPU) \(\mbs\) of the power of two is advised to fully exhaust the whole hardware (array sizes of the power of two achieve better runtime in GPUs). Small \(\mbs\) can have a regularizing effect hence improve generalization, but also amplify the noisy behavior \cite{Goodfellow}[p.270ff].\\
Stochasticity is key in SDG and is achieved by shuffling the input data, which was already performed for the training and validation split. On every update the gradient is evaluated as an average over a minibatch of randomly sampled examples, yielding an unbiased estimate of the true gradient. This measure leads to noisy updates of \(\frepar\) which improves generalization and converges faster than common gradient descent. SGD has the ability to find a minimum even before the whole batch is processed, especially with large datasets, as elaborated in \cite{Goodfellow}[p.286 ff]. 
The noisy updates require a small learning rate, especially when the the minibatch size  \(\mbs\) is small, that decays over time as the noise does not vanish when a minimum is found. Up to now we saw that small minibatches improve generalization, underutilize available multicore hardware and amplify noisiness which needs to be tackled with small learning rates. Therefore the learning process is slowed down. Here momentum comes into play, which accelerates learning.\\
Momentum applies an exponentially decaying moving average of previous gradients to the current gradient. Momentum is an analogy to momentum in physics and therefore named after. Imagine a tightrope walker, who uses the moment of inertia of a long rod for balance as crossing the rope. Oscillations of the walker need to overcome the moment of inertia of the long rod in order to destabilize the walker. Another analogy is a fast car taking a sharp turn. The car drifts because the new direction is influenced by the old orthogonal momentum. If we take the update rule for the first moment estimate \(\mathbf{s}\) of \cref{Alg:Adam} and look at three consecutive updates
\begin{align}
	\mathbf{s}_i &= d_1\mathbf{s}_{i-1}+(1-d_1)\mathbf{g}_{i}\mathrm{,} \label{Eq: fm}\\
	\mathbf{s}_{i-1}&=d_1\mathbf{s}_{i-2}+(1-d_1)\mathbf{g}_{i-2}\mathrm{,}\\
	\mathbf{s}_{i-2}&=d_1\mathbf{s}_{i-3}+(1-d_1)\mathbf{g}_{i-3}\mathrm{,}
\end{align}
we see that by combining and simplifying the three updates yields
\begin{equation}
	\mathbf{s}_i = d_1d_1(1-d_1)\mathbf{g}_{i-2}+\dots+d_1(1-d_1)\mathbf{g}_{i-1}+\dots+(1-d_1)\mathbf{g}\mathrm{.}
\end{equation}
Note that this entanglement of the updates is taken from \cite{bushaev_2017}. Obviously the contributions to the current gradient vanish exponentially over iterations. This leads to a moving average of past and current gradients. Now taking into account that if \cref{Eq: fm} computes the moment of the first iteration, there are no previous gradients available which leads to a biased first computation. Hence we divide \(\mathbf{s}_i\) by \((1-d_1^i)\) for the bias correction of the first moment in \cref{Alg:Adam} which yields \(\hat{\mathbf{s}}\). In consecutive iterations obviously the bias correction approaches zero. All the same steps does the second biased moment estimate \(\mathbf{r}\) take. The second moment estimate is nothing more than the squared past gradients gradients written as \(\grd \circ\grd\) in \cref{Alg:Adam}. Together they from the parameter update \(\Delta\frepar\). Without going into further details, it can be shown that the parameter updates have an approximate upper bound of \(\Delta\frepar\lessapprox \epsilon\), see \cite{kingma2017adam}. This upper bound is a necessary criteria for the updates, as the learning rate \(\epsilon\) is a specifically tuned hyperparameter that should not be exceeded during training. The ratio \(\frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}}+\delta}\) is called signal to noise ratio (SNR) in \cite{kingma2017adam}, which evolves towards zero as an optimum is reached. Furthermore, the updates are invariant to the scale of the gradients as a factor \(a\) is always canceled out with \((a\cdot\hat{\mathbf{s}})/\sqrt{(a^2\cdot\hat{\mathbf{r}})}=\hat{\mathbf{s}}/\sqrt{\hat{\mathbf{r}}}\). To sum up, using first and second momentum we can accelerate training as the moving average gradient smooths the noisy updates and in turn is a better estimate of the true gradient, the bias in the first iteration is corrected and we stay invariant to gradient scaling.\\       
\begin{figure}[htbp!]
	\centering
	\input{Figures/Reduced_Order_Algorithms/Training.tex}
	\caption{\footnotesize Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch\) for training is split up into a training \(\btch_{train}\) and a validation \(\btch_{val}\) set with a \(80/20\) ratio (a). The \(\btch_{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite{LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib_{i})=\tilde{\minib}_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib_{i}\) and \(\tilde{\minib}_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd\). This gradient is then used to optimize weights \(w\) and biases \(b\) with  \(Adam\), an optimization algorithm seen in \cref{Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar\) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch_{val}\), which it has not seen before, in (h) with \(AE(\btch_{val})=\tilde{\btch}_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch_{val}\) and \(\tilde{\btch}_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline{E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.}
	\label{Fig:Training}
\end{figure}\noindent
The flowchart in \cref{Fig:Training} shows how the generalization query, weight initialization, backpropagation and the integration of Adam into the training procedure is carried out within this thesis. The network is trained in two loops, where the outer runs over so called epochs and the inner over all minibatches. One epoch is completed after all minibatches are shown to the network and the validation error is calculated as well as the train error, for which we take the average over all minibatches. The number of epochs \(H\) is determined based on over- and underfitting as well as on how many epochs are needed for Adam to converge and is another hyperparameter. In the following capacity, regularization, over- and underfitting is described.\\
\begin{figure}[htbp!]
	\centering
	\input{Figures/Reduced_Order_Algorithms/Capacity.tex}
	\caption{\footnotesize Figurative example of how capacity influences the evolution of training and validation error. With increasing capacity the model i.e. autoencoder is able to fit the training and validation set thus training and validation error decreases. Typically the training error is less than the validation error. Yet both errors are too high and the model is underfitting. Further increasing the capacity leads to an increase of the validation error while the training error further decreases. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the decrease in training error the model is overfitting and capacity passed the point of optimal capacity. Optimal capacity is the point where both optimization and generalization are in balance.}
	\label{Fig:Capacity}
\end{figure}\noindent
\textbf{Capacity, regularization, over- and underfitting:} In order to satisfy the objective of learning an optimal basis in the code layer, autoencoders need to be regularized. Regularization is defined in \cite{Goodfellow} as any modification made to a learning algorithm that is intended to reduce its generalization error but not it's training error. As already mentioned autoencoders have a central hidden layer between the equally sized input -and output layer. In undercomplete autoencoders the central hidden layer is also called bottleneck layer, because it's size is smaller than the input. Thus the undercomplete autoencoder is forced to decide which information to copy and by that measure is regularized. Overcomplete autoencoders on the other hand have the central hidden layer greater or of the same size as the input, hence making it indispensable to use additional regularization, which, necessary to add, can be also fruitful in undercomplete autoencoders. Note that an undercomplete autoencoder as shown in \cref{Fig:Autoencoder} is used in this thesis which is called autoencoder for simplicity. Regularization is always advantageous when the capacity of an autoencoder is too big. The capacity of any neural network can be altered through the number of free parameters available for the model to fit the input data. Recall that an autoencoder has a variable number of hidden layers (excluding the bottleneck layer) of variable size and thus a variable number of free parameters. Imagine having a model big enough in terms of free parameters to memorize the whole training set, we would achieve good results for the optimization task, but miss out on learning useful features thus failing at the generalization task. This relationship is exemplary shown in \cref{Fig:Capacity} where a point of optimal capacity is defined. Optimal capacity is reached when optimization and generalization in the form of training error and validation error are in balance. Note that \cref{Fig:Capacity} is taken from \cite{Goodfellow}[p.112]. Another way that alters the capacity of a neural network is adding non-linear activation functions to layers. So far we saw that every node and by that layers are connected through linear transformations as in \cref{Eq:Linear Transformation}. Thus the networks so called hypothesis space solely includes linear transformations of the input. The hypothesis space from which a neural network can choose the composition of the solution can be enriched with non-linear functions by simply evaluating layers through non-linear functions \(\nla\), also called activations, leading to
\begin{equation}
	\tilde{\mathbf{u}}_1 = \nla(\mathbf{u}_1W + \mathbf{b})\mathrm{.} \label{Eq:Non-Linear Transformation}
\end{equation}\noindent
\textbf{Activations:} Non-linar activations typically used for neural networks are summarized in \cref{Tab:Non-lin-func}. In this thesis these are also the activations used in the final network design or during hyperparameter search. We use two classes of activations. One is of type rectifier, which are linear for positive inputs and are zero, or decay below linear for negative inputs. The other is the tangens hyperbolicus (Tanh) and the Sigmoid-weighted linear unit (SiLU). Tanh and sigmoid share a similar "S"-shaped curve hence are put in one class. SiLU can be written as \(\textrm{SiLU}=\frac{1}{2}x(1+\tanh(\frac{x}{2}))\). Note that Tanh saturates for input values exceeding the range \([-2,2]\) which leads to a so called vanishing gradient. In the most severe case all updates can be zero. However in this thesis a vanishing gradient was not encountered. Theoretically it is possible to take any function as an activation. Though indispensable is their differentiability, which reduces to partly differentiable for rectifiers, to allow backpropagation. Activations \(\nla\) and where in the network to place them is yet another hyperparameter.
\begin{table}[htp]
	\centering
	\caption{\footnotesize Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha\) is typically, and also in this thesis set to \(\alpha=0.001\). Activations based on tangens hyperbolicus (Tanh) are the sigmoid function with \(\mathrm{sig}(x)= \frac{1}{2}(1+\tanh(\frac{x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.}
	\begin{tabular*}{16cm}{ @{\extracolsep{-4pt}} c c c c c @{} }
		\toprule
		ReLU & LeakyReLU & ELU & SiLU & Tanh \\   
		\midrule
		\small$\nla=\begin{cases}
			x,& \text{if } x\geq 0\\
			0,& \text{if } x<0
		\end{cases}$& 
		\small$\nla=\begin{cases}
		x,& \text{if } x\geq 0\\
		\alpha x,& \text{if } x<0
		\end{cases}$& 
		\small$\nla=\begin{cases}
		x,& \text{if } x>0\\
		\alpha (e^x-1),& \text{if } x\leq0
		\end{cases}$&
		\small$\nla=x \mathrm{sig}(x)$&
		\small$\nla=\tanh(x)$ \\
		\input{Figures/Reduced_Order_Algorithms/ReLU.tex}& 		\input{Figures/Reduced_Order_Algorithms/LeakyReLU.tex}&  \input{Figures/Reduced_Order_Algorithms/ELU.tex} &
		\input{Figures/Reduced_Order_Algorithms/SiLU.tex}&
		\input{Figures/Reduced_Order_Algorithms/Tanh.tex}\\
		\bottomrule
	\end{tabular*} \label{Tab:Non-lin-func}
\end{table}
\\
%Obviously deep learning emphasizes the focus on the depth of a model. This is because linear models with just one layer can only approximate linear functions. Adding a non-linear activation function to the output of the proposed model wouldn't be sufficient in modeling any nonlinear behavior of the function. However the universal approximation theorem \cite{Hornik1989} states that MLPs with at least one hidden layer and any non-linear activation function can approximate any function given that enough hidden layers can be provided. In conclusion, MLPs are universal approximators \cite{Goodfellow}.\\
\textbf{Layer types:} So far we have adoped the idea that, for two successive layers, all nodes from the first layer are connected to all nodes from the following layer and that all layers are vectors. Hence the types of layers are called fully connected or \texttt{Linear} in \texttt{pytorch}\cite{torch_ini}. If we now want the input to be a vector, a matrix or a three dimensional tensor we have to adopt another type of layer called convolutional layer. In \texttt{pytorch} these layers are called \texttt{Conv1D}, \texttt{Conv2D} and \texttt{Conv3D} respectively.  
Let us start with a simple example illustrating conceptual features of convolutional layers. A digital image can be viewed as a distribution of pixels over a two dimensional surface, where the surfaces dimensions are described with the images width and height. Furthermore, each pixel has a color expressed in values of the primary colours red, green and blue (RGB). Hence each pixel is a combination of different RGB values. The RGB values for each pixel are stored in so called channels. The red channel, green channel and blue channel. This concludes, that the dimension of a digital image is additionally equipped with the channel dimension. Let an image be \(Img\), then  \(Img \in \mathbb{R}^{m\times n\times c}\), with \(m\) the width this image, \(n\) the height of this image and \(c\) the three RGB channels. Another important characteristic of images is the spatial correlation between neighboring pixels. This can be for example sharp corners that separate foreground from background or differentiate between entities portrayed in the image. Therefore similarly as saving information about colour composition in channels, we can imagine saving spatial information in additional channels. For example corners and other geometric shapes saved in a channels for themselves. With this idea in mind convolutional layers can be introduced. Convolutional layers comprise a so called kernel, which moves over the image and by that all the input channels. Therefore, if the kernel is $K$ then $K \in \mathbb{R}^{i\times j\times c}$. The height $i$ and width $j$ of the kernel can be adjusted and is usually smaller than the input's height and width. The $c$ dimension is always equal to the $c$ dimension of the input. The distance the kernel travels over the input at every step is specified with the so called stride $s$. For a two dimensional input image like $Img$ the strides are given in direction $s_1$  and $s_2$, where the latter corresponds to $m$ and the former corresponds to $n$ of $Img$. The kernel performs the same linear transformations of the input as fully connected layers at every part of the input visited. This action outputs a so called feature map, which as the name implies should capture features of the input. A comparison of a one strided convolution with $s_1=s_2=1$ and a two strided convolution with $s_1=s_2=2$ is given in \cref{Fig: Downsampling}. The one strided convolution yields a feature map with a reduction of $m$ and $n$ by one, while the two strided convolution yields a feature map with a reduction of $m$ and $n$ by 2. Note that in \cref{Fig: Downsampling} eventual channels are omitted.
\\Even tough the name convolutional layer implies the use of the convolutional operation \texttt{PyTorch} and many other neural network libraries instead use the cross correlation operation, which is an operation closely related to a convolution \cite{Goodfellow} \cite{Pytorch website}. Details on why the cross correlation is used instead of the convolution is discussed further in \cite{Goodfellow}. In \cref{Eq. Discrete Cross Correlation} the two dimensional discrete cross correlation without channels is given with
\begin{align}
M_{m,n} = \sum_{s_1,s_2,i,j}Img_{(m\times s_1)+i,(n\times s_2)+j}K_{i,j}\mathrm{.}
\label{Eq. Discrete Cross Correlation}
\end{align}
There $M_{m,n}$ represents a point on a feature map, where $m$ and $n$ refer to the location of that point. The input to the cross correlation is exemplary our image $Img$ where coordinates of a pixel are also given in $m$ and $n$. The kernel is given as $K$ with coordinates in $i$ and $j$. The summation in \cref{Eq. Discrete Cross Correlation} is evaluated over $i$, $j$ and $s_1$ and $s_2$. Imagine beeing on $Img$ at $m=1$ and $n=1$ with $s_1=1$ and $s_2=1$, then the summation is performed over all pixels of $Img$ that the kernel covers in $i$ and $j$ from the starting point. If we then evolve to $s_1=2$ and $s_2=1$ we can see that we start with the same summation at $m=2$ and $n=1$ as seen in \cref{Fig: Downsampling} (a). Hence the kernel $K$ moved along the $m$ axis of the input image $Img$. Therefore by evolving in $s_1$ and $s_2$ the kernel moves over the $Img$ and eventually covers the whole input.
\begin{figure}[htbp!]
	\begin{subfigure}{.459\textwidth}
		\centering
		\input{Figures/Reduced_Order_Algorithms/Stride_01.tex}
		\subcaption{Two dimensional example of of a \textbf{one} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(3\times 3\) matrix, as we downsampled the input by a factor of \(0.75\).}
		\label{Fig: Downsampling(a)}
	\end{subfigure}\hfill
	\begin{subfigure}{.441\textwidth}
		\centering
		\input{Figures/Reduced_Order_Algorithms/Stride_02.tex}
		\subcaption{Two dimensional example of of a \textbf{two} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors.. The resulting feature map is a \(2\times 2\) matrix, as we downsampled the input by a factor of \(0.5\).}
		\label{Fig: Downsampling(b)}
	\end{subfigure}
	\caption{Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.}
	\label{Fig: Downsampling}
\end{figure}
\\Next, if we want more feature maps, because we may assume many features are present in the input, we can specify more than one kernel to move over the input, each yielding a different feature map. In \cref{Fig: Kernel} an illustrative example of a convolutional neural network with four convolutional layers, which downsamples the width and height of the input in every layer while at the same time adds channels (feature maps) in every layer. When arriving at the last convolutional layer, the featuremaps are flattened and fully connected to the code layer. A comparable scheme for the encoder side of a convolutional autoencoder is used in this thesis. When the encoder in \cref{Fig: Kernel} is mirrored at the code layer and additionally all convolutional layers are replaced with transposed convolutional layers, we get the decoder and by that arrive at the typical autoencoder architecture as in \cref{Fig:Autoencoder}. In conclusion, up to now we have identified four more hyperparameters for a single convolutional layer: kernel width and height, number of kernels i.e. feature maps, which are usually called output channels and stride.\\
\begin{figure}
	\centering
	\input{Figures/Reduced_Order_Algorithms/Kernel.tex}
	\caption{\footnotesize Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.}
	\label{Fig: Kernel}
\end{figure}\noindent
Fully connected layers are able to down- and upsample making them applicable for the decoder and encoder in autoencoders. Convolutional layers on the other hand are not able to upsample a given input. Therefore the transpose of a convolutional layer is applied for upsampling. The respective modules for vectors, matrices and three dimensional tensors are called \texttt{ConvTranspose1d}, \texttt{ConvTranspose2d} and \texttt{ConvTranspose3d} in \texttt{pyTroch}. The transpose of a convolutional layer is not a transpose in the mathematical sense when tranposing a matrix for example. It is still called that way as in a transposed convolutional layer the backward- and forward pass are swapped. In implementation a transposed convolutional layer one simply swaps backward and forward pass of the respective convolutional layer \cite{Goodfellow}. A two dimensional transposed convolution without channels can be described by taking a point of the input and performing a linear transformation of that input with all kernel entries. The result is then the first part of the output located at the upper left corner. A successive point is taken and again linear transformed with all kernel entries to create the second part of the output, which is part of the output by $s_1$ strides counted from the upper left corner, away from the first part. This action can lead to overlapping entries on the ouput which are simply added. In \cref{Fig: Upsampling(a)} this process is shown for an $3 \times 3$ input matrix, which is upsampled by a $3\times 3$ kernel moving with stride length $s_1=s_2=2$. The upsampled result is of size $7 \times 7$. With a kernel size of 3 in one direction and a stride $s_1=2$ in the same direction, there is always an uneven overlap in the output entries resulting in the checkerboard structure as seen in \cref{Fig: Upsampling(a)}. One common problem when using transposed convolutional layers are artifacts that resemble a checkerboard structure. In \cref{Fig: Upsampling(a)} an example is given showing how the checkerboard structure on the upsampled output arises.
Zero padding is available for convolutional layers as well as for the transpose of convolutional layers. Zero padding refers to pad the outer edges of the input with zeros. As shown in \cref{Fig: Downsampling}, is the factor by which the input size is reduced controlled by the kernel size and stride width. Unfortunately customizing both hyperparameters only for the donwsampling rate can be detrimental for learning off the the input. Imagine having a kernel small enough to not be able to recognize overkill edges. Or similarly a step size large enough to overpass edges. Hence kernel size and stride width are somewhat determined by the structure of the input. By zero padding outlines of the input down- and upsampling rate can be tuned independently from kernel size and stride width.
\begin{equation}
	o = \left[\frac{i+2p-k}{s}\right]+1
\end{equation}
A detailed description of what Dumoulin et. al call convolutional arithmetric can be found \cite{dumoulin2018guide}.  
\begin{figure}[htbp!]
	\begin{subfigure}{.435\textwidth}
		\centering
		\input{Figures/Reduced_Order_Algorithms/Transpose_even.tex}
		\subcaption{Two dimensional example of the transpose of a \textbf{one} strided convolution over a \(3\times 3\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(o\) and \(p\) of the feature map are given. The resulting feature map is a \(3\times 3\) matrix, as we upsampled the input by a factor of \(0.75\). Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors.}
		\label{Fig: Upsampling(a)}
	\end{subfigure}\hfill
	\begin{subfigure}{.455\textwidth}
		\centering
		\input{Figures/Reduced_Order_Algorithms/Transpose_uneven.tex}
		\subcaption{Two dimensional example of the transpose of a \textbf{one} strided convolution over over a \(2\times 2\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(o\) and \(p\) of the feature map are given. The resulting feature map is a \(4\times 4\) matrix. Upsampling by a factor of \(0.5\) is achieved with zero padding (dashed lines). Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors.}
		\label{Fig: Upsampling(b)}
	\end{subfigure}
	\caption{Upsampling of the convolutions illustrated in \cref{Fig: Downsampling} using transposed convolutions. Upsampling a $3\times 3$ input matrix to a $4\times 4$ featuremap is reversing \cref{Fig: Downsampling(a)}. Upsampling a $2\times 2$ input matrix to a $4\times 4$ featuremap is reversing \cref{Fig: Downsampling(b)}.}
	\label{Fig: Upsampling}
\end{figure}
%In the discrete cross correlation in \cref{Eq. Discrete Cross Correlation} The given equation for the discrete cross correlation in two dimensions (excluding channels) illustrate the movement of a kernel over a two dimensional input. First is the so called stride of the kernel, which results in a increased down sampling with increased stride size. Considering strides of the kernel along one dimension of the input results in a shrinkage of that dimension by 
%\begin{equation}\label{Eq:Downsampling}
%	o = \frac{i -k}{s} + 1.
%\end{equation} 
%Here \(o\), \(i\), \(s\) and \(k\) are the output, input, stride and kernel size of one dimension respectively \cite{dumoulin2018guide}. Second are the so called channels, which allow convolutional layers to extract a different feature for every channel from the same input. For a two dimensional input like images this could be different features for the same location on the image, like edges and color in the RGB color space \cite{Goodfellow}. Adding strides and kernel to \cref{Eq. Discrete Cross Correlation} yields
%\begin{equation}
%	\mathsf{S}_{i,j,k} = c(\mathsf{K},\mathsf{I},s)_{i,j,k}\sum_{l,m,n}\left[\mathsf{I}_{l,(j-1)\times s+m,(k -1)\times s+n}\mathsf{K}_{i,l,m,n}\right]. \label{Eq. Cross correlation stride channel}
%\end{equation}
%The kernel \(\mathsf{K}\) is a four dimensional tensor with \(i\) indexing into the output channels of \(S\), \(l\) indexing into the input channels of \(I\), \(m\) and \(n\) indexing into the rows and colums. The input \(\mathsf{I}\) and output \(S\) are three dimensional tensors with \(j\) and \(k\) indexing into the rows and columns. Note that in \texttt{PyTorch} \cref{Eq. Cross correlation stride channel} is a so called valid cross correlation (valid convolution) \cite{bibid} meaning the kernel will only move over input units for which  all \(m\) and \(n\) are inside the rows and columns of the input. Zero padding the input can prevent the kernel from omitting corners in that case.
%\\
%For autoencoders the necessity to perform a transposition of the applied layers arises, which is straight forward for fully connected layers. Convolutional layers with strides greater than unity on the other hand  the transposition needs the kernel to be padded with zeros to realize an upsampling of the input data \cite{dumoulin2018guide}. Note that the padding of the kernel with zeros is only used to illustrate how the upsampling works. In \cref{Eq: Transposed convolution} taken from \cite{Goodfellow} multiplications with zero are omitted. The size of one dimension during upsampling can be calculated with the transposition of \cref{Eq:Downsampling}.
%\begin{equation}\label{Eq: Transposed convolution}
%	t(\mathsf{K},\mathsf{H},s)_{i,j,k} = \sum_{\substack{l,m\\s.t\\(l-1)\times s+m=j}}
%												  \sum_{\substack{n,p\\s.t\\(n-1)\times s+p=k}}
%												  \sum_q \mathsf{K}_{q,i,m,p}\mathsf{H}_{q,l,n} 
%\end{equation}


%\subsection{Autoencoders}
%Autoencoders have many hyperparameters determining their capability for compression and subsequent reconstruction. These parameters include : \textit{depth}, \textit{width of layers}, \textit{activation functions}, \textit{batch-size}, \textit{learning rate}, \textit{number of filters}, \textit{stride width}, \textit{kernel size}. Their finding is discussed in this section, for both $\hy$ and $\rare$.\\
%For the fully connected autoencoder the order of determining the hyperparameters is as follows:
%Depth $\rightarrow$ Hidden Units $\rightarrow$ Batch Size $\rightarrow$  Activation Fuctions.
%For the convolutional autoencoder the order of determining the hyperparameters is as follows:
%Depth $\rightarrow$ Channels $\rightarrow$ Batch Size $\rightarrow$ Activation Functions.
%Both models are evaluated on the course of the training. Figures showing the training process for all experiments are provided in the appendix. After training the L2-Error\cref{labellist} is evaluated on the unshuffled, complete dataset as described in \cref{Sec: Data Sampling}.\\
%Important to mention is that the difficulty of finding the right set of hyperparameters for MLPs led to an intensive search prior to the creation of this contribution. The difficulty lies in the fact that there is little systematic knowledge about how the hyperparameters interact in the model and answer to a variety of input data. Goodfellow et al. point out that with a combination of intuition, certain methods and first of all experience practitioners find hyperparameters that work well \cite{Goodfellow}. As for this thesis a list of the prior search is provided in \cref{Sec:AppendixA}. This list does not claim to cover the complete search and can by no means give enough insight to enable reproducability by another person.\\
%For this reason an insight into the methods employed to find the set of hyperparameters used in this contribution are described in the following. 
%
%The convolutional autoencoder architecture 1.1 is a composition of six convolutional and three fully conected layers, \cref{f_Conv}.
%\begin{equation}
%y_p = f_{C}^9(f_{C}^8(f_{C}^7(f_{F}^6(f_{F}^5(f_{F}^4(f_{C}^3(f_{C}^2(f_{C}^1(y_0)))))))))
%\label{f_Conv}
%\end{equation}