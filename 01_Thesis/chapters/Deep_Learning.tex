% !TEX root = master.tex

\chapter{Dimensionality reduction algorithms}
\label{Ch:DimRedAl}
%\pagenumbering{arabic}

This chapter introduces the dimensionality reduction algorithms, which will be applied to solutions of the BGK model in Sod's shock tube: Proper orthogonal decomposition (POD) and Autoencoders (AEs). Firstly, a short introduction of POD is given. Secondly, autoencoders and significant aspects of deep learning are described. The main focus of this thesis lies in the application of autoencoders for which POD serves as a comparative method.\\

Dimensionality reduction algorithms lie at the heart of any reduced order model (ROM) which is described in the following chapter. As input serve datasets such as solutions of a full order model (FOM) or experimental data. These datasets may contain the dynamics of a spatio-temporal problem. The output is an approximation of the input. It is reconstructed from a low-dimensional representation that captures the underlying dynamics of the input problem.  
\section{Proper orthogonal decomposition (POD)}
\label{Sec: POD}
%POD
The solution of PDEs, precisely \(f(x,v,t)\), can be approximated either through a discretization into a system of ODEs as described in \cref{Ch:BGK}, or alternatively through a separation of variables ansatz
\begin{equation}
	f(t,v,x) = \sum_{i=1}^n a_i(t)\Phi_i(x,v)\mathrm{,}
\end{equation}  
as descried in \cite{Kutz}. Temporal dependence rendered through \(a_i(t)\) is independent from the spatial information carried in \(\Phi_i(x,v)\). Here \(\Phi_i(x,v)\) is called the \(i\)-th basis mode. With increasing \(i\), the accuracy of the solution consequently increases as well, which is similar to increasing the spatial resolution in finite difference methods outlined in \cref{Ch:BGK}. The essence of dimensionality reduction algorithms is to find optimal basis modes \(\Phi_i(x,v)\). Optimality specifically implies capturing the dynamic answer to a given geometry and initial conditions, thus permitting to exploit a minimal number \(p\) of basis modes to reconstruct the dynamics.\\
An optimal basis can be provided by POD. It leverages a "physically interpretable spatio-temporal decomposition"\cite{Kutz}[p.375] of the input data. At first, this data needs to be preprocessed in a fashion, that separates the temporal and spatial axis. Each temporal state is called snapshot and is stacked in a matrix \(\btch\) such that
\begin{equation}
	\btch=\left[ \mathbf{u}_{1},\mathbf{u}_2,\dots,\mathbf{u}_n \right] \qquad \mathrm{with}\qquad \mathbf{u}_i=\left[f(v_1,x_1),\dots,f(v_k,x_1),f(v_1,x_2),\dots,f(v_k,x_j)\right]^T \mathrm{,}
	\label{Eq:PiPOD}
\end{equation}
where \(n\) is the number of available snapshots and \(\mathbf{u}_i\) the \(i\)-th snapshot with \(\mathbf{u}_i \in \mathbb{R}^{j\times k}\). Afterward, \(\btch\) is decomposed using the singular value decomposition (SVD) which solves the left and right singular value problem leveraging
\begin{equation}
\btch = U\Sigma V^*\mathrm{,}
\end{equation}
where \(U\) is a unitary matrix containing the left singular vectors of \(\btch\) in its columns and \(V\), also a unitary matrix, containing the right singular vectors in its columns. Here, a superscript asterisk denotes the complex conjugate transpose. Furthermore, \(\Sigma\) is a sparse matrix with the singular values in descending order on its diagonal \cite{Kutz}. Note that the SVD always produces as many singular values and thus singular vectors as existing elements on the shortest axis of the input matrix. Hence, preprocessing the input data as described above delivers as many singular values -and vectors, as there are snapshots, given that the resolution in time is smaller than the spatial resolution.\\
Next, by applying the Eckard-Young theorem, which can be looked up in \cite{Kutz}, it is possible to harness the first leading singular values and corresponding vectors to approximate \(\btch\) to a desired accuracy. The theorem states that the optimal rank-r approximation to $\btch$, in a least-squares sense, is given by the rank-r SVD truncation \(\tilde{\btch}\):
\begin{equation}
\underset{\tilde{\btch}, s.t. rank(\tilde{\btch})=r}{\operatorname{argmin}} \| 
\btch -\tilde{\btch}\|_F
=\tilde{U}\tilde{\Sigma}\tilde{V}^*\mathrm{.}
\label{Eq:EckardYoung}
\end{equation} 
Here \(\tilde{U}\) and \(\tilde{V}\) denote the first \(r\) leading columns of \(\mathbf{U}\) and \(\mathbf{V}\), and \(\tilde{\Sigma}\) contains the leading \(r \times r\) sub-block of \(\Sigma\). \(\|\cdot\|_F\) is the Frobenius norm \cite{Kutz}.\\
When decomposing a matrix that contains snapshots of a dynamical system, the columns of \(\tilde{U}\) and \(\tilde{V}\) encompass dominant patterns that describe the dynamical system. Moreover, they provide "a hierarchical set of modes, that characterize the observed attractor on which we may project a low-dimensional dynamical system to obtain reduced order models"\cite{Kutz}[p.8]. That being said, we use the left singular values of \(\tilde{U}\) as optimal basis modes such that
\begin{equation}
	\tilde{U} = \Phi = \left[\Phi_1,\Phi_2,\dots,\Phi_r\right]\mathrm{.}
\end{equation}
Vectors in \(\Phi\) are orthogonal to each other thus supply a coordinate transformation from the high-dimensional input space into the low-dimensional pattern space.
\section{Autoencoders}
\label{Sec:AE}
%Autoencoder

An alternative to obtaining an optimal basis is to employ a machine learning architecture situated in the field of deep learning called autoencoders. An autoencoder is a feedforward neural network, that is trained to learn salient features of its input.  Hence, the input is compressed and successively reconstructed from the compressed version. Arguably, an autoencoder simply copies its input to its output \cite{Goodfellow}. \Cref{Fig:Autoencoder} shows a schematic representation of the architecture for a deep undercomplete autoencoder and the necessary terminology used to describe its components. For now, the difference between an undercomplete and overcomplete autoencoder is ignored, however, introduced when arriving at regularization.\\\
The distinction between encoder and decoder represents the biggest allocation units in autoencoders. They are separated at the central code layer. The encoder is a mapping \(h\). Thus, the encoder maps the input \(\btch\) to the code \(\Phi\) whilst compressing it, written as \(h(\btch)=\Phi\). Hence the encoder consists of the input layer, a variable number of hidden layers, and the code layer. This is the left side of the schematic depiction of an autoencoder in \cref{Fig:Autoencoder}. The decoder mirrors the encoder, which is not a necessity but often chosen that way, seen on the right side of \cref{Fig:Autoencoder}. It comprises the same code layer, a variable number of hidden layers, and the output layer. Similarily it is a mapping \(g\) which maps \(\Phi\) to \(\tilde{\btch}\), an approximation to the initial \(\btch\), and is written as \(g(\Phi)=\tilde{\btch}\). The number of hidden layers in the encoder and the decoder is the first of the so-called hyperparameters that can be tuned to improve the performance of the autoencoder. Note that autoencoders with more than one hidden layer, the code layer, are referred to as deep autoencoders. The reverse is termed shallow autoencoder. The second-largest allocation unit in autoencoders (and in general neural networks) are the layers. Each layer can be a vector, a matrix, or a three-dimensional tensor. For the time being, each layer is chosen to be vector-valued. Therefore, the input layer can be an input vector e.g. \(\mathbf{u_1}\subseteq \btch\). The vectors in the hidden layers are abstractions of the input. The term hidden stems from the fact, that one usually does not look at them as the interest mainly lies in the code -and output layer. The code layer is of main interest, as it holds the compressed abstraction of the input. Coming to the smallest allocation unit, the nodes. Each node refers to an entry in the vector/layer and is displayed as a circle in \cref{Fig:Autoencoder}. The number of nodes per hidden layer is a second hyperparameter. Two nodes are connected in a forward pass through solving \(O=I*w+b\). This is represented as a computational graph in \cref{Fig:Autoencoder}. Here the input is \(I\) is multiplied with a weight \(w\), and by adding a bias \(b\), one obtains the output \(O\). Hence every connection between nodes contains two free parameters: a weight \(w\) and a bias \(b\). Hence the whole network holds a set of parameters which we call \(\frepar \in \mathbb{R}\) in the following. A forward pass in this sense refers to the flow of information from the left side of the encoder to the right side of the decoder in \cref{Fig:Autoencoder}. The aforementioned is provided in \cite{Goodfellow}.\\
\begin{figure}
	\centering
	\input{Figures/Chapter_3/AE_scheme.tex}
	\caption{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. An input layer, an output layer, the code or bottleneck layer, and two hidden layers. Every layer is made up of nodes, represented as circles. More hidden layers deepen the architecture of the autoencoder thus increasing the capacity of the model. Not shown are possible activations for each layer. Labeled are decoder and encoder. The decoder is a mapping \(g(\Phi)=\tilde{\btch}\), taking \(\Phi\) as an input, outputting an approximation \(\tilde{\btch}\) of \(\btch\). Similarly, the encoder is a mapping \(h(\btch)=\Phi\), taking \(\Phi\) as an input and outputting the code \(\Phi\). The box, left of the autoencoder, shows a computational graph for the feedforward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\), which yields the output \(O\), the input of a node right of the initial node.}
	\label{Fig:Autoencoder}
\end{figure}
\subsection{Training}
%Training

In a feedforward neural network the information flows forward from the input layer to the output layer evaluating  \(AE(\btch) = g(h(\btch)) =\tilde{\btch }\). When applied, the layer structure in \cref{Fig:Autoencoder} equates to a composition of functions
	\begin{equation}
		l^{(4)}(l^{(3)}(l^{(2)}(l^{(1)}(l^{(0)}(\btch))))) = g(h(\btch)) = AE(\btch) = \tilde{\btch}\mathrm{,}
		\label{Eq:Composition}
	\end{equation}
where every function represents one layer. Here \(l_{(0)}\) and \(l_{(4)}\) are input layer and output layer, \(l_{(1)}\) and \(l_{(3)}\) the hidden layers in the encoder and the decoder respectively with \(l_{(2)}\) the bottleneck layer. The evaluation of one layer or function is a linear transformation of the incoming data with
	\begin{equation}
	\tilde{\mathbf{u}}_1 = \mathbf{u}_1W + \mathbf{b}\mathrm{.} \label{Eq:Linear Transformation}
	\end{equation}
Here \(\mathbf{u}_1\) can be chosen as in \cref{Eq:PiPOD}, \(W\) is the weight matrix and \(\mathbf{b}\) a bias vector, which is the same equation as shown in the computational graph in \cref{Fig:Autoencoder}. The weight matrices and bias vectors of each layer are aggregated in the set \(\frepar \in \mathbb{R}\). \(\frepar\) does not minimize the cost function \(J(\frepar)\) from the start with
\begin{equation}
	J(\frepar) := \frac{1}{n}\sum_{i=1}^n (\mathbf{u}_i - AE(\frepar;\mathbf{u}_i))^2\mathrm{,}
	\label{Eq:Cost}
\end{equation}
and needs to be optimized through a learning process, which is called training. Note that the cost function \(J(\theta)\) comprises the mean squared error over all snapshots \(\mathbf{u}_i\),  written as \(L(\btch,\tilde{\btch})\) the so-called loss with
\begin{equation}
	L(\btch,\tilde{\btch}) := \frac{1}{n}\sum_{i=1}^n(\mathbf{u}_i - \tilde{\mathbf{u}}_i)^2 = E \mathrm{,}
\end{equation}
when specifically referring to \(E\) the distance between \(\btch\) and $\tilde{\btch}$. In order to minimize \(J(\frepar)\), the gradient of \(J(\frepar)\) with respect to the free parameters \(\frepar\) written as \(\nabla_{\frepar}J\) is required. In particular, the derivative of the network with respect to the free parameters as seen in \cref{Eq:Cost}. The opposite direction of the gradient yields an update of \(\frepar\). The so-called learning rate \(\epsilon\) determines the magnitude of the updates with
\begin{equation}
	\frepar \leftarrow \frepar - \epsilon\grd\mathrm{,} \qquad\mathrm{where}\qquad \grd:= \nabla_{\frepar}J\mathrm{.}
	\label{Eq: update}
\end{equation}
\begin{wrapfigure}{r}{.2\textwidth}
	\centering
	\vspace{-10pt}
	\input{Figures/Chapter_3/Sing_expample.tex}
	\vspace{-5pt}
	\caption{\footnotesize A single node network with one hidden node between input and output node.}
	\label{Fig:Sing_example}
\end{wrapfigure}
\textbf{Backpropagation:} The computation of the gradient \(\grd\) is performed with the backpropagation algorithm. As the name implies, the information flows backward through the network, from output layer to input layer, and propagates \(E\) backward through the network. Responsible for this backward motion is the recursive application of the chain rule of calculus. Considering as a simple illustrative example \cref{Fig:Sing_example} with \(y=g(z,b)=g(f(x,a),b)\), a single node connection from input \(x\) to output \(y\) with a single hidden node. Then \(a\) is a weighting constant of the first node and \(b\) defines a weighting constant of the hidden node. Backpropagating the distance \(E\) with \(E = \frac{1}{2}(y_0 -y)^2\) between the output and the ground truth \(y_0\) though the network yields
\begin{align}
	\frac{\partial E}{\partial a} &= -(y_0 - y)\frac{\partial y}{\partial z}\frac{\partial z}{\partial a}\qquad \mathrm{and}\\
	\frac{\partial E}{\partial b} &= -(y_0 - y)\frac{\partial y}{\partial b}\mathrm{,}
\end{align}
which requires \(\frac{\partial E}{\partial a} = -(y_0 - y)\frac{\partial y}{\partial z}\frac{\partial z}{\partial a} = 0\) to minimize \(E\). The update rule is then given by
\begin{equation}
	a_{i+1} = a_i + \epsilon \frac{\partial E}{\partial a_i} \qquad\mathrm{and}\qquad
	b_{i+1} = b_i + \epsilon \frac{\partial E}{\partial b_i}\mathrm{.}
\end{equation}
This example is taken from \cite{Kutz} and shows how the chain rule of calculus provides the backward direction when deriving the gradients of each layer with respect to the free parameters.\\

\textbf{Generalization:}
The objective for training a neural network is called generalization. The difference between pure optimization and learning lies in the differing objectives. Pure optimization minimizes a cost function like \(J(\frepar)\)  in order to fit \(\btch\) to \(\tilde{\btch}\). That is the objective of the optimization task. Learning uses the same objective as the optimization task but equally cares about the so-called generalization task. Generalization is the ability of the learning algorithm to not only fit the input data but at the same time to fit different data from the same source. Thus, using salient features of the input data enables generalization, which can only be achieved to a certain extent and is sometimes intractable. But how is the generalizing performance measured? The input data \(\btch\) is randomly shuffled and split it into a training and a validation set in a 80/20 fashion with \(\btch=\{\btch_{train},\btch_{val}\}\). The random shuffling eliminates any bias on both sets. Then \(J(\frepar)\) is minimized using only \(\btch_{train}\) which minimizes \(L(\btch_{train},\tilde{\btch}_{train})\) and it is checked if indirectly \(L(\btch_{val},\tilde{\btch}_{val})\) is being minimized as well. The indirect minimization of \(L(\btch_{val},\tilde{\btch}_{val})\) is called the generalization task. The loss over the validation set is called the validation error.  The loss over the training set is called the training error.  Both are equally valued. As previously stated, an optimal basis \(\base\) that specifically contains salient features of \(\btch\) is sought for. By minimizing only the optimization task, the same basis as in POD is acquired. With learning, it is tried to find an even more powerful basis \(\base\) that enables the network to generalize.\\

\textbf{Initialization:}
Initial values of \(\frepar\) are crucial for the success of any neural network to learn something useful in a reasonable time since a strong bias is introduced to the network, considering that Adam only makes marginal contributions to \(\frepar\) at every update. Usually, the biases have a heuristically constant value. The weights are required to break the symmetry between nodes. For example, if two hidden nodes with the same activation function are connected to the same input, then the same gradient would lead to symmetric evolution during training. Thus, making one of the nodes redundant. Therefore weights are initialized randomly drawn from a Gaussian distribution. The spectrum of initial weight sizes ranges from large initial weights to small initial weights and sparse initialization. If the scale of the distribution yields large weights, then all nodes are characteristically connected and symmetry is broken. Contrarily, small initial weights give less connectivity. Thus, the learning algorithm can connect nodes and choose the strength \cite{Goodfellow}.\\
Several heuristics for the scale of the initial weights exists. Each preserves the norm of the weight matrix of each layer to stay close to or below unity. Envisage that during forward propagation and backward propagation matrix multiplications from one layer to the next are performed. Keeping the norm of each layer from exceeding unity prevents the development of exploding values and gradients in the respective last layer. A preventive measure is to scale the distribution for one layer with respect to its non-linear activation with a factor called gain. Gains for non-linear activations can be found in \cite{torch_ini} along with several sampling methods. The effects described above become severe with large layers or equally with networks of a certain depth. This is not the case for the autoencoders used in this thesis allowing comfortably to choose the default initialization in \texttt{pytorch} for linear -and convolutional layers, where the initial weights are drawn from a normal distribution with \(U(-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}})\) as suggested in \cite{LeCun98}. The number of input nodes, also called fan-in, per layer, is \(m\).\\
More details are available in \cite{Goodfellow}, and initialization for deep networks with more than eight layers and rectifying non-linear activations see \cite{he2015}.\\

\textbf{Adam:}
The algorithm to compute the updates to \(\frepar\) is called Adam, introduced by Kingma(2017) \cite{kingma2017adam}. Adam is an upgraded version of the classic stochastic gradient descent algorithm (SGD) with moments. The name stems from adaptive moment estimation, which refers to the adaptation of moments which in combination with the learning rate \(\epsilon\) applies scaled, directional updates to \(\frepar\) during training. The steps of Adam are shown in \cref{Alg:Adam}. Initial hyperparameters are the step size/learning rate \(\epsilon\), the exponential decay rates \(d_1\) and \(d_2\) in \([0,1)\) and a small constant \(\delta\) for numerical stability. The default values are \(\epsilon=1e-2\), \(d_1=0.9\), \(d_2=0.999\) and \(\delta=1e-8\).  Note, despite the learning rate \(\epsilon\), Adam is fairly robust w.r.t. changing it's hyperparameters. Hence solely the hyperparameter \(\epsilon\) requires tuning \cite{Goodfellow}.
\begin{algorithm}[hbp!]
	\begin{algorithmic}
		\SetAlgoLined
		\REQUIRE Step size $\epsilon$
		\REQUIRE Exponential decay rates from moment estimates $d_1$ and $d_2$ in \([0,1)\)\\
		(Suggested defaults: 0.9 and 0.999 respectively)
		\REQUIRE Small constant $\delta$ used for numerial stabilization (Suggested default: $10^{-8}$)
		\REQUIRE Initial parameters $\frepar$\\
		Initialize 1st and 2nd moment variables \(\mathbf{s}=0\) and \(\mathbf{r}=0\)\;
		Initialize time step i = 0\;
		\While{stopping criterion not met}{
			Sample a minibatch $\minib$ containing  \(m\) examples from the training set \(\btch_{train}\)\;
			Compute gradient: \(\grd\ \leftarrow \frac{1}{m} \nabla_{\frepar} \sum_m L(\minib - AE(\frepar;\minib)\)\;
			\(t\leftarrow t + 1\)\;
			Update biased first moment estimate: $\mathbf{s} \leftarrow d_1\mathbf{s} + (1-d_1)\grd$\;
			Update biased second moment estimate: $\mathbf{r} \leftarrow d_2\mathbf{r} + (1-d_2) \grd\circ\grd$\;
			Correct bias in first moment: $\hat{\mathbf{s}} \leftarrow \frac{\mathbf{s}}{1-d_1^t}$\;
			Correct bias in second moment: $\hat{\mathbf{r}} \leftarrow \frac{\mathbf{r}}{1-d_2^t}$\;
			Compute update: $\Delta\frepar = -\epsilon \frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}}+\delta}$ (operations applied element-wise)\;
			Apply update: $\frepar \leftarrow \frepar + \Delta\frepar$
		}
	\end{algorithmic}
	\caption{The Adam algorithm}
	\label{Alg:Adam}
\end{algorithm} 
SGD derives from the common gradient descent method, where the whole batch \(\btch_{train}\) is used per iteration to compute the gradient for an update of \(\frepar\). Therefore gradient descent is a batch gradient method or deterministic gradient method. SGD is a stochastic method. It approaches updates for \(\frepar\) as in \cref{Eq: update} in a stochastic manner, using only small portions of \(\btch_{train}\) per update. Randomly sampling only a small number of examples from \(\btch_{train}\) and taking the average gradient, computes an unbiased estimate of the true gradient, which is cheaper. Additionally, SGD can find a minimum even before processing the whole batch, important especially for large datasets \cite{Goodfellow}. Unfortunately, this leads to noisy updates of \(\frepar\). Thus, SGD trains with mini-batches of size \(\mbs\), delivering \(\btch_{train} = \{\btch_{t_i},\dots,\btch_{t_n}\}\). Here \(i\) counts through all the mini-batches, \(n\) is the total number of mini-batches. The mini-batch size \(\mbs\) constitutes yet another hyperparameter. Minibatch sizes \(\mbs\) touch the following observations: One minibatch can be processed in parallel to compute an update. Hence, small \(\mbs\) underutilize multicore architectures. Large \(\mbs\), on the other hand, can be limited by available memory. Furthermore, small \(\mbs\) can have a regularizing effect, which improves generalization. But they amplify the noisy behavior. Therefore, noisy updates require a small learning rate or equally one that decays over time. Besides, \(\mbs\) of the power of two is advised for graphics processing units (GPUs) to fully exhaust the hardware (array sizes of the power of two achieve better runtime in GPUs)\cite{Goodfellow}. Up to now, small mini-batches improve generalization, underutilize available multicore hardware and amplify noisiness which needs to be tackled with small learning rates. Hence, the learning process is slow.  Momentum can stabilize training besides reducing the learning rate.\\
Momentum applies an exponentially decaying moving average of previous gradients to the current gradient. It is an analogy to momentum in physics and therefore named after. Goodfellow(2016) consults the following examples to explain momentum. A tightrope walker uses the moment of inertia of a long rod for balance as crossing the rope. Oscillations of the walker need to overcome the moment of inertia of the long rod to destabilize the walker.  Equally, a fast car taking a sharp turn will drift because of momentum. The new direction is influenced by the old orthogonal momentum. The update rule for the first moment estimate \(\mathbf{s}\) of Adam in \cref{Alg:Adam} gives for three consecutive updates
\begin{align}
	\mathbf{s}_i &= d_1\mathbf{s}_{i-1}+(1-d_1)\mathbf{g}_{i}\mathrm{,} \label{Eq: fm}\\
	\mathbf{s}_{i-1}&=d_1\mathbf{s}_{i-2}+(1-d_1)\mathbf{g}_{i-2}\mathrm{,}\\
	\mathbf{s}_{i-2}&=d_1\mathbf{s}_{i-3}+(1-d_1)\mathbf{g}_{i-3}\mathrm{.}
\end{align}
A combination and simplification gives
\begin{equation}
	\mathbf{s}_i = d_1d_1(1-d_1)\mathbf{g}_{i-2}+\dots+d_1(1-d_1)\mathbf{g}_{i-1}+\dots+(1-d_1)\mathbf{g}\mathrm{.}
\end{equation}
Past gradients contribute to the current gradient, just like in the car example or the tightrope walker. Moreover, these contributions vanish exponentially over the iterations. This leads to a moving average of past and current gradients, which is called momentum. Note, the entanglement of updates is taken from \cite{bushaev_2017}. 

Now taking into account that if \cref{Eq: fm} computes the moment of the first iteration, there are no previous gradients available which leads to a biased first computation. Hence \(\mathbf{s}_i\) is divided by \((1-d_1^i)\) for the bias correction of the first moment in \cref{Alg:Adam} which yields \(\hat{\mathbf{s}}\). In consecutive iterations, the bias correction approaches zero. The same steps apply to the second biased moment estimate \(\mathbf{r}\). It is nothing more than the squared past gradients written as \(\grd \circ\grd\) in \cref{Alg:Adam}. Together they form the parameter update \(\Delta\frepar\). Without going into further details, it is shown in  \cite{kingma2017adam} that the parameter updates have an approximate upper bound of  \(\Delta\frepar\lessapprox \epsilon\), see \cite{kingma2017adam}. This upper bound is a necessary criterion for the updates, as the learning rate \(\epsilon\) is a specifically tuned hyperparameter that should not be exceeded a threshold during training. The ratio  \(\frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}}+\delta}\) is called signal to noise ratio (SNR) in \cite{kingma2017adam}, which evolves towards zero as an optimum is reached. Furthermore, the updates are invariant to the scale of the gradients. The factor \(a\) cancels out with \((a\cdot\hat{\mathbf{s}})/\sqrt{(a^2\cdot\hat{\mathbf{r}})}=\hat{\mathbf{s}}/\sqrt{\hat{\mathbf{r}}}\). To sum up, using first and second momentum accelerates training because the moving average gradient smooths the noisy updates. Thus, giving a better estimate of the true gradient. The bias is corrected in the first iteration and invariance to gradient scaling is provided.\\       
\begin{figure}[htbp!]
	\centering
	\input{Figures/Chapter_3/Training.tex}
	\caption{Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch\) for training is split up into a training \(\btch_{train}\) and a validation \(\btch_{val}\) set with a \(80/20\) ratio (a). The \(\btch_{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite{LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib_{i})=\tilde{\minib}_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib_{i}\) and \(\tilde{\minib}_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd\). This gradient is then used to optimize weights \(w\) and biases \(b\) with  \(Adam\), an optimization algorithm seen in \cref{Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar\) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch_{val}\), which it has not seen before, in (h) with \(AE(\btch_{val})=\tilde{\btch}_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch_{val}\) and \(\tilde{\btch}_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline{E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.}
	\label{Fig:Training}
\end{figure}\noindent
The flowchart in \cref{Fig:Training} shows how the generalization query, weight initialization, backpropagation, and the integration of Adam into the training procedure within this thesis. The training encompasses two loops. The outer loop runs over so-called epochs. The inner over the mini-batches. One epoch completes after the network saw all mini-batches.  Finally, the validation error and the training error are calculated. The number of epochs \(H\) is determined based on over- and underfitting and how many epochs Adam needs to converge, which is yet another hyperparameter.\\
\begin{figure}[htbp!]
	\centering
	\input{Figures/Chapter_3/Capacity.tex}
	\caption{This is a figurative example of capacity influencing the evolution of training and validation error. Increasing the capacity enables the model to fit the training and validation set thus both error decreases. Typically the training error is smaller than the validation error. Yet both errors are too high. The model underfits. A further increase of capacity equally increases the validation error with decreasing the training error further. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the training error, the model is overfitting. The capacity passed the point of optimality. Optimal capacity is the point where both optimization and generalization are in balance.}
	\label{Fig:Capacity}
\end{figure}\noindent

\textbf{Capacity, regularization, over- and underfitting:} To satisfy the objective of learning an optimal basis in the code layer, autoencoders need to be regularized. Regularization is defined as "any modification made to a learning algorithm that is intended to reduce its generalization error but not its training error"\cite{Goodfellow}[p.117]. As previously mentioned, autoencoders have a central hidden layer between the equally sized input -and output layer. In undercomplete autoencoders, the central hidden layer is called the bottleneck layer, because its size is smaller than the input. Thus the undercomplete autoencoder is forced to decide which information to copy and by that measure is regularized. Overcomplete autoencoders on the other hand have the central hidden layer greater or of the same size as the input, thus making it indispensable to use additional regularization, which, necessary to add, can be also fruitful in undercomplete autoencoders. Note that an undercomplete autoencoder as shown in \cref{Fig:Autoencoder} is used in this thesis which is called autoencoder for simplicity. Regularization is always advantageous when the capacity of an autoencoder is too big. The capacity of any neural network can be altered through the number of free parameters available for the model to fit the input data. Recall that an autoencoder has a variable number of hidden layers (excluding the bottleneck layer) of variable size and thus a variable number of free parameters. For example having a model big enough in terms of free parameters to memorize the whole training set, good results would be achieved for the optimization task, but missed out on learning useful features thus failing at the generalization task. This relationship is shown in \cref{Fig:Capacity} where a point of optimal capacity is defined. Optimal capacity is reached when optimization and generalization in the form of training error and validation error are in balance. Note that \cref{Fig:Capacity} is taken from \cite{Goodfellow}[p.112]. Another way that alters the capacity of a neural network is to add non-linear activation functions to layers. So far every node and by that layers are connected through linear transformations as in \cref{Eq:Linear Transformation}. Thus the network's so-called hypothesis space solely includes linear transformations of the input. The hypothesis space from which a neural network can choose the composition of the solution can be enriched with non-linear functions by simply evaluating layers through non-linear functions \(\nla\), also called activations, leading to
\begin{equation}
	\tilde{\mathbf{u}}_1 = \nla(\mathbf{u}_1W + \mathbf{b})\mathrm{.} \label{Eq:Non-Linear Transformation}
\end{equation}\noindent
\textbf{Activations:} Non-linear activations typically used for neural networks are summarized in \cref{Tab:Non-lin-func}. In this thesis, these are also the activations used in the final network design or during hyperparameter search. Two classes of activations are used. One is of type rectifier, which is linear for positive inputs and is zero, or decay below linear for negative inputs. The other is the hyperbolic tangent  (Tanh) and the Sigmoid-weighted linear unit (SiLU). Tanh and sigmoid share a similar "S"-shaped curve hence are put in one class. SiLU can be written as \(\textrm{SiLU}=\frac{1}{2}x(1+\tanh(\frac{x}{2}))\). Note that Tanh saturates for input values exceeding the range \([-2,2]\) which leads to a so-called vanishing gradient. In the most severe case, all updates can be zero. However, in this thesis, a vanishing gradient was not encountered. Theoretically, it is possible to take any function as an activation. Though indispensable is their differentiability, which reduces to partly differentiable for rectifiers, to allow backpropagation. Activations \(\nla\) and to place them in the network is yet another hyperparameter.
\begin{table}[htp]
	\centering
	\caption{Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha\) is typically, and also in this thesis set to \(\alpha=0.001\). Activations based on the hyperbolic tangent (Tanh) are the sigmoid function with \(\mathrm{sig}(x)= \frac{1}{2}(1+\tanh(\frac{x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.}
	\begin{tabular*}{16cm}{ @{\extracolsep{-4pt}} c c c c c @{} }
		\toprule
		ReLU & LeakyReLU & ELU & SiLU & Tanh \\   
		\midrule
		\small$\nla=\begin{cases}
			x,& \text{if } x\geq 0\\
			0,& \text{if } x<0
		\end{cases}$& 
		\small$\nla=\begin{cases}
		x,& \text{if } x\geq 0\\
		\alpha x,& \text{if } x<0
		\end{cases}$& 
		\small$\nla=\begin{cases}
		x,& \text{if } x>0\\
		\alpha (e^x-1),& \text{if } x\leq0
		\end{cases}$&
		\small$\nla=x \mathrm{sig}(x)$&
		\small$\nla=\tanh(x)$ \\
		\input{Figures/Chapter_3/ReLU.tex}& 		\input{Figures/Chapter_3/LeakyReLU.tex}&  \input{Figures/Chapter_3/ELU.tex} &
		\input{Figures/Chapter_3/SiLU.tex}&
		\input{Figures/Chapter_3/Tanh.tex}\\
		\bottomrule
	\end{tabular*} \label{Tab:Non-lin-func}
\end{table}
\\
\textbf{Layer types:} The idea that, for two successive layers, all nodes from the first layer are connected to all nodes from the following layer and that all layers are vectors, has been adopted. Hence the types of layers are called fully connected or \texttt{Linear} in \texttt{pytorch}\cite{torch_ini}. If now the input is a vector, a matrix, or a three-dimensional tensor another type of layer called convolutional layer has to be used. In \texttt{pytorch} these layers are called \texttt{Conv1D}, \texttt{Conv2D} and \texttt{Conv3D} respectively.  
To begin with, a simple example is given to illustrate conceptual features of convolutional layers. A digital image can be viewed as a distribution of pixels over a two-dimensional surface, where the dimensions of the surface are described with the width and height of the image. Furthermore, each pixel has a color expressed in values of the primary colors red, green, and blue (RGB). Hence each pixel is a combination of different RGB values. The RGB values for each pixel are stored in so-called channels. The red channel, green channel, and blue channel. This concludes, that the dimension of a digital image is additionally equipped with the channel dimension. Let an image be \(Img\), then  \(Img \in \mathbb{R}^{m\times n\times c}\), with \(m\) the width of this image, \(n\) the height of this image and \(c\) the three RGB channels. Another important characteristic of images is the spatial correlation between neighboring pixels. This can be for example sharp corners that separate foreground from background or differentiate between entities portrayed in the image. Therefore similarly to saving information about color composition in channels, we can imagine saving spatial information in additional channels. For example corners and other geometric shapes,  saved in channels for themselves. With this idea in mind, convolutional layers can be introduced. Convolutional layers comprise a so-called kernel, which moves over the image and by that all the input channels. Therefore, if the kernel is $K$ then $K \in \mathbb{R}^{i\times j\times c}$. The height $i$ and width $j$ of the kernel can be adjusted and is usually smaller than the input's height and width. The $c$ dimension is always equal to the $c$ dimension of the input. The distance the kernel travels over the input at every step is specified with the so-called stride $s$. For a two dimensional input image like $Img$ the strides are given in direction $s_1$  and $s_2$, where the latter corresponds to $m$ and the former corresponds to $n$ of the $Img$. The kernel performs the same linear transformations of the input as fully connected layers at every part of the input visited. This action outputs a so-called feature map, which as the name implies should capture features of the input. A comparison of a one strided convolution with $s_1=s_2=1$ and a two strided convolution with $s_1=s_2=2$ is given in \cref{Fig: Downsampling}. The one strided convolution yields a feature map with a reduction of $m$ and $n$ by one, while the two strided convolution yields a feature map with a reduction of $m$ and $n$ by two. Note that in \cref{Fig: Downsampling} eventual channels are omitted.\\
Even though the name convolutional layer implies the use of the convolutional operation \texttt{PyTorch} and other neural network libraries instead use the cross-correlation operation, which is an operation closely related to the convolution \cite{Goodfellow} \cite{Pytorch website}. Details on why the cross correlation is used instead of the convolution are available in \cite{Goodfellow}. In \cref{Eq. Discrete Cross Correlation} the two dimensional discrete cross correlation without channels is given with
\begin{align}
M_{m,n} = \sum_{i,j}Img_{((m-1)\times s_1)+i,((n-1)\times s_2)+j}K_{i,j}\mathrm{.}
\label{Eq. Discrete Cross Correlation}
\end{align}
Here $M_{m,n}$ represents a point on a feature map, where $m$ and $n$ refer to the location of that point. The input to the cross correlation is exemplary the image $Img$, where coordinates of a pixel are also given in $m$ and $n$. The kernel is given as $K$ with coordinates in $i$ and $j$. The summation in \cref{Eq. Discrete Cross Correlation} is evaluated over $i$, $j$. For example, taking a point on $M$ at $m=1$ and $n=1$ with $s_1=1$ and $s_2=1$, then the summation is performed over all pixels of $Img$ that the kernel covers in $i$ and $j$ from the starting point $m=0$ and $n=0$ on $Img$. Further, by moving to $m=2$ as seen in \cref{Fig: Downsampling} (a), the new starting point on $Img$ is at $m=1$ and $n=0$. Hence, the kernel $K$ moved along the $m$ axis of the input image $Img$. Eventually the kernel will cover the whole input $Img$.
\begin{figure}[htbp!]
	\begin{subfigure}{.459\textwidth}
		\centering
		\input{Figures/Chapter_3/Stride_01.tex}
		\subcaption{Two dimensional example of a \textbf{one} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(3\times 3\) matrix, as the input is downsampled by a factor of \(0.75\).}
		\label{Fig: Downsampling(a)}
	\end{subfigure}\hfill
	\begin{subfigure}{.441\textwidth}
		\centering
		\input{Figures/Chapter_3/Stride_02.tex}
		\subcaption{Two dimensional example of of a \textbf{two} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(2\times 2\) matrix, as the input is downsampled by a factor of \(0.5\).}
		\label{Fig: Downsampling(b)}
	\end{subfigure}
	\caption{Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.}
	\label{Fig: Downsampling}
\end{figure}
\\
If many features are present in the input then more feature maps are required. Thus, more kernels can be specified to move over the input, which yields as many feature maps as there are kernels. In \cref{Fig: Kernel} an illustrative example of a convolutional neural network with four convolutional layers, which downsamples the width and height of the input in every layer while adding channels (feature maps) in every layer. When arriving at the last convolutional layer, the feature maps are flattened and fully connected to the code layer. A comparable scheme for the encoder side of a convolutional autoencoder is used in this thesis. The decoder is acquired by mirroring the encoder in \cref{Fig: Kernel} at the code layer and replacing all convolutional layers with transposed convolutional layers.  With encoder and decoder, the architecture arrives at the typical autoencoder architecture as in \cref{Fig:Autoencoder}. In conclusion, five additional hyperparameters can be identified for a single convolutional layer: kernel width and height, number of feature maps, usually called output channels, and stride in two directions.\\
\begin{figure}
	\centering
	\input{Figures/Chapter_3/Kernel.tex}
	\caption{\footnotesize Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.}
	\label{Fig: Kernel}
\end{figure}\noindent
Fully connected layers can down- and upsample making them applicable for the decoder and encoder in autoencoders. This applies also to convolutional layers, nevertheless not every downsampling can be reversed with convolutional layers. Hence the transposed convolutional layer is introduced. The respective modules for vectors, matrices and three dimensional tensors are called \texttt{ConvTranspose1d}, \texttt{ConvTranspose2d} and \texttt{ConvTranspose3d} in \texttt{pyTorch}. The transpose of a convolutional layer has nothing in coming with the transposition of a matrix. The transpose simply refers to recovering a specific shape. Note, that a transpose of a convolutional layer can be viewed as a convolutional layer in which the backward- and forward passes are swapped. Thus they are often implemented in that way \cite{Goodfellow} \cite{lane_2018} \cite{divyanshu_2020}. In \cref{Fig: Upsampling(a)} and \cref{Fig: Upsampling(b)} two simplified examples of two dimensional transposed convolutions without channels are sketched out. In both figures, with point $a$ of the input a linear transformation is performed with every kernel entry $k_1$ to $k_6$ resulting in the first part of the upsampled output located at the upper left corner. This first part is of the same size as the kernel. Linear transforming a successive point $b$ of the input with all six kernel entries gives the second part of the upsampled output. Again, the second part is of the same size as the kernel. The distance the two parts separate from each other on the output is determined by the stride. When the stride in one direction is shorter than the kernel in that direction, both parts overlap. Overlaps are added and therefore constitute bigger values. In \cref{Fig: Upsampling(a)} this process is shown for an $3 \times 3$ input matrix, which is upsampled by a $3\times 3$ kernel moving with stride length $s_1=s_2=2$. The upsampled result is of size $7 \times 7$. With a kernel size of 3 in one direction and a stride $s_1=2$ in the same direction, there is always an unevenly distributed overlap in the output entries resulting in the checkerboard structure as seen in \cref{Fig: Upsampling(a)}. This is a common problem with transposed convolutional layers. When the kernel width and height are divisible by the respective stride length $s_1$ and $s_2$ this issue can be mitigated, as the overlaps are evenly distributed. For example, when the stride lengths $s_1=s_2=3$ are of the same size as the kernel width and height no overlap occurs. This case is depicted in \cref{Fig: Upsampling(b)}.\\
Transposed convolutional layers have the same hyperparameters as convolutional layers. Their impact on the down- and the upsampling rate is non-negligible. But customizing both hyperparameters only that matter can be detrimental for learning off the input. For example, a kernel can be too small thus miss overkill edges in pictures. Similarly, a large step size that overpasses thin edges. Hence kernel size and stride width are somewhat determined by the structure of the input. Through zero-padding outlines of the input, down- and upsampling rates can be tuned independently from kernel size and stride width. Zero-padding is available for convolutional layers as well as for the transpose of convolutional layers. It refers to padding the outer edges of the input with zeros. Putting everything together, the output size of convolutional layers and transposed convolutional layers can be computed with
\vspace{-3.5ex}
\begin{multicols}{2}
	\noindent
	\begin{equation}
		o = \left[\frac{i+2p-k}{s}\right]+1  
	\end{equation}
	\begin{equation}
		\mathrm{and}\qquad o = (i-1)s - 2p + k
	\end{equation}
\end{multicols}
respectively. Here all variables are the sizes of one-dimension e.g. the width of an image. Hence, $o$ is the output, $i$ the input, $2p$ the padding of opposing edges, $k$ the kernel, and $s$ to the stride size. Besides, whenever the stride width exceeds the input size in a particular direction, then the input is automatically padded with the necessary number of zeros on the left or bottom. Note, nearly every transpose of a convolutional layer can be described by an associated convolutional layer for which the input is spread using zeros in between every point. Calculating output sizes of convolutional layers and transposed convolutional layers is available in \cite{dumoulin2018guide}.   
\begin{figure}[htbp!]
	\begin{subfigure}{.435\textwidth}
		\centering
		\input{Figures/Chapter_3/Transpose_even.tex}
		\subcaption{Mechanism of a two dimensional \textbf{two} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. Upsampled by a factor of \(2.33\) produces a \(7\times 7\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is not divisible by the stride width. Uneven overlapping produces a checkerboard-like structure.}
		\label{Fig: Upsampling(a)}
	\end{subfigure}\hfill
	\begin{subfigure}{.455\textwidth}
		\centering
		\input{Figures/Chapter_3/Transpose_uneven.tex}
		\subcaption{Mechanism of a two-dimensional \textbf{three} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. The resulting output upsampled by a factor of \(3\) is a \(9\times 9\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is identical to the stride width. Overlaps and checkerboard-like structures are avoided.}
		\label{Fig: Upsampling(b)}
	\end{subfigure}
	\caption{Visualization of the mechanism in transposed convolutional layers. Shown is pixelation, a common problem with transposed convolutional layers (a) and a possible solution, mitigating the pixelation effect.}
	\label{Fig: Upsampling}
\end{figure}\\
Neural networks have many hyperparameters determining their capability for learning which were introduced in this section. There is little systematic knowledge about how the hyperparameters interact in the model. Goodfellow(2016) points out that with a combination of intuition, specific methods, and first of all experience, practitioners find hyperparameters that work well \cite{Goodfellow}. As for this thesis, a discussion of the selection of hyperparameters for a fully-connected and  a convolutional autoencoder is given in \cref{Ch:ApA} and \cref{Ch:ApB} respectively. In the following, the fully-connected autoencoder is named FCNN and the convolutional autoencoder is named CNN.