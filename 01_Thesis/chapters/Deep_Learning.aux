\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Dimensionality reduction algorithms}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:DimRedAl}{{3}{8}{Dimensionality reduction algorithms}{chapter.3}{}}
\newlabel{Ch:DimRedAl@cref}{{[chapter][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Proper orthogonal decomposition (POD)}{8}{section.3.1}\protected@file@percent }
\newlabel{Sec: POD}{{3.1}{8}{Proper orthogonal decomposition (POD)}{section.3.1}{}}
\newlabel{Sec: POD@cref}{{[section][1][3]3.1}{[1][8][]8}}
\newlabel{Eq:PiPOD}{{3.2}{8}{Proper orthogonal decomposition (POD)}{equation.3.1.2}{}}
\newlabel{Eq:PiPOD@cref}{{[equation][2][3]3.2}{[1][8][]8}}
\citation{Kutz}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq:EckardYoung}{{3.4}{9}{Proper orthogonal decomposition (POD)}{equation.3.1.4}{}}
\newlabel{Eq:EckardYoung@cref}{{[equation][4][3]3.4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Autoencoders}{9}{section.3.2}\protected@file@percent }
\newlabel{Sec:AE}{{3.2}{9}{Autoencoders}{section.3.2}{}}
\newlabel{Sec:AE@cref}{{[section][2][3]3.2}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \footnotesize  Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde  {\btch  }\), taking \(\Phi \) as input and outputs an approximation \(\tilde  {\btch  }\) of \(\btch  \). Similarily the encoder is a mapping \(h(\btch  )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), input of a node right of the inital node.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{Fig:Autoencoder}{{3.1}{10}{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde {\btch }\), taking \(\Phi \) as input and outputs an approximation \(\tilde {\btch }\) of \(\btch \). Similarily the encoder is a mapping \(h(\btch )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), input of a node right of the inital node.\relax }{figure.caption.6}{}}
\newlabel{Fig:Autoencoder@cref}{{[figure][1][3]3.1}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training}{10}{subsection.3.2.1}\protected@file@percent }
\citation{Kutz}
\newlabel{Eq:Composition}{{3.6}{11}{Training}{equation.3.2.6}{}}
\newlabel{Eq:Composition@cref}{{[equation][6][3]3.6}{[1][10][]11}}
\newlabel{Eq. Linear Transformation}{{3.7}{11}{Training}{equation.3.2.7}{}}
\newlabel{Eq. Linear Transformation@cref}{{[equation][7][3]3.7}{[1][11][]11}}
\newlabel{Eq:Cost}{{3.8}{11}{Training}{equation.3.2.8}{}}
\newlabel{Eq:Cost@cref}{{[equation][8][3]3.8}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \footnotesize  A single node network with one hidden node between input and output node.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{Fig:Sing_example}{{3.2}{11}{\footnotesize A single node network with one hidden node between input and output node.\relax }{figure.caption.7}{}}
\newlabel{Fig:Sing_example@cref}{{[figure][2][3]3.2}{[1][11][]11}}
\citation{Goodfellow}
\citation{LeCun98}
\citation{he2015}
\citation{torch_ini}
\citation{LeCun98}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The Adam algorithm\relax }}{12}{algocf.1}\protected@file@percent }
\newlabel{Alg:Adam}{{1}{12}{Training}{algocf.1}{}}
\newlabel{Alg:Adam@cref}{{[algorithm][1][]1}{[1][11][]12}}
\citation{LeCun98}
\citation{LeCun98}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \footnotesize  Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch  \) for training is split up into a training \(\btch  _{train}\) and a validation \(\btch  _{val}\) set with a \(80/20\) ratio (a). The \(\btch  _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite  {LeCun98} (c). A successive evaluation of the first minibatch using Kaiming weights with \(AE(\minib  _{i})=\tilde  {\minib  }_{i}\) takes place in (d). The mean squared error between \(\minib  _{i}\) and \(\tilde  {\minib  }_{i}\) yields the error \(E_{i}\). Thereafter \(E_{i}\) is backpropagated through the network (f) yielding a gradient of the loss function \(L\). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref  {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(w\) and \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\tilde  {\btch  }_{val}\), which it has not seen before, in (h) with \(AE(\btch  _{val})=\tilde  {\btch  }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch  _{val}\) and \(\tilde  {\btch  }_{val}\), produces the error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{i}\) is taken. This produces \(E\) and concludes the first epoch. When the maximum number of epochs \(H\) is reached, \(E\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }}{14}{figure.caption.8}\protected@file@percent }
\newlabel{Fig:Training}{{3.3}{14}{\footnotesize Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch \) for training is split up into a training \(\btch _{train}\) and a validation \(\btch _{val}\) set with a \(80/20\) ratio (a). The \(\btch _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite {LeCun98} (c). A successive evaluation of the first minibatch using Kaiming weights with \(AE(\minib _{i})=\tilde {\minib }_{i}\) takes place in (d). The mean squared error between \(\minib _{i}\) and \(\tilde {\minib }_{i}\) yields the error \(E_{i}\). Thereafter \(E_{i}\) is backpropagated through the network (f) yielding a gradient of the loss function \(L\). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(w\) and \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\tilde {\btch }_{val}\), which it has not seen before, in (h) with \(AE(\btch _{val})=\tilde {\btch }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch _{val}\) and \(\tilde {\btch }_{val}\), produces the error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{i}\) is taken. This produces \(E\) and concludes the first epoch. When the maximum number of epochs \(H\) is reached, \(E\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }{figure.caption.8}{}}
\newlabel{Fig:Training@cref}{{[figure][3][3]3.3}{[1][13][]14}}
\citation{Hornik1989}
\citation{Goodfellow}
\citation{bibid}
\citation{bibid}
\citation{bibid}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Pytorch website}
\citation{dumoulin2018guide}
\citation{Goodfellow}
\newlabel{Eq. Convolution}{{3.15}{16}{Training}{equation.3.2.15}{}}
\newlabel{Eq. Convolution@cref}{{[equation][15][3]3.15}{[1][16][]16}}
\newlabel{Eq. Discrete Convolution}{{3.16}{16}{Training}{equation.3.2.16}{}}
\newlabel{Eq. Discrete Convolution@cref}{{[equation][16][3]3.16}{[1][16][]16}}
\newlabel{Eq. Discrete Convolution Flip}{{3.17}{16}{Training}{equation.3.2.17}{}}
\newlabel{Eq. Discrete Convolution Flip@cref}{{[equation][17][3]3.17}{[1][16][]16}}
\newlabel{Eq. Discrete Cross Correlation}{{3.18}{16}{Training}{equation.3.2.18}{}}
\newlabel{Eq. Discrete Cross Correlation@cref}{{[equation][18][3]3.18}{[1][16][]16}}
\newlabel{Eq:Downsampling}{{3.19}{16}{Training}{equation.3.2.19}{}}
\newlabel{Eq:Downsampling@cref}{{[equation][19][3]3.19}{[1][16][]16}}
\citation{bibid}
\citation{dumoulin2018guide}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq. Cross correlation stride channel}{{3.20}{17}{Training}{equation.3.2.20}{}}
\newlabel{Eq. Cross correlation stride channel@cref}{{[equation][20][3]3.20}{[1][17][]17}}
\newlabel{Eq: Transposed convolution}{{3.21}{17}{Training}{equation.3.2.21}{}}
\newlabel{Eq: Transposed convolution@cref}{{[equation][21][3]3.21}{[1][17][]17}}
\newlabel{Eq. Backpropagation3}{{3.22}{17}{Training}{equation.3.2.22}{}}
\newlabel{Eq. Backpropagation3@cref}{{[equation][22][3]3.22}{[1][17][]17}}
\newlabel{Eq. Backpropagation2}{{3.23}{17}{Training}{equation.3.2.23}{}}
\newlabel{Eq. Backpropagation2@cref}{{[equation][23][3]3.23}{[1][17][]17}}
\newlabel{Eq. Backpropagation1}{{3.24}{17}{Training}{equation.3.2.24}{}}
\newlabel{Eq. Backpropagation1@cref}{{[equation][24][3]3.24}{[1][17][]17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Autoencoders}{17}{subsection.3.2.2}\protected@file@percent }
\newlabel{f_Conv}{{3.25}{18}{Autoencoders}{equation.3.2.25}{}}
\newlabel{f_Conv@cref}{{[equation][25][3]3.25}{[1][18][]18}}
\@setckpt{chapters/Deep_Learning}{
\setcounter{page}{19}
\setcounter{equation}{25}
\setcounter{enumi}{13}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{Item}{13}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{13}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{1}
\setcounter{algocfproc}{1}
\setcounter{algocf}{1}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{section@level}{0}
}
