\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Dimensionality reduction algorithms}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:DimRedAl}{{3}{8}{Dimensionality reduction algorithms}{chapter.3}{}}
\newlabel{Ch:DimRedAl@cref}{{[chapter][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Proper orthogonal decomposition (POD)}{8}{section.3.1}\protected@file@percent }
\newlabel{Sec: POD}{{3.1}{8}{Proper orthogonal decomposition (POD)}{section.3.1}{}}
\newlabel{Sec: POD@cref}{{[section][1][3]3.1}{[1][8][]8}}
\newlabel{Eq:PiPOD}{{3.2}{8}{Proper orthogonal decomposition (POD)}{equation.3.1.2}{}}
\newlabel{Eq:PiPOD@cref}{{[equation][2][3]3.2}{[1][8][]8}}
\citation{Kutz}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq:EckardYoung}{{3.4}{9}{Proper orthogonal decomposition (POD)}{equation.3.1.4}{}}
\newlabel{Eq:EckardYoung@cref}{{[equation][4][3]3.4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Autoencoders}{9}{section.3.2}\protected@file@percent }
\newlabel{Sec:AE}{{3.2}{9}{Autoencoders}{section.3.2}{}}
\newlabel{Sec:AE@cref}{{[section][2][3]3.2}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \footnotesize  Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde  {\btch  }\), taking \(\Phi \) as input and outputs an approximation \(\tilde  {\btch  }\) of \(\btch  \). Similarily the encoder is a mapping \(h(\btch  )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), input of a node right of the inital node.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{Fig:Autoencoder}{{3.1}{10}{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde {\btch }\), taking \(\Phi \) as input and outputs an approximation \(\tilde {\btch }\) of \(\btch \). Similarily the encoder is a mapping \(h(\btch )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), input of a node right of the inital node.\relax }{figure.caption.6}{}}
\newlabel{Fig:Autoencoder@cref}{{[figure][1][3]3.1}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training}{11}{subsection.3.2.1}\protected@file@percent }
\newlabel{Eq:Composition}{{3.6}{11}{Training}{equation.3.2.6}{}}
\newlabel{Eq:Composition@cref}{{[equation][6][3]3.6}{[1][11][]11}}
\newlabel{Eq:Linear Transformation}{{3.7}{11}{Training}{equation.3.2.7}{}}
\newlabel{Eq:Linear Transformation@cref}{{[equation][7][3]3.7}{[1][11][]11}}
\newlabel{Eq:Cost}{{3.8}{11}{Training}{equation.3.2.8}{}}
\newlabel{Eq:Cost@cref}{{[equation][8][3]3.8}{[1][11][]11}}
\newlabel{Eq: update}{{3.10}{11}{Training}{equation.3.2.10}{}}
\newlabel{Eq: update@cref}{{[equation][10][3]3.10}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \footnotesize  A single node network with one hidden node between input and output node.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{Fig:Sing_example}{{3.2}{11}{\footnotesize A single node network with one hidden node between input and output node.\relax }{figure.caption.7}{}}
\newlabel{Fig:Sing_example@cref}{{[figure][2][3]3.2}{[1][11][]11}}
\citation{Kutz}
\citation{Goodfellow}
\citation{LeCun98}
\citation{he2015}
\citation{torch_ini}
\citation{LeCun98}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The Adam algorithm\relax }}{13}{algocf.1}\protected@file@percent }
\newlabel{Alg:Adam}{{1}{13}{Training}{algocf.1}{}}
\newlabel{Alg:Adam@cref}{{[algorithm][1][]1}{[1][12][]13}}
\citation{bushaev_2017}
\citation{kingma2017adam}
\citation{kingma2017adam}
\citation{LeCun98}
\citation{LeCun98}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq: fm}{{3.14}{14}{Training}{equation.3.2.14}{}}
\newlabel{Eq: fm@cref}{{[equation][14][3]3.14}{[1][14][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \footnotesize  Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch  \) for training is split up into a training \(\btch  _{train}\) and a validation \(\btch  _{val}\) set with a \(80/20\) ratio (a). The \(\btch  _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite  {LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib  _{i})=\tilde  {\minib  }_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib  _{i}\) and \(\tilde  {\minib  }_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd  \). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref  {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar  \) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch  _{val}\), which it has not seen before, in (h) with \(AE(\btch  _{val})=\tilde  {\btch  }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch  _{val}\) and \(\tilde  {\btch  }_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline  {E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{Fig:Training}{{3.3}{16}{\footnotesize Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch \) for training is split up into a training \(\btch _{train}\) and a validation \(\btch _{val}\) set with a \(80/20\) ratio (a). The \(\btch _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite {LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib _{i})=\tilde {\minib }_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib _{i}\) and \(\tilde {\minib }_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd \). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar \) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch _{val}\), which it has not seen before, in (h) with \(AE(\btch _{val})=\tilde {\btch }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch _{val}\) and \(\tilde {\btch }_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline {E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }{figure.caption.9}{}}
\newlabel{Fig:Training@cref}{{[figure][3][3]3.3}{[1][14][]16}}
\citation{torch_ini}
\citation{Goodfellow}
\citation{Pytorch website}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \footnotesize  Figurative example of how capacity influences the evolution of training and validation error. With increasing capacity the model i.e. autoencoder is able to fit the training and validation set thus training and validation error decreases. Typically the training error is less than the validation error. Yet both errors are too high and the model is underfitting. Further increasing the capacity leads to an increase of the validation error while the training error further decreases. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the decrease in training error the model is overfitting and capacity passed the point of optimal capacity. Optimal capacity is the point where both optimization and generalization are in balance.\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{Fig:Capacity}{{3.4}{17}{\footnotesize Figurative example of how capacity influences the evolution of training and validation error. With increasing capacity the model i.e. autoencoder is able to fit the training and validation set thus training and validation error decreases. Typically the training error is less than the validation error. Yet both errors are too high and the model is underfitting. Further increasing the capacity leads to an increase of the validation error while the training error further decreases. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the decrease in training error the model is overfitting and capacity passed the point of optimal capacity. Optimal capacity is the point where both optimization and generalization are in balance.\relax }{figure.caption.10}{}}
\newlabel{Fig:Capacity@cref}{{[figure][4][3]3.4}{[1][14][]17}}
\newlabel{Eq:Non-Linear Transformation}{{3.18}{17}{Training}{equation.3.2.18}{}}
\newlabel{Eq:Non-Linear Transformation@cref}{{[equation][18][3]3.18}{[1][17][]17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \footnotesize  Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha \) is typically, and also in this thesis set to \(\alpha =0.001\). Activations based on tangens hyperbolicus (Tanh) are the sigmoid function with \(\mathrm  {sig}(x)= \frac  {1}{2}(1+\qopname  \relax o{tanh}(\frac  {x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.\relax }}{18}{table.caption.11}\protected@file@percent }
\newlabel{Tab:Non-lin-func}{{3.1}{18}{\footnotesize Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha \) is typically, and also in this thesis set to \(\alpha =0.001\). Activations based on tangens hyperbolicus (Tanh) are the sigmoid function with \(\mathrm {sig}(x)= \frac {1}{2}(1+\tanh (\frac {x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.\relax }{table.caption.11}{}}
\newlabel{Tab:Non-lin-func@cref}{{[table][1][3]3.1}{[1][17][]18}}
\newlabel{Eq. Discrete Cross Correlation}{{3.19}{18}{Training}{equation.3.2.19}{}}
\newlabel{Eq. Discrete Cross Correlation@cref}{{[equation][19][3]3.19}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{Fig: Downsampling}{{3.5}{19}{Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.\relax }{figure.caption.12}{}}
\newlabel{Fig: Downsampling@cref}{{[figure][5][3]3.5}{[1][18][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \footnotesize  Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{Fig: Kernel}{{3.6}{20}{\footnotesize Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.\relax }{figure.caption.13}{}}
\newlabel{Fig: Kernel@cref}{{[figure][6][3]3.6}{[1][18][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Upsampling of the convolutions illustrated in \cref  {Fig: Downsampling} using transposed convolutions. Upsampling a $3\times 3$ input matrix to a $4\times 4$ featuremap is reversing \cref  {Fig: Downsampling}(a). Upsampling a $2\times 2$ input matrix to a $4\times 4$ featuremap is reversing \cref  {Fig: Downsampling}(b).\relax }}{21}{figure.caption.14}\protected@file@percent }
\newlabel{Fig: Upsampling}{{3.7}{21}{Upsampling of the convolutions illustrated in \cref {Fig: Downsampling} using transposed convolutions. Upsampling a $3\times 3$ input matrix to a $4\times 4$ featuremap is reversing \cref {Fig: Downsampling}(a). Upsampling a $2\times 2$ input matrix to a $4\times 4$ featuremap is reversing \cref {Fig: Downsampling}(b).\relax }{figure.caption.14}{}}
\newlabel{Fig: Upsampling@cref}{{[figure][7][3]3.7}{[1][18][]21}}
\@setckpt{chapters/Deep_Learning}{
\setcounter{page}{22}
\setcounter{equation}{19}
\setcounter{enumi}{13}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{1}
\setcounter{Item}{13}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{12}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{1}
\setcounter{algocfproc}{1}
\setcounter{algocf}{1}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{section@level}{0}
}
