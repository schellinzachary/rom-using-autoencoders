\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Dimensionality reduction algorithms}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Ch:DimRedAl}{{3}{8}{Dimensionality reduction algorithms}{chapter.3}{}}
\newlabel{Ch:DimRedAl@cref}{{[chapter][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Proper orthogonal decomposition (POD)}{8}{section.3.1}\protected@file@percent }
\newlabel{Sec: POD}{{3.1}{8}{Proper orthogonal decomposition (POD)}{section.3.1}{}}
\newlabel{Sec: POD@cref}{{[section][1][3]3.1}{[1][8][]8}}
\citation{Kutz}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq:EckardYoung}{{3.4}{9}{Proper orthogonal decomposition (POD)}{equation.3.1.4}{}}
\newlabel{Eq:EckardYoung@cref}{{[equation][4][3]3.4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Autoencoders}{9}{section.3.2}\protected@file@percent }
\newlabel{Sec:AE}{{3.2}{9}{Autoencoders}{section.3.2}{}}
\newlabel{Sec:AE@cref}{{[section][2][3]3.2}{[1][9][]9}}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \footnotesize  Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde  {\Pi }\), taking \(\Phi \) as input and outputs an approximation \(\tilde  {\Pi }\) of \(\Pi \). Similarily the encoder is a mapping \(h(\Pi )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The content of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), content of a node right of the inital node.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{Fig:Autoencoder}{{3.1}{10}{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. Input layer, output layer, the code or bottleneck layer and two hidden layers. Every layer is made up of nodes, which are represented as circles. The more hidden layers are added to the architecture the deeper is the autoencoder and the greater the capacity of the model. Not shown are possible activations for each layer. Labelled is decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde {\Pi }\), taking \(\Phi \) as input and outputs an approximation \(\tilde {\Pi }\) of \(\Pi \). Similarily the encoder is a mapping \(h(\Pi )=\Phi \), taking \(\Phi \) as input and outputs the code \(\Phi \). The box left of the autoencoder shows a computational graph for the feed forward connection of two nodes. The content of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\) which yields the output \(O\), content of a node right of the inital node.\relax }{figure.caption.6}{}}
\newlabel{Fig:Autoencoder@cref}{{[figure][1][3]3.1}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training}{10}{subsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \footnotesize  training\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{Fig:Training}{{3.2}{11}{\footnotesize training\relax }{figure.caption.7}{}}
\newlabel{Fig:Training@cref}{{[figure][2][3]3.2}{[1][10][]11}}
\citation{Hornik1989}
\citation{Goodfellow}
\citation{bibid}
\citation{bibid}
\citation{bibid}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Pytorch website}
\citation{dumoulin2018guide}
\citation{Goodfellow}
\newlabel{Eq. Linear Transformation}{{3.6}{12}{Training}{equation.3.2.6}{}}
\newlabel{Eq. Linear Transformation@cref}{{[equation][6][3]3.6}{[1][12][]12}}
\newlabel{Eq. Convolution}{{3.7}{12}{Training}{equation.3.2.7}{}}
\newlabel{Eq. Convolution@cref}{{[equation][7][3]3.7}{[1][12][]12}}
\newlabel{Eq. Discrete Convolution}{{3.8}{12}{Training}{equation.3.2.8}{}}
\newlabel{Eq. Discrete Convolution@cref}{{[equation][8][3]3.8}{[1][12][]12}}
\newlabel{Eq. Discrete Convolution Flip}{{3.9}{12}{Training}{equation.3.2.9}{}}
\newlabel{Eq. Discrete Convolution Flip@cref}{{[equation][9][3]3.9}{[1][12][]12}}
\newlabel{Eq. Discrete Cross Correlation}{{3.10}{12}{Training}{equation.3.2.10}{}}
\newlabel{Eq. Discrete Cross Correlation@cref}{{[equation][10][3]3.10}{[1][12][]12}}
\newlabel{Eq:Downsampling}{{3.11}{12}{Training}{equation.3.2.11}{}}
\newlabel{Eq:Downsampling@cref}{{[equation][11][3]3.11}{[1][12][]12}}
\citation{bibid}
\citation{dumoulin2018guide}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq. Cross correlation stride channel}{{3.12}{13}{Training}{equation.3.2.12}{}}
\newlabel{Eq. Cross correlation stride channel@cref}{{[equation][12][3]3.12}{[1][13][]13}}
\newlabel{Eq: Transposed convolution}{{3.13}{13}{Training}{equation.3.2.13}{}}
\newlabel{Eq: Transposed convolution@cref}{{[equation][13][3]3.13}{[1][13][]13}}
\newlabel{Eq. Backpropagation3}{{3.14}{13}{Training}{equation.3.2.14}{}}
\newlabel{Eq. Backpropagation3@cref}{{[equation][14][3]3.14}{[1][13][]13}}
\newlabel{Eq. Backpropagation2}{{3.15}{13}{Training}{equation.3.2.15}{}}
\newlabel{Eq. Backpropagation2@cref}{{[equation][15][3]3.15}{[1][13][]13}}
\newlabel{Eq. Backpropagation1}{{3.16}{13}{Training}{equation.3.2.16}{}}
\newlabel{Eq. Backpropagation1@cref}{{[equation][16][3]3.16}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Autoencoders}{13}{subsection.3.2.2}\protected@file@percent }
\newlabel{f_Conv}{{3.17}{14}{Autoencoders}{equation.3.2.17}{}}
\newlabel{f_Conv@cref}{{[equation][17][3]3.17}{[1][14][]14}}
\@setckpt{chapters/Deep_Learning}{
\setcounter{page}{15}
\setcounter{equation}{17}
\setcounter{enumi}{13}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{Item}{13}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{13}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{section@level}{0}
}
