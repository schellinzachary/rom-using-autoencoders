\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\citation{Kutz}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Dimensionality reduction algorithms}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Ch:DimRedAl}{{3}{8}{Dimensionality reduction algorithms}{chapter.3}{}}
\newlabel{Ch:DimRedAl@cref}{{[chapter][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Proper orthogonal decomposition (POD)}{8}{section.3.1}\protected@file@percent }
\newlabel{Sec: POD}{{3.1}{8}{Proper orthogonal decomposition (POD)}{section.3.1}{}}
\newlabel{Sec: POD@cref}{{[section][1][3]3.1}{[1][8][]8}}
\newlabel{Eq:PiPOD}{{3.2}{8}{Proper orthogonal decomposition (POD)}{equation.3.1.2}{}}
\newlabel{Eq:PiPOD@cref}{{[equation][2][3]3.2}{[1][8][]8}}
\citation{Kutz}
\citation{Kutz}
\citation{Goodfellow}
\citation{Goodfellow}
\newlabel{Eq:EckardYoung}{{3.4}{9}{Proper orthogonal decomposition (POD)}{equation.3.1.4}{}}
\newlabel{Eq:EckardYoung@cref}{{[equation][4][3]3.4}{[1][9][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Autoencoders}{9}{section.3.2}\protected@file@percent }
\newlabel{Sec:AE}{{3.2}{9}{Autoencoders}{section.3.2}{}}
\newlabel{Sec:AE@cref}{{[section][2][3]3.2}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \footnotesize  Scheme of an undercomplete autoencoder with five fully connected layers. An input layer, an output layer, the code or bottleneck layer, and two hidden layers. Every layer is made up of nodes, represented as circles. More hidden layers deepen the architecture of the autoencoder thus increasing the capacity of the model. Not shown are possible activations for each layer. Labeled are decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde  {\btch  }\), taking \(\Phi \) as an input, outputting an approximation \(\tilde  {\btch  }\) of \(\btch  \). Similarly, the encoder is a mapping \(h(\btch  )=\Phi \), taking \(\Phi \) as an input and outputting the code \(\Phi \). The box, left of the autoencoder, shows a computational graph for the feedforward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\), which yields the output \(O\), the input of a node right of the initial node.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{Fig:Autoencoder}{{3.1}{10}{\footnotesize Scheme of an undercomplete autoencoder with five fully connected layers. An input layer, an output layer, the code or bottleneck layer, and two hidden layers. Every layer is made up of nodes, represented as circles. More hidden layers deepen the architecture of the autoencoder thus increasing the capacity of the model. Not shown are possible activations for each layer. Labeled are decoder and encoder. The decoder is a mapping \(g(\Phi )=\tilde {\btch }\), taking \(\Phi \) as an input, outputting an approximation \(\tilde {\btch }\) of \(\btch \). Similarly, the encoder is a mapping \(h(\btch )=\Phi \), taking \(\Phi \) as an input and outputting the code \(\Phi \). The box, left of the autoencoder, shows a computational graph for the feedforward connection of two nodes. The input of a node is \(I\) which is multiplied with \(w\) a weight. A bias \(b\) is added to the result \(I^*\), which yields the output \(O\), the input of a node right of the initial node.\relax }{figure.caption.6}{}}
\newlabel{Fig:Autoencoder@cref}{{[figure][1][3]3.1}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training}{11}{subsection.3.2.1}\protected@file@percent }
\newlabel{Eq:Composition}{{3.6}{11}{Training}{equation.3.2.6}{}}
\newlabel{Eq:Composition@cref}{{[equation][6][3]3.6}{[1][11][]11}}
\newlabel{Eq:Linear Transformation}{{3.7}{11}{Training}{equation.3.2.7}{}}
\newlabel{Eq:Linear Transformation@cref}{{[equation][7][3]3.7}{[1][11][]11}}
\newlabel{Eq:Cost}{{3.8}{11}{Training}{equation.3.2.8}{}}
\newlabel{Eq:Cost@cref}{{[equation][8][3]3.8}{[1][11][]11}}
\newlabel{Eq: update}{{3.10}{11}{Training}{equation.3.2.10}{}}
\newlabel{Eq: update@cref}{{[equation][10][3]3.10}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \footnotesize  A single node network with one hidden node between input and output node.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{Fig:Sing_example}{{3.2}{11}{\footnotesize A single node network with one hidden node between input and output node.\relax }{figure.caption.7}{}}
\newlabel{Fig:Sing_example@cref}{{[figure][2][3]3.2}{[1][11][]11}}
\citation{Kutz}
\citation{Goodfellow}
\citation{torch_ini}
\citation{LeCun98}
\citation{Goodfellow}
\citation{he2015}
\citation{kingma2017adam}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The Adam algorithm\relax }}{13}{algocf.1}\protected@file@percent }
\newlabel{Alg:Adam}{{1}{13}{Training}{algocf.1}{}}
\newlabel{Alg:Adam@cref}{{[algorithm][1][]1}{[1][13][]13}}
\citation{bushaev_2017}
\citation{kingma2017adam}
\citation{kingma2017adam}
\citation{kingma2017adam}
\citation{LeCun98}
\citation{LeCun98}
\newlabel{Eq: fm}{{3.14}{14}{Training}{equation.3.2.14}{}}
\newlabel{Eq: fm@cref}{{[equation][14][3]3.14}{[1][14][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch  \) for training is split up into a training \(\btch  _{train}\) and a validation \(\btch  _{val}\) set with a \(80/20\) ratio (a). The \(\btch  _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite  {LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib  _{i})=\tilde  {\minib  }_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib  _{i}\) and \(\tilde  {\minib  }_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd  \). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref  {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar  \) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch  _{val}\), which it has not seen before, in (h) with \(AE(\btch  _{val})=\tilde  {\btch  }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch  _{val}\) and \(\tilde  {\btch  }_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline  {E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{Fig:Training}{{3.3}{15}{Flowchart of the training process used in this thesis for an autoencoder. The data set \(\btch \) for training is split up into a training \(\btch _{train}\) and a validation \(\btch _{val}\) set with a \(80/20\) ratio (a). The \(\btch _{train}\) set is equally devided into \(n\) minibatches (b). The network will be trained over \(H\) epochs and \(n\) minibatches. First the weights \(w\) are initialized as in \cite {LeCun98} and biases set to zero (c). A successive evaluation of the first minibatch with \(AE(\minib _{i})=\tilde {\minib }_{i}\) called forward propagation takes place in (d). The mean squared error between \(\minib _{i}\) and \(\tilde {\minib }_{i}\) yields the average error over one minibatch \(E_{B_i}\). Thereafter \(E_{B_i}\) is backpropagated through the network (f) yielding the gradient \(\grd \). This gradient is then used to optimize weights \(w\) and biases \(b\) with \(Adam\), an optimization algorithm seen in \cref {Alg:Adam}, (g). In the fashion of using (d) to (g) all \(n\) minibatches are shown to the network and lead to an optimization of the network's free parameters \(\frepar \) which are weights \(w\) and biases \(b\). After all minibatches are shown to the network a validation step is taken. Hence the network evaluates \(\btch _{val}\), which it has not seen before, in (h) with \(AE(\btch _{val})=\tilde {\btch }_{val}\). Subsequently, the evaluation of the mean squared error between \(\btch _{val}\) and \(\tilde {\btch }_{val}\), produces the validation error \(E_{val}\) in (i). Afterwards the arithemtric mean of all minibatch errors \(E_{B_i}\) is taken. This produces \(\overline {E}_B\) and concludes the first epoch. The maximum number of epochs \(H\) is reached when \(E_B\) and \(E_{val}\), have dropped to a satisfactory value and the training finishes.\relax }{figure.caption.9}{}}
\newlabel{Fig:Training@cref}{{[figure][3][3]3.3}{[1][14][]15}}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces This is a figurative example of capacity influencing the evolution of training and validation error. Increasing the capacity enables the model to fit the training and validation set thus both error decreases. Typically the training error is smaller than the validation error. Yet both errors are too high. The model underfits. A further increase of capacity equally increases the validation error with decreasing the training error further. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the training error, the model is overfitting. The capacity passed the point of optimality. Optimal capacity is the point where both optimization and generalization are in balance.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{Fig:Capacity}{{3.4}{16}{This is a figurative example of capacity influencing the evolution of training and validation error. Increasing the capacity enables the model to fit the training and validation set thus both error decreases. Typically the training error is smaller than the validation error. Yet both errors are too high. The model underfits. A further increase of capacity equally increases the validation error with decreasing the training error further. The gap between training- and validation error is called the generalization gap. Once the generalization gap dominates the training error, the model is overfitting. The capacity passed the point of optimality. Optimal capacity is the point where both optimization and generalization are in balance.\relax }{figure.caption.10}{}}
\newlabel{Fig:Capacity@cref}{{[figure][4][3]3.4}{[1][14][]16}}
\citation{torch_ini}
\citation{Goodfellow}
\citation{Pytorch website}
\citation{Goodfellow}
\newlabel{Eq:Non-Linear Transformation}{{3.18}{17}{Training}{equation.3.2.18}{}}
\newlabel{Eq:Non-Linear Transformation@cref}{{[equation][18][3]3.18}{[1][17][]17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha \) is typically, and also in this thesis set to \(\alpha =0.001\). Activations based on the hyperbolic tangent (Tanh) are the sigmoid function with \(\mathrm  {sig}(x)= \frac  {1}{2}(1+\qopname  \relax o{tanh}(\frac  {x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.\relax }}{17}{table.caption.11}\protected@file@percent }
\newlabel{Tab:Non-lin-func}{{3.1}{17}{Non-linear activation functions of type rectifier are the rectified linear unit (ReLU), it's leaky variant LeakyReLU, the exponential version ELU. The negaive slope of LeakyReLU below zero \(\alpha \) is typically, and also in this thesis set to \(\alpha =0.001\). Activations based on the hyperbolic tangent (Tanh) are the sigmoid function with \(\mathrm {sig}(x)= \frac {1}{2}(1+\tanh (\frac {x}{2}))\) and it's variant SiLU. The functions are shown over an illustrative domain.\relax }{table.caption.11}{}}
\newlabel{Tab:Non-lin-func@cref}{{[table][1][3]3.1}{[1][17][]17}}
\citation{Goodfellow}
\citation{lane_2018}
\citation{divyanshu_2020}
\newlabel{Eq. Discrete Cross Correlation}{{3.19}{18}{Training}{equation.3.2.19}{}}
\newlabel{Eq. Discrete Cross Correlation@cref}{{[equation][19][3]3.19}{[1][18][]18}}
\newlabel{Fig: Downsampling(a)}{{3.5a}{19}{Two dimensional example of a \textbf {one} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(3\times 3\) matrix, as the input is downsampled by a factor of \(0.75\).\relax }{figure.caption.12}{}}
\newlabel{Fig: Downsampling(a)@cref}{{[subfigure][1][3,5]3.5a}{[1][18][]19}}
\newlabel{sub@Fig: Downsampling(a)}{{a}{19}{Two dimensional example of a \textbf {one} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(3\times 3\) matrix, as the input is downsampled by a factor of \(0.75\).\relax }{figure.caption.12}{}}
\newlabel{sub@Fig: Downsampling(a)@cref}{{[subfigure][1][3,5]3.5a}{[1][18][]19}}
\newlabel{Fig: Downsampling(b)}{{3.5b}{19}{Two dimensional example of of a \textbf {two} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(2\times 2\) matrix, as the input is downsampled by a factor of \(0.5\).\relax }{figure.caption.12}{}}
\newlabel{Fig: Downsampling(b)@cref}{{[subfigure][2][3,5]3.5b}{[1][18][]19}}
\newlabel{sub@Fig: Downsampling(b)}{{b}{19}{Two dimensional example of of a \textbf {two} strided convolution over a \(4\times 4\) input matrix with a \(2\times 2\) kernel matrix. The equations for obtaining the components \(i\) and \(j\) of the feature map are given. Note that all biases are omitted for simplicity and bold symbols should help readability and do not represent vectors. The resulting feature map is a \(2\times 2\) matrix, as the input is downsampled by a factor of \(0.5\).\relax }{figure.caption.12}{}}
\newlabel{sub@Fig: Downsampling(b)@cref}{{[subfigure][2][3,5]3.5b}{[1][18][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{Fig: Downsampling}{{3.5}{19}{Comparison of a one strided convolution (a) and a two strided convolution (b). Illustrated are two steps of a kernel matrix moving over an input matrix. The two strided convolution yields an increased downscaling compared to the one strided convolution.\relax }{figure.caption.12}{}}
\newlabel{Fig: Downsampling@cref}{{[figure][5][3]3.5}{[1][18][]19}}
\citation{dumoulin2018guide}
\citation{Goodfellow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \footnotesize  Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.\relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{Fig: Kernel}{{3.6}{20}{\footnotesize Schematic representation of typical components and their structural set-up in a convolutional neural network (CNN). Shown is the convolutional operation and the associated kernel, the flattening and successive fully connected operation. First a kernel moves over the one dimensional input performing convolutional operations, which yields the components of a feature map in the successive hidden layer. One channel in a hidden layer can aswell be called feature map. Here the input has just one channel, but four different kernels move over the input which produces four feature maps in the first hidden layer. Hence the first hidden layer comprises four channels or feature maps. Whenever one layer has different channels, the kernel moves with different parameters over all channels at the same time. Therefore we can think of it in this case as a three dimensional kernel tensor. Note that for simplicity this is not depicted in this figure. While the width and height of the input decreases over the hidden layers, the number of channels typically increases . The last hidden layer is flattend and successively connected to the code layer through a fully connected operation.\relax }{figure.caption.13}{}}
\newlabel{Fig: Kernel@cref}{{[figure][6][3]3.6}{[1][18][]20}}
\newlabel{Fig: Upsampling(a)}{{3.7a}{21}{Mechanism of a two dimensional \textbf {two} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. Upsampled by a factor of \(2.33\) produces a \(7\times 7\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is not divisible by the stride width. Uneven overlapping produces a checkerboard-like structure.\relax }{figure.caption.14}{}}
\newlabel{Fig: Upsampling(a)@cref}{{[subfigure][1][3,7]3.7a}{[1][20][]21}}
\newlabel{sub@Fig: Upsampling(a)}{{a}{21}{Mechanism of a two dimensional \textbf {two} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. Upsampled by a factor of \(2.33\) produces a \(7\times 7\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is not divisible by the stride width. Uneven overlapping produces a checkerboard-like structure.\relax }{figure.caption.14}{}}
\newlabel{sub@Fig: Upsampling(a)@cref}{{[subfigure][1][3,7]3.7a}{[1][20][]21}}
\newlabel{Fig: Upsampling(b)}{{3.7b}{21}{Mechanism of a two-dimensional \textbf {three} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. The resulting output upsampled by a factor of \(3\) is a \(9\times 9\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is identical to the stride width. Overlaps and checkerboard-like structures are avoided.\relax }{figure.caption.14}{}}
\newlabel{Fig: Upsampling(b)@cref}{{[subfigure][2][3,7]3.7b}{[1][20][]21}}
\newlabel{sub@Fig: Upsampling(b)}{{b}{21}{Mechanism of a two-dimensional \textbf {three} strided transposed convolution over a \(3\times 3\) input matrix with a \(3\times 3\) kernel matrix. The resulting output upsampled by a factor of \(3\) is a \(9\times 9\) matrix. Three kernel locations on the output are indicated with step 1, step 2, and step 3. The extent of the kernel is identical to the stride width. Overlaps and checkerboard-like structures are avoided.\relax }{figure.caption.14}{}}
\newlabel{sub@Fig: Upsampling(b)@cref}{{[subfigure][2][3,7]3.7b}{[1][20][]21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualization of the mechanism in transposed convolutional layers. Shown is pixelation, a common problem with transposed convolutional layers (a) and a possible solution, mitigating the pixelation effect.\relax }}{21}{figure.caption.14}\protected@file@percent }
\newlabel{Fig: Upsampling}{{3.7}{21}{Visualization of the mechanism in transposed convolutional layers. Shown is pixelation, a common problem with transposed convolutional layers (a) and a possible solution, mitigating the pixelation effect.\relax }{figure.caption.14}{}}
\newlabel{Fig: Upsampling@cref}{{[figure][7][3]3.7}{[1][20][]21}}
\@setckpt{chapters/Deep_Learning}{
\setcounter{page}{22}
\setcounter{equation}{21}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{1}
\setcounter{dblbotnumber}{2}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{9}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{1}
\setcounter{algocfproc}{1}
\setcounter{algocf}{1}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{dirtytalk@qdepth}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{section@level}{0}
}
