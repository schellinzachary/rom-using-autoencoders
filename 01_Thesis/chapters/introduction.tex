% !TEX root = master.tex

\chapter{Introduction}
\label{CH:intro}
%\pagenumbering{arabic}


The Bhatnagar-Gross-Krook equation (BGK) is a kinetic collision model of ionized and neutral gases valid for rarefied as well as other pressure regimes \cite{BGK}. Generating data of such a flow field is essential for various industry and scientific applications[\textbf{REF}]. With the intention to reduce time and cost during the data generating process, experiments were substituted with computational fluid dynamics (CFD) computations. Consequently reduced-order models (ROMs) coupled to aforementioned computations were introduced to further the reduction of time and cost. The thriving field of artificial intelligence successfully operates on natural language processing and object recognition and has now surfaced in fluid mechanics for model order reduction as seen in \cite{Carlberg} and \cite{bukka2020assessment}. This thesis wants to estimate the capability of artifical intelligence for model order reduction in fluid mechanic by specifically revising the performance of autoencoders in a fully connected and convolutional version using proper orthogonal decomposition (POD) as a benchmark. Thereto the BGK model in Sod's shock tube is employed. \\

The thesis is structured in five sections. First the BGk model in Sod's sock tube is introduced in \cref{Ch:BGK}. Furthermore, dimensionality reduction algorithms used in this thesis namely proper orthogonal decomposition (POD) and autoencoders are introduced in \cref{Ch:DimRedAl}. Additionally model order reduction and the integration of POD and autoencoders with emphasis on the so called offline phase is described in \cref{Ch:ROM}. The comparison of both methods by evaluating their reconstruction loss and the interpretability of the so called reduced variables can be found in \cref{Ch:Results}. In addition the autoencoders abillity to generalize is also tested in this section. A description covering the selection of design features for the convolutional and fully connected autoencoder is available in \cref{Ch:ApA}. The code for this thesis is written in \texttt{Python} version 3.8. Both autoencoders are implemented using the open source machine learning library \texttt{pyTorch} version 1.8.1 developed by Facebook's AI Reasearch lab (FAIR) which is available from \url{www.pytorch.org}. Furthermore \texttt{NumPy} for any additional computations aside the machine learning aspect and \texttt{pandas} for data manipulation available through the open source scientific computing library \texttt{SciPy} version 1.6.3 available through \url{www.scipy.org} is used.  

State of the art model reduction of dynamical systems can be done via proper orthogonal decomposition (POD) which is an algorithm feeding on the idea of singular value decomposition (SVD)\cite{Franz}\cite{Kutz}. POD captures a low-rank representation on a linear manifold. So called POD modes, derived from SVD, describe the principle components of a problem which can be coupled within a Galerkin framework to produce an approximation of a lower dimension \(r\). 
Bernard et al. use POD-Galerkin with an additional population of their snapshot database via optimal transport for the proposed BGK equation, bisecting computational run time (cost) in conjunction with an approximation error of \(\sim\) 1 \% in \cite{Bernard}. Artificial intelligence in the form of autoencoders replacing the POD within a Galerkin framework is evaluated against the POD performance by Kookjin et al. for advection-dominated problems\cite{Carlberg} resulting in sub 0.1\% errors. An additional time inter- and extrapolation is evaluated. Using machine learning/ deep learning for reduced order modeling in CFD is a novel approach although "the idea of autoencoders has been part of the historical landscape of neural networks for decades"\cite[p.493]{Goodfellow}. Autoencoders, or more precisely learning internal representations by the delta rule (backpropagation) and the use of hidden units in a feed forward neural network architecture, premiered by Rumelhart et al. (1986) \cite{Rumelhart}.  Through so called hierarchical training Ballard et al.(1987) introduce a strategy to train auto autoassociative networks (nowadays referred to as autoencoders), in a reasonable time promoting further development despite computational limitations \cite{Ballard}. The so called bottleneck of autoencoders yields a non-smooth and entangled representation thus beeing uninterpretable by practitioners\cite{Rifai2011} leading to developements in this field. Rifai et al. introduce the contractive autoencoder (CAE) for classification tasks (2011), with the aim to extract robust features which are insensitive to input variations orthogonal to the low-dimensional non-linear manifold by adding a penalty on the frobenius norm of the instrinsic variables with respect to the input, surpassing other classification algorithms \cite{Rifai2011}. Subsequent development emerges with the manifold tangent classifier (MTC) \cite{Rifai_2011a}. A local chart for each datapoint is obtained hence characterizing the manifold  which in turn improves classification performance. On that basis a generative process for the CAE is developed. Through movements along the manifold with directions defined by the Jacobian of the bottleneck layer with respect to the input \begin{math}	\vec{x}_m=JJ^T \end{math}, sampling is realized \cite{rifai2012generative}....
Proper orthogonal decomposition (POD) and it's numerous variants like shifted-POD\cite{bibid}, POD-Galerkin\cite{bibid}, POD+I \cite{bibid} to name only a few of them, try to solve this problem by......	