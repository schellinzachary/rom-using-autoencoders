% !TEX root = master.tex

\chapter{Introduction}
\label{CH:intro}
%\pagenumbering{arabic}


The Bhatnagar-Gross-Krook equation (BGK) is a kinetic collision model for ionized and neutral gases valid in rarefied as well as other pressure regimes \cite{BGK}. Generating data of such a flow field is essential for various industry and scientific applications\cite{rarefiedGDapplc}. With the intention to reduce time and cost during the data generating process, experiments were substituted with computational fluid dynamics (CFD) simulations. Consequently, reduced-order models (ROMs) coupled to the aforementioned simulations were introduced to further the reduction of time and cost. The thriving field of artificial intelligence successfully operates on natural language processing and object recognition and has now surfaced in fluid mechanics for model order reduction as seen in \cite{Carlberg} and \cite{bukka2020assessment}. This thesis will attempt to estimate the capability of artificial intelligence for model order reduction for the BGK model in Sod's shock tube. Specifically, the performance of autoencoders in a fully connected and convolutional version is revised using proper orthogonal decomposition (POD) as a benchmark.\\

Using neural networks i.e. deep learning in the form of autoencoders for reduced-order modeling in CFD is a novel approach. Though, "the idea of autoencoders has been part of the historical landscape of neural networks for decades"\cite{Goodfellow}[p.493]. Autoencoders, or more precisely learning internal representations by the delta rule (backpropagation) and the use of hidden units in a feed-forward neural network architecture, premiered by Rumelhart et al in \cite{Rumelhart} in 1986. Through, so-called, hierarchical training, Ballard et al. introduce in  \cite{Ballard} in 1987 a strategy to train auto auto-associative networks, nowadays referred to as autoencoders, in a reasonable time promoting further development despite computational limitations. The so-called bottleneck of autoencoders often yields a non-smooth and entangled representation thus being uninterpretable by practitioners leading to developments in this field is stated in \cite{Rifai2011} by Rifai et al. in 2011. They introduce the contractive autoencoder (CAE) for classification tasks intending to extract robust features which are insensitive to input variations orthogonal to the low-dimensional non-linear manifold by adding a penalty on the Frobenius norm of the intrinsic variables with respect to the input, surpassing other classification algorithms. In 2014, Titled "Auto-Encoding Variational Bayes" in \cite{kingma2014autoencoding}, Kingma et. al introduce the variational autoencoder with which an intractable posterior density distribution can be approximated. Disentangled latent variables are obtained from which sampling is possible. A similar approach formulated in 2019 in \cite{tolstikhin2019wasserstein} by Tolstikhin et. al enforces as well entangled latent variables during autoencoding. Timeseries forecasting using neural networks experiences a boost when in 1997 Hochreiter et. al introduce the long short-term memory (LSTM) cell that uses an "efficient truncated backprop version"[p.22] in \cite{HochSchm97} pivotal decreasing training time for recurrent networks. Further developments in 2017 by Vaswani et. al in \cite{vaswani2017attention} concerning natural language translation introduce the transformer which eschews convolutions and recurrence, solely relying on global attention mechanisms that are used to completely replace convolutions for image recognition in \cite{dosovitskiy2021image} and through their computational efficacy constitute a small revolution.

Proper orthogonal decomposition (POD) coupled with Galerkin projection methods can be found in numerous tracks towards model order reduction for dynamical systems like shifted POD (sPOD) introduced by Reiss et. al in 2018 in \cite{reiss2018shifted} tailored for transport problems and wavelet POD (wPOD) by Krah et. al in \cite{krah2020wavelet}  tailored for three-dimensional high-resolution data, which limits the applicability of POD, to name only a few of them. Bernard et al. use POD-Galerkin in \cite{Bernard} with an additional population of their snapshot database via optimal transport for the proposed BGK equation and test case, reducing computational runtime by 94\% in conjunction with an approximation error of \(\sim\) 1\%. Artificial intelligence in the form of convolutional autoencoders, replacing POD within a Galerkin framework, is evaluated against the POD performance by Kookjin et al. in \cite{Carlberg} for advection-dominated problems resulting in sub-0.1\% errors for two experiments using approximately, what they call, the intrinsic solution manifold dimension. An additional temporal interpolation requires to train the autoencoders with fewer samples which are found to be insensitive above 100 examples, for their experiments, along with degradation of accuracy of 20\% only after that. Eivazi et. al advance in \cite{eivazi2021recurrent} a step further not only replacing POD by autoencoders but also using an LSTM for the Galerkin projection method to evolve the intrinsic variables in time, which culminates as well in sub-.1\% errors. Both contributions mention the computational cost needed for training the neural networks questioning their fit in model order reduction.\\ 

The thesis is divided into five sections. Firstly, the BGk model in Sod's sock tube is introduced in \cref{Ch:BGK}. Furthermore, dimensionality reduction algorithms used in this thesis, namely proper orthogonal decomposition (POD) and autoencoders, are introduced in \cref{Ch:DimRedAl}. Additionally, model order reduction and the integration of POD and autoencoders with emphasis on the so-called offline phase is described in \cref{Ch:ROM}. The comparison of both methods by evaluating their reconstruction loss and the interpretability of the so-called reduced variables can be found in \cref{Ch:Results}. In addition, the autoencoders ability to generalize is also tested in this section. A description covering the selection of design features for the convolutional and fully connected autoencoder is available in \cref{Ch:ApA} and \cref{Ch:ApB}. The code for this thesis is written in \texttt{Python} version 3.8. Both autoencoders are implemented using the open-source machine learning library \texttt{pyTorch} version 1.8.1 developed by Facebook's AI Research lab (FAIR) which is available from \url{www.pytorch.org}. Furthermore, \texttt{NumPy}, for any additional computations aside the machine learning aspect, and \texttt{pandas}, for data manipulation available through the open-source scientific computing library \texttt{SciPy} version 1.6.3 available through \url{www.scipy.org}, are used.\\  
