% !TEX root = master.tex

\chapter{Introduction}
\label{CH:intro}
%\pagenumbering{arabic}


The Bhatnagar-Gross-Krook equation (BGK) is a kinetic collision model of ionized and neutral gases valid for rarefied as well as other pressure regimes \cite{BGK}. Generating data of such a flow field is essential for various industry and scientific applications\cite{rarefiedGDapplc}. With the intention to reduce time and cost during the data generating process, experiments were substituted with computational fluid dynamics (CFD) computations. Consequently reduced-order models (ROMs) coupled to aforementioned computations were introduced to further the reduction of time and cost. The thriving field of artificial intelligence successfully operates on natural language processing and object recognition and has now surfaced in fluid mechanics for model order reduction as seen in \cite{Carlberg} and \cite{bukka2020assessment}. This thesis wants to estimate the capability of artifical intelligence for model order reduction in computational fluid mechanics by specifically revising the performance of autoencoders in a fully connected and convolutional version using proper orthogonal decomposition (POD) as a benchmark. Thereto the BGK model in Sod's shock tube is employed. \\

Using neural networks i.e. deep learning in the form of autoencoders for reduced order modeling in CFD is a novel approach. Though, "the idea of autoencoders has been part of the historical landscape of neural networks for decades"\cite{Goodfellow}[p.493]. Autoencoders, or more precisely learning internal representations by the delta rule (backpropagation) and the use of hidden units in a feed forward neural network architecture, premiered by Rumelhart et al in \cite{Rumelhart} in 1986. Through, so called, hierarchical training, Ballard et al. introduce in  \cite{Ballard} in 1987 a strategy to train auto autoassociative networks, nowadays referred to as autoencoders, in a reasonable time promoting further development despite computational limitations. The so called bottleneck of autoencoders often yield a non-smooth and entangled representation thus being uninterpretable by practitioners leading to developements in this field is stated in \cite{Rifai2011} by Rifai et al. in 2011. They introduce the contractive autoencoder (CAE) for classification tasks with the aim to extract robust features which are insensitive to input variations orthogonal to the low-dimensional non-linear manifold by adding a penalty on the frobenius norm of the instrinsic variables with respect to the input, surpassing other classification algorithms. In 2014, Titled "Auto-Encoding Variational Bayes" in \cite{kingma2014autoencoding}, Kingma et. al introduce the variational autoencoder with which an intractable posterior density distribution can be approximated. Disentangled latent variables are obtained from which sampling is possible. A similar approach formulated in 2019 in \cite{tolstikhin2019wasserstein} by Tolstikhin et. al enforces as well entangled latent variables during autoencoding. Timeseries forecasting using neural networks experiences a boost when in 1997 Hochreiter et. al introduce the long short-term memory (LSTM) cell that uses an "efficient truncated backprop version"[p.22] in \cite{HochSchm97} pivotal decreasing training time for recurrent networks. Further developements in concerning natural language translation introduce the transformer  

Proper orthogonal decomposition (POD) and it's numerous variants like shifted-POD\cite{bibid}, POD-Galerkin\cite{bibid}, POD+I \cite{bibid} to name only a few of them, try to solve this problem by......	\\
State of the art model reduction of dynamical systems is conducted utilizing proper orthogonal decomposition (POD) within a Galerkin framework. Bernard et al. use POD-Galerkin  in \cite{Bernard} with an additional population of their snapshot database via optimal transport for the proposed BGK equation and test case, using reducing computational runtime by 94\% in conjunction with an approximation error of \(\sim\) 1\%. Artificial intelligence in the form of convolutional autoencoders, replacing POD within a Galerkin framework, is evaluated against the POD performance by Kookjin et al. in \cite{Carlberg} for advection-dominated problems resulting in sub-0.1\% errors for two experiments using approximately, what they call, the intrinsic solution manifold dimension. An additional temporal interpolation requires to train the autoencoders with less samples resulting in an insensitivity above 100 examples along with a degradation of accuracy of 20\% only after that. Eivazi et. al go in \cite{eivazi2021recurrent} a step further not only replacing POD with autoencoders, but also using an LSTM for the Petrov-Galerkin method to evolve the intrinsic variables in time. 

The thesis is divided into five sections. First, the BGk model in Sod's sock tube is introduced in \cref{Ch:BGK}. Furthermore, dimensionality reduction algorithms used in this thesis, namely proper orthogonal decomposition (POD) and autoencoders, are introduced in \cref{Ch:DimRedAl}. Additionally, model order reduction and the integration of POD and autoencoders with emphasis on the so called offline phase is described in \cref{Ch:ROM}. The comparison of both methods by evaluating their reconstruction loss and the interpretability of the so called reduced variables can be found in \cref{Ch:Results}. In addition the autoencoders abillity to generalize is also tested in this section. A description covering the selection of design features for the convolutional and fully connected autoencoder is available in \cref{Ch:ApA}. The code for this thesis is written in \texttt{Python} version 3.8. Both autoencoders are implemented using the open source machine learning library \texttt{pyTorch} version 1.8.1 developed by Facebook's AI Reasearch lab (FAIR) which is available from \url{www.pytorch.org}. Furthermore \texttt{NumPy} for any additional computations aside the machine learning aspect and \texttt{pandas} for data manipulation available through the open source scientific computing library \texttt{SciPy} version 1.6.3 available through \url{www.scipy.org} is used.\\  
